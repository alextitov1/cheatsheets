{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hello this is yet another cheat sheet pages","title":"Home"},{"location":"#hello-this-is-yet-another-cheat-sheet-pages","text":"","title":"Hello this is yet another cheat sheet pages"},{"location":"AWS/Athena/","text":"Analyze VPC Flow Logs Data in Athena Create the Athena Table On the right, click Launch query editor. Select the Settings tab and then click Manage. In the Location of query result field, paste your copied S3 URI. Click Save. Create Partitions and Analyze the Data Select the query editor's Editor tab. In the Query 1 editor, paste the following query, replacing {your_log_bucket} and {account_id} with your log bucket and account ID details (you can pull these from the S3 URI path you copied): CREATE EXTERNAL TABLE IF NOT EXISTS default.vpc_flow_logs ( version int, account string, interfaceid string, sourceaddress string, destinationaddress string, sourceport int, destinationport int, protocol int, numpackets int, numbytes bigint, starttime int, endtime int, action string, logstatus string ) PARTITIONED BY (dt string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' LOCATION 's3://{your_log_bucket}/AWSLogs/{account_id}/vpcflowlogs/us-east-1/' TBLPROPERTIES (\"skip.header.line.count\"=\"1\"); Click Run. You should see a message indicating that the query was successful. On the right, click the + icon to open a new query editor. In the editor, paste the following query, replacing YYYY-MM-DD with the current date, and replacing the existing location with your copied S3 URI: ALTER TABLE default.vpc_flow_logs ADD PARTITION (dt='YYYY-MM-DD') location 's3://{your_log_bucket}/AWSLogs/{account_id}/vpcflowlogs/us-east-1/YYYY/MM/DD/'; Click Run. You should see a message indicating that the query was successful. On the right, click the + icon to open a new query editor. In the editor, paste the following query: SELECT day_of_week(from_iso8601_timestamp(dt)) AS day, dt, interfaceid, sourceaddress, destinationport, action, protocol FROM vpc_flow_logs WHERE action = 'REJECT' AND protocol = 6 order by sourceaddress LIMIT 100; Click Run. Your partitioned data should display in the query results. Conclusion","title":"Analyze VPC Flow Logs Data in Athena"},{"location":"AWS/Athena/#analyze-vpc-flow-logs-data-in-athena","text":"","title":"Analyze VPC Flow Logs Data in Athena"},{"location":"AWS/Athena/#create-the-athena-table","text":"On the right, click Launch query editor. Select the Settings tab and then click Manage. In the Location of query result field, paste your copied S3 URI. Click Save.","title":"Create the Athena Table"},{"location":"AWS/Athena/#create-partitions-and-analyze-the-data","text":"Select the query editor's Editor tab. In the Query 1 editor, paste the following query, replacing {your_log_bucket} and {account_id} with your log bucket and account ID details (you can pull these from the S3 URI path you copied): CREATE EXTERNAL TABLE IF NOT EXISTS default.vpc_flow_logs ( version int, account string, interfaceid string, sourceaddress string, destinationaddress string, sourceport int, destinationport int, protocol int, numpackets int, numbytes bigint, starttime int, endtime int, action string, logstatus string ) PARTITIONED BY (dt string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' LOCATION 's3://{your_log_bucket}/AWSLogs/{account_id}/vpcflowlogs/us-east-1/' TBLPROPERTIES (\"skip.header.line.count\"=\"1\"); Click Run. You should see a message indicating that the query was successful. On the right, click the + icon to open a new query editor. In the editor, paste the following query, replacing YYYY-MM-DD with the current date, and replacing the existing location with your copied S3 URI: ALTER TABLE default.vpc_flow_logs ADD PARTITION (dt='YYYY-MM-DD') location 's3://{your_log_bucket}/AWSLogs/{account_id}/vpcflowlogs/us-east-1/YYYY/MM/DD/'; Click Run. You should see a message indicating that the query was successful. On the right, click the + icon to open a new query editor. In the editor, paste the following query: SELECT day_of_week(from_iso8601_timestamp(dt)) AS day, dt, interfaceid, sourceaddress, destinationport, action, protocol FROM vpc_flow_logs WHERE action = 'REJECT' AND protocol = 6 order by sourceaddress LIMIT 100; Click Run. Your partitioned data should display in the query results. Conclusion","title":"Create Partitions and Analyze the Data"},{"location":"AWS/Cloudwatch/","text":"CloudWatch Create a CloudWatch Log Metric Filter Select the Metric filters tab and then click Create metric filter. In the Filter pattern field, enter the following pattern to track failed SSH attempts on port 22: [version, account, eni, source, destination, srcport, destport=\"22\", protocol=\"6\", packets, bytes, windowstart, windowend, action=\"REJECT\", flowlogstatus] Use the Select log data to test dropdown to select Custom log data. In the Log event messages field, replace the existing log data with the following: 2 086112738802 eni-0d5d75b41f9befe9e 61.177.172.128 172.31.83.158 39611 22 6 1 40 1563108188 1563108227 REJECT OK 2 086112738802 eni-0d5d75b41f9befe9e 182.68.238.8 172.31.83.158 42227 22 6 1 44 1563109030 1563109067 REJECT OK 2 086112738802 eni-0d5d75b41f9befe9e 42.171.23.181 172.31.83.158 52417 22 6 24 4065 1563191069 1563191121 ACCEPT OK 2 086112738802 eni-0d5d75b41f9befe9e 61.177.172.128 172.31.83.158 39611 80 6 1 40 1563108188 1563108227 REJECT OK Click Test pattern and then review the results. Click Next. Fill in the metric details: Filter name: In the text field, enter dest-port-22-reject. Metric namespace: In the text field, enter a name (e.g., vpcflowlogs). Metric name: In the text field, enter SSH Rejects. Metric value: In the text field, enter 1. Leave the other fields blank and click Next. Review the metric details and then click Create metric filter . Use CloudWatch Logs Insights In the CloudWatch sidebar menu, navigate to Logs and select Logs Insights. Use the Select log group(s) search bar to select VPCFlowLogs. In the right-hand pane, select Queries. In the Sample queries section, expand VPC Flow Logs and then expand Top 20 source IP addresses with highest number of rejected requests. Click Apply and note the changes applied in the query editor. Click Run query. After a few moments, you'll see some data start to populate.","title":"CloudWatch"},{"location":"AWS/Cloudwatch/#cloudwatch","text":"","title":"CloudWatch"},{"location":"AWS/Cloudwatch/#create-a-cloudwatch-log-metric-filter","text":"Select the Metric filters tab and then click Create metric filter. In the Filter pattern field, enter the following pattern to track failed SSH attempts on port 22: [version, account, eni, source, destination, srcport, destport=\"22\", protocol=\"6\", packets, bytes, windowstart, windowend, action=\"REJECT\", flowlogstatus] Use the Select log data to test dropdown to select Custom log data. In the Log event messages field, replace the existing log data with the following: 2 086112738802 eni-0d5d75b41f9befe9e 61.177.172.128 172.31.83.158 39611 22 6 1 40 1563108188 1563108227 REJECT OK 2 086112738802 eni-0d5d75b41f9befe9e 182.68.238.8 172.31.83.158 42227 22 6 1 44 1563109030 1563109067 REJECT OK 2 086112738802 eni-0d5d75b41f9befe9e 42.171.23.181 172.31.83.158 52417 22 6 24 4065 1563191069 1563191121 ACCEPT OK 2 086112738802 eni-0d5d75b41f9befe9e 61.177.172.128 172.31.83.158 39611 80 6 1 40 1563108188 1563108227 REJECT OK Click Test pattern and then review the results. Click Next. Fill in the metric details: Filter name: In the text field, enter dest-port-22-reject. Metric namespace: In the text field, enter a name (e.g., vpcflowlogs). Metric name: In the text field, enter SSH Rejects. Metric value: In the text field, enter 1. Leave the other fields blank and click Next. Review the metric details and then click Create metric filter .","title":"Create a CloudWatch Log Metric Filter"},{"location":"AWS/Cloudwatch/#use-cloudwatch-logs-insights","text":"In the CloudWatch sidebar menu, navigate to Logs and select Logs Insights. Use the Select log group(s) search bar to select VPCFlowLogs. In the right-hand pane, select Queries. In the Sample queries section, expand VPC Flow Logs and then expand Top 20 source IP addresses with highest number of rejected requests. Click Apply and note the changes applied in the query editor. Click Run query. After a few moments, you'll see some data start to populate.","title":"Use CloudWatch Logs Insights"},{"location":"AWS/networking/BGP/","text":"BGP Exterior routing protocol Network routes (prefixes) are shared between mutually-configured peers No prefixes are automatically shared Select the \"best\" of multiple paths to the same destination TCP 179 Interior Gateway Protocol (IGP) - e.g. OSPF eBGP - exterior iBGP - interior; prefix learned from from iBGP not advertised further BGP Prefix BGP Prefix Preference Sequence (Abridged): * Highest Weight (Default for local 32768 (max 16 bit)) \"Cisco feature, applied to the prefix on the router, not advertised to peers\" * Highest Local Preference (Default is 100) \"Like Weight, but advertised to iBGP peers\" * Shortest AS Path (Example: 65413 65412 i (i for iBGP)) \"AS Path prepending used to set preferred route on VGW side, passed to beyond peers\" * eBGP over iBGP * Lowest Metric (Example: 0) \"Not passed to beyond peers\" VGW - alway automatically share all prefixes (unlike real routers) ASN - autonomous system numbers 16-bit and 32 bit * 0 - 65535 * 65536 - 4294967295 Public and private ASNs * Public ASNs are controlled by the IANA * 16-bit private range: 64512-65534 * 32-bit private range: 420000000+","title":"BGP"},{"location":"AWS/networking/BGP/#bgp","text":"Exterior routing protocol Network routes (prefixes) are shared between mutually-configured peers No prefixes are automatically shared Select the \"best\" of multiple paths to the same destination TCP 179 Interior Gateway Protocol (IGP) - e.g. OSPF eBGP - exterior iBGP - interior; prefix learned from from iBGP not advertised further","title":"BGP"},{"location":"AWS/networking/BGP/#bgp-prefix","text":"BGP Prefix Preference Sequence (Abridged): * Highest Weight (Default for local 32768 (max 16 bit)) \"Cisco feature, applied to the prefix on the router, not advertised to peers\" * Highest Local Preference (Default is 100) \"Like Weight, but advertised to iBGP peers\" * Shortest AS Path (Example: 65413 65412 i (i for iBGP)) \"AS Path prepending used to set preferred route on VGW side, passed to beyond peers\" * eBGP over iBGP * Lowest Metric (Example: 0) \"Not passed to beyond peers\" VGW - alway automatically share all prefixes (unlike real routers)","title":"BGP Prefix"},{"location":"AWS/networking/BGP/#asn-autonomous-system-numbers","text":"16-bit and 32 bit * 0 - 65535 * 65536 - 4294967295 Public and private ASNs * Public ASNs are controlled by the IANA * 16-bit private range: 64512-65534 * 32-bit private range: 420000000+","title":"ASN - autonomous system numbers"},{"location":"AWS/networking/CloudFront/","text":"CloudFront CDN, operates on Level4 Key Concepts Origin Server : Location of stored content for distributions to use. (S3, EC2, ALB, Lambda, MediaStore, CloudFront Origin Group) Distribution : Configuration telling CloudFront which origin server to use. Domain Name : CloudFront-assigned DNS for use in web requests. Edge Location (aka Point of presents): Geographically-dispersed servers that cache your files. Cache Missed Cache flow i.Edge Location Checked, ii. Regional Edge Checked iii. Origin Fetch Cache behaviors Default cache behavior - required for all distributions. Additional cache behaviors to define responses based on requests . (Example: Match for specific path pattern, like .png, and send to S3 origin) Cache behaviors use one specific origin based on settings. If you want to use multiple origins , you must have a matching number of cache behaviors *. Cache behaviors settings Precedence - the order of precedence for evaluating behaviors (which cache behavior preferred over another) Path Pattern - request path pattern the cache behavior will apply to. Example: *.gif Origin/Origin Groups - which origin/origin group the request are routed to Viewer Protocol Policy - which protocol policy you want to use for edge locations Restrict Viewer Access - require use of signed URLs or signed cookies TLS/HTTP implementation supports CNAMEs supports SNI Native ACM integration. Certs MUST be issued in ua-east-1 Viewer protocol - protocol user uses Origin protocol - cloudfront uses to connection to origins Status codes Always cached : 404, 414: Request-URL too large, 500, 501: Not Implemented, 502: Bad Gateway, 503: Service Unavailable, 504: Gateway Timeout Conditionally Cached : 400: Bad Request, 403, 405: Method Not Allowed, 412: Precondition Failed, 415: Unsupported Media Type Origins S3, EC2, ALB, Lambda, MediaStore, CloudFront Origin Group Origin access control (OAC) is the successor to an origin access identity (OAI) OAC supports actions: Using all S3 buckets in all AWS Regions, AWS KMS (SSE-KMS), Dynamic requests (PUT and DELETE) to S3 OAC: * Requires an Amazon S3 bucket origin that is NOT a website endpoint. * Grant OAC access to bucket via the Amazon S3 bucket policy. * Must grant access to OAC manually for distribution. * OAC is a type of identity that is NOT a role or user. Private viewer Signed URLs (take precedence) Signed Cookies CloudFront Field-Level Encryption Additional layer of security for protecting data throughout a process/request within CloudFront. help to protect sensitive data within CloudFront Information is encrypted at the edge and remains encrypted throughout completely separate from the actual HTTP/S tunnels Uses a public and private key pair for all encryption/decryption Configuration steps Cet your public and private key pair Create field-level encryption config Create field-level encryption profile Link to a cache behavior CloudFront Geo Blocking CloudFront Built-it Feature Restricts all access to files associated with a distribution. Restricts at a country level. allow list deny list returns 403. Third-party database leveraged that offers 99.8% accuracy Third-Party Service Restricts access to a subset of files. Usually allows for finer granularity than country-level restrictions. Restriction requirements that do not require entire countries Invalidating and Expiring Files within Amazon CloudFront Control Caching Durations By default(24 hours), CloudFront servers a cached file until the specified cache duration. After expiration, origin fetch occurs to update cached data. Cache hit (returns 304 Not Modified): CloudFront has the lates version for immediate return. Cache miss (returns 200 Ok): CloudFront retrieves lates version from origin. Cache TTL Header Examples 12-Hours TTL Using Cache-Control Key: Cache Control Value: maxage:43200 Specific Data Using Expires Key: Expires Value: Mon, 1 Jan 2024 06:00:00 GMT Invalidate Files Remove cached files to force origin fetch before specified object TTL. Two options: - Invalidation: Specify the path you want to invalidate (URL/files/*.png) - Versioning: Provide version names in files for best control (acloudguru_v3.png) CloudFront Lambdas Lambda at Edge lives within lambdas an extension of the AWS Lambda service must be in us-east-1 executes at edge locations - precess request closer to viewers for reduced latency allows for interception of request and responses for customizations node.js or python scale automatically up to thousand request per sec CloudFront Functions Overview native functions that live entirely within CloudFront Can only be written in JavaScript Extremely similar use cases to Lambda@Edge Leverage CloudFront Functions and Lambda@Edge if desired Best latency - sub-millisecond startup and millions of request per second","title":"CloudFront"},{"location":"AWS/networking/CloudFront/#cloudfront","text":"CDN, operates on Level4","title":"CloudFront"},{"location":"AWS/networking/CloudFront/#key-concepts","text":"Origin Server : Location of stored content for distributions to use. (S3, EC2, ALB, Lambda, MediaStore, CloudFront Origin Group) Distribution : Configuration telling CloudFront which origin server to use. Domain Name : CloudFront-assigned DNS for use in web requests. Edge Location (aka Point of presents): Geographically-dispersed servers that cache your files.","title":"Key Concepts"},{"location":"AWS/networking/CloudFront/#cache","text":"","title":"Cache"},{"location":"AWS/networking/CloudFront/#missed-cache-flow","text":"i.Edge Location Checked, ii. Regional Edge Checked iii. Origin Fetch","title":"Missed Cache flow"},{"location":"AWS/networking/CloudFront/#cache-behaviors","text":"Default cache behavior - required for all distributions. Additional cache behaviors to define responses based on requests . (Example: Match for specific path pattern, like .png, and send to S3 origin) Cache behaviors use one specific origin based on settings. If you want to use multiple origins , you must have a matching number of cache behaviors *.","title":"Cache behaviors"},{"location":"AWS/networking/CloudFront/#cache-behaviors-settings","text":"Precedence - the order of precedence for evaluating behaviors (which cache behavior preferred over another) Path Pattern - request path pattern the cache behavior will apply to. Example: *.gif Origin/Origin Groups - which origin/origin group the request are routed to Viewer Protocol Policy - which protocol policy you want to use for edge locations Restrict Viewer Access - require use of signed URLs or signed cookies","title":"Cache behaviors settings"},{"location":"AWS/networking/CloudFront/#tlshttp-implementation","text":"supports CNAMEs supports SNI Native ACM integration. Certs MUST be issued in ua-east-1 Viewer protocol - protocol user uses Origin protocol - cloudfront uses to connection to origins","title":"TLS/HTTP implementation"},{"location":"AWS/networking/CloudFront/#status-codes","text":"Always cached : 404, 414: Request-URL too large, 500, 501: Not Implemented, 502: Bad Gateway, 503: Service Unavailable, 504: Gateway Timeout Conditionally Cached : 400: Bad Request, 403, 405: Method Not Allowed, 412: Precondition Failed, 415: Unsupported Media Type","title":"Status codes"},{"location":"AWS/networking/CloudFront/#origins","text":"S3, EC2, ALB, Lambda, MediaStore, CloudFront Origin Group Origin access control (OAC) is the successor to an origin access identity (OAI) OAC supports actions: Using all S3 buckets in all AWS Regions, AWS KMS (SSE-KMS), Dynamic requests (PUT and DELETE) to S3 OAC: * Requires an Amazon S3 bucket origin that is NOT a website endpoint. * Grant OAC access to bucket via the Amazon S3 bucket policy. * Must grant access to OAC manually for distribution. * OAC is a type of identity that is NOT a role or user.","title":"Origins"},{"location":"AWS/networking/CloudFront/#private-viewer","text":"Signed URLs (take precedence) Signed Cookies","title":"Private viewer"},{"location":"AWS/networking/CloudFront/#cloudfront-field-level-encryption","text":"Additional layer of security for protecting data throughout a process/request within CloudFront. help to protect sensitive data within CloudFront Information is encrypted at the edge and remains encrypted throughout completely separate from the actual HTTP/S tunnels Uses a public and private key pair for all encryption/decryption","title":"CloudFront Field-Level Encryption"},{"location":"AWS/networking/CloudFront/#configuration-steps","text":"Cet your public and private key pair Create field-level encryption config Create field-level encryption profile Link to a cache behavior","title":"Configuration steps"},{"location":"AWS/networking/CloudFront/#cloudfront-geo-blocking","text":"","title":"CloudFront Geo Blocking"},{"location":"AWS/networking/CloudFront/#cloudfront-built-it-feature","text":"Restricts all access to files associated with a distribution. Restricts at a country level. allow list deny list returns 403. Third-party database leveraged that offers 99.8% accuracy","title":"CloudFront Built-it Feature"},{"location":"AWS/networking/CloudFront/#third-party-service","text":"Restricts access to a subset of files. Usually allows for finer granularity than country-level restrictions. Restriction requirements that do not require entire countries","title":"Third-Party Service"},{"location":"AWS/networking/CloudFront/#invalidating-and-expiring-files-within-amazon-cloudfront","text":"","title":"Invalidating and Expiring Files within Amazon CloudFront"},{"location":"AWS/networking/CloudFront/#control-caching-durations","text":"By default(24 hours), CloudFront servers a cached file until the specified cache duration. After expiration, origin fetch occurs to update cached data. Cache hit (returns 304 Not Modified): CloudFront has the lates version for immediate return. Cache miss (returns 200 Ok): CloudFront retrieves lates version from origin.","title":"Control Caching Durations"},{"location":"AWS/networking/CloudFront/#cache-ttl-header-examples","text":"12-Hours TTL Using Cache-Control Key: Cache Control Value: maxage:43200 Specific Data Using Expires Key: Expires Value: Mon, 1 Jan 2024 06:00:00 GMT","title":"Cache TTL Header Examples"},{"location":"AWS/networking/CloudFront/#invalidate-files","text":"Remove cached files to force origin fetch before specified object TTL. Two options: - Invalidation: Specify the path you want to invalidate (URL/files/*.png) - Versioning: Provide version names in files for best control (acloudguru_v3.png)","title":"Invalidate Files"},{"location":"AWS/networking/CloudFront/#cloudfront-lambdas","text":"","title":"CloudFront Lambdas"},{"location":"AWS/networking/CloudFront/#lambda-at-edge","text":"lives within lambdas an extension of the AWS Lambda service must be in us-east-1 executes at edge locations - precess request closer to viewers for reduced latency allows for interception of request and responses for customizations node.js or python scale automatically up to thousand request per sec","title":"Lambda at Edge"},{"location":"AWS/networking/CloudFront/#cloudfront-functions-overview","text":"native functions that live entirely within CloudFront Can only be written in JavaScript Extremely similar use cases to Lambda@Edge Leverage CloudFront Functions and Lambda@Edge if desired Best latency - sub-millisecond startup and millions of request per second","title":"CloudFront Functions Overview"},{"location":"AWS/networking/ELB/","text":"Core ELB Concepts (non-GWLB) Each ELB works with a single VPC. ELBs must be configured to work in at least one AZ. Use at least two AZ to support HA. Requests are sent to AWS-assigned FQDN Public if ELB scheme is internet-facing Private if ELB scheme is internal Listener: Processes that check for connection requests to specific protocols and ports. Matching traffic forwarded to targets. Settings and options vary by ELB type. Targe groups: Types(cannot be changed): EC2 Instances IP addresses A single Lambda function Protocols: ALB HTTP HTTPS NLB TCP TLS UDP TCP_UDP GWLB GENEVE Worker nodes created in each configured subnet . Each node accessed by IP address. (private IPs for internal, public for internet-facing) Requests to ELB will be evenly distributed across all nodes. Nodes determine which targets receive requests. Routing algorithms: Round robin Least outstanding requests Hash (NLB) Cross-Zone Load Balancing if disabled: ELB nodes only forward to targets in the same AZ. (disabled by default for: NLB, GWLB, CLB created using GLI/API) if enabled: ELB nodes may forward to targets in any AZ. NO data transfer charges for ALB and CLB ELB Access Logging Disabled by default Captures information about requests Records stored in S3 bucket Supported for: ALB CLB NLB for TLS protocol only Network load balancer","title":"Core ELB Concepts (non-GWLB)"},{"location":"AWS/networking/ELB/#core-elb-concepts-non-gwlb","text":"Each ELB works with a single VPC. ELBs must be configured to work in at least one AZ. Use at least two AZ to support HA. Requests are sent to AWS-assigned FQDN Public if ELB scheme is internet-facing Private if ELB scheme is internal Listener: Processes that check for connection requests to specific protocols and ports. Matching traffic forwarded to targets. Settings and options vary by ELB type. Targe groups: Types(cannot be changed): EC2 Instances IP addresses A single Lambda function Protocols: ALB HTTP HTTPS NLB TCP TLS UDP TCP_UDP GWLB GENEVE Worker nodes created in each configured subnet . Each node accessed by IP address. (private IPs for internal, public for internet-facing) Requests to ELB will be evenly distributed across all nodes. Nodes determine which targets receive requests. Routing algorithms: Round robin Least outstanding requests Hash (NLB) Cross-Zone Load Balancing if disabled: ELB nodes only forward to targets in the same AZ. (disabled by default for: NLB, GWLB, CLB created using GLI/API) if enabled: ELB nodes may forward to targets in any AZ. NO data transfer charges for ALB and CLB ELB Access Logging Disabled by default Captures information about requests Records stored in S3 bucket Supported for: ALB CLB NLB for TLS protocol only","title":"Core ELB Concepts (non-GWLB)"},{"location":"AWS/networking/ELB/#network-load-balancer","text":"","title":"Network load balancer"},{"location":"AWS/networking/Fundamental/","text":"Availability Zone ID - AZ ID (e.g. eu-west-1a) - a unique identifier for an Availability Zone within a Region. Public Services - AWS IAM; AMAZON SQS; AMAZON S3 Private Services - VPC, VPC Endpoints HA (Highly available) - your system will almost maintain uptime; guarantees essential services uptime, NOT full functionality FT (Fault Tolerant) - your system can continue to operate regardless of disruption VPC Virtual Private Cloud (VPC) - lets you provision a logically isolated section of the AWS Cloud: * Soft limit: 5 VPC in single region in a single account * Best Practice is a multi-account strategy. (not VPC) * Don't use default VPC for production workloads VPC Router VPC Routers are highly available devices and it occupies the .1 addressing space on every subnet associated with our VPC. 5 reserved IPs in each subnet: * 10.0.64.0 - network address * 10.0.64.1 - VPC router * 10.0.64.2 - DNS Server * 10.0.64.3 - Reserved by AWS for future use. * 10.0.71.255/21 - Network broadcast address (AWS doesn't support broadcast) Route Tables You can influence the routing for your VPC by editing the main route table or creating custom route tables for each of your subnets. Internet Gateway(IGW) public to private IP address mapping Resource must have a public IP. There must be a router entry for the IGW. Egress-Only Internet Gateway acts like a NAT gateway for IPv6 compute Nat Instance (legacy) Lives in a public subnet; Non-public subnets use it as a default rule Disable check source/destination flag NAT Gateway up to 45 GBit/s Zone-independent architectures Need one per AZ Private NAT Gateway No elastic IPs Private traffic is routed from a private NAT gateway through a VGW or TGW Use case: Non-routable IP Ranges (Overlapping) Transit gateway (TGW) Virtual Private Gateway(VGW) Works with AWS site to site VPN connections or AWS direct connect * Managed AWS Service * Acts as a router between your VPC and non-AWS-managed networks * Can be associated with multiple external connections * Can attach to only one VPC at a time Limitations: * VGW are NOT VPC transitive * VGW VPN throughput capped at 1.25 Gbps over ALL VPN connections * VGW Single Tunnel Use * VPN Tunnel Supports Single SA Pair * AWS S2S VPN only supports the IPSec protocol Only private ASNs may be used for VGW configuration. * AWS default ASN is 64512 in all regions * 7224 in most regions prior to June 30, 2018 Site-to-Site VPN Tunnels are established by traffic flowing from on-prem to AWS (by default) AWS VPN tunnels can only support a single pair of IPSec security associations. AWS creates two (Active/passive) tunnel endpoints in different AZs per VPN connection Accelerated Site-to-Site VPN Uses AWS Global Accelerator (GA) to route traffic Uses congestion-free AWS global network to route traffic to the best endpoint NOT compatible with VGW or CX (Cross Connect), must be attached to TGW Offers the absolute best application network performance AWS creates and manages two accelerators for you (One for each VPN tunnel) VPN Anywhere AWS-managed version of well-known OpenVPN The endpoint gets added to only one VPC VPN CloudHub Hub-and-Spoke VPN AWS solution for IPsec VPN tunnels. Up to 10 customer gateways. Each cgw must have a unique BGP ASN! VPN Gateway deployed via AWS VPN CloudHub with no VPC attachment (can also be attached to VPC). Customer Gateway Device (CGD) Represents a customer device in AWS configuration. ENI (Elastic Network Interface) EIP (Elastic IP) 5 IPs limit IP 169.254.0.0/16 - link-local non-redistributed addresses Classless Inter-Domain Routing (CIDR) How to cal the number of available IPs e.g.: * /27 subnet = 2^(32-27)-5(reserved IPs) = 27 * /24 subnet = 2^(32-24)-5 = 2^8-5 = 256-5 = 251 IPv4 va IPv6 IPv4: * required for all VPCs. * You have a choice between a size of /16 to /28 CIDR block. * You can choose the private CIDR block. * Public and private addresses are a thing. IPv6: * Fixed size of /56 CIDR blocks (when using AWS-assigned ranges). * Using AWS-owned ranges means you cannot select the assigned CIDR block. * You associate a /64 block to each subnet. * No difference in public and private addresses. Security is controlled with routing and security. Address spaces are binary! Subnets Public, Private or VPN only Private ranges: * 192.168.0.0 - 192.168.255.255 (offers 1 /16 range) * 172.16.0.0 - 172.31.255.255 (offers 2 /16 ranges) * 10.0.0.0 - 10.255.255.255 (offers 256 /16 ranges) Ephemeral ports: * Linux 32768-61000 * Windows 49152-65535 * AWS NAT Gateway 1024-65535 Network Access Control Lists(NACLs) Applied at the subnet level; Stateless; Can specify both allow and deny rules; Traffic is processed before it enters or leaves the subnet. Security Groups also works like an IP list Applied at the resource level (ENI) Stateful; Can specify allow rules, but not deny rules; Scoped to VPC; Can be applied to: * EC2 instances * RDS instances * Elastic load balancers * etc Supports self-referencing feature VPC Endpoints (VPCE) VPC-EID: identifier of the endpoint (vpce-xyz) pl-id s3 (prefix list) ???? limits: * Endpoints are a reginal service * Endpoints are not extendable across VPC boundaries * DNS resolution is Required * Default VPC Endpoint Policy is unrestricted Interface Endpoints ENI with a private IP address. Powered by AWS PrivateLink. Services: API Gateway, Athena, Kinesis, etc. Gateway Endpoints Gateway specify as a target in a route table Gateway endpoints do NOT support hybrid connectivity! Leverage interface endpoints for this. Services: S3, DynamoDB Free?? VPC Peering pcx-abc limits: * no transitive peering allowed * You can reference security group IDs across peering connections only if the VPCs are in the same AWS Region! VPC Flow Logs Flow logs may be defined for: VPCs Subnets ENIs Flow log definitions apply to all ENIs within scope. Traffic will be logged separately for each definition. Flow log data may be sent to: CloudWatch log group 1 log stream per ENI S3 bucket 1 log file object per-publication limitations: * Does not capture all IP traffic * EC2 DNS requests to Route 53 * Amazon Windows license activation * Instance metadata * Amazon Time Sync Service * DHCP traffic * Default VPC router (only source and destination ENI) * Endpoint services created by AWS customers * Does not capture application data (operates at l3 and l4 of OSI) Flow logs formats version 2: version account-id interface-id srcaddr dstaddr srcport dstport protocol packets bytes start end action log-status version 3: vpc-id subnet-id instance-id tcp-flags type pkt-srcaddr pkt-dstaddr version 4: region az-id sublocation-type sublocation-id version 5: pkt-src-aws-service pkt-dst-aws-service flow-direction traffic-path VPC Traffic Mirroring Reachability Analyzer (AWS Network Manager) VPC tool for performing connectivity testing between a source and destination involving your AWS VPC resources. Create paths for analysis. BYOIP Import your IP space into AWS and then grant AWS the ability to advertise via BGP using their ASNs of 16509 and 14618. Use cases: * Hardcoded Dependencies * Allow List * Compliance Requirements * On-Premises IPv6 Policy * IP Reputations limits: * the most specific IPv4 address range that you can bring is a /24 * the most specific publicly advertised IPv6 range that you can bring is /48. Non-publicly advertised IPv6 range can be /56. * Each address range can only belong to one Region at a time. * You are allowed five BYOIP ranges per Region per AWS account. * No support for Local Zones, Wavelength Zone, AWS Outposts. Bastion host (Jump Server) Server running at the edge of a network Bastion and their firewalls can be leveraged to perform port forwarding Bastion hosts should be locked down to a small set of allowed users. AWS Systems Manager - Session Manager Agent-based (SSM agent) management of all managed instances, edge devices, and managed on-premises VM. No more SSH or RDP required, port forwarding still supported. Centralized access control via IAM policies. HPC (high performance connectivity) Enhanced networking uses single root I/O virtualization Clustering placement groups packs instances close together Enabling Enhanced Networking - jumbo frames Instance Types: * Elastic Network Adapter (ENA) - up to 100 Gbps * Intel 82599 Virtual Function (VF) interface - up to 10 Gbps AWS Cloud WAN Helps centrally build, manage, and monitor global networks. Core network policy: Declarative document applied to core network capturing intent. Defines all segments, regional routing, and attachments. Attachments: Any connection or resource added to your core network. VPCs, VPNs, and TGW attachments. Core network edge: Regional connection point managed by AWS in each of your Regions. AWS Transit Gateway behind scenes. Network segments: Dedicated routing domains. Segment actions: Define how routing works between segments. Global Network Single, private network serving as high-level container for network objects Core Network Part of it is manged by AWS, including regional connections (VPNs, VPC, TGWs) GuardDuty GuardDuty is a security service that continuously monitors your AWS accounts for malicious activity. It uses threat intelligence and machine learning to analyze various data sources like logs and network traffic. GuardDuty helps you identify and respond to potential threats in your AWS environment. AWS Shield AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield Standard AWS Shield Standard primarily mitigates attacks that occur at Layer 3 of the OSI model. AWS Web Application Firewall (WAF) AWS Web Application Firewall mitigates attacks that primarily occur at Layer 7 (the Application Layer) of the OSI model. AWS Config AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. AWS Firewall Manager AWS Firewall Manager enforces deployment of Web Application Firewall ACLs, fundamental","title":"Fundamental"},{"location":"AWS/networking/Fundamental/#vpc","text":"Virtual Private Cloud (VPC) - lets you provision a logically isolated section of the AWS Cloud: * Soft limit: 5 VPC in single region in a single account * Best Practice is a multi-account strategy. (not VPC) * Don't use default VPC for production workloads","title":"VPC"},{"location":"AWS/networking/Fundamental/#vpc-router","text":"VPC Routers are highly available devices and it occupies the .1 addressing space on every subnet associated with our VPC. 5 reserved IPs in each subnet: * 10.0.64.0 - network address * 10.0.64.1 - VPC router * 10.0.64.2 - DNS Server * 10.0.64.3 - Reserved by AWS for future use. * 10.0.71.255/21 - Network broadcast address (AWS doesn't support broadcast)","title":"VPC Router"},{"location":"AWS/networking/Fundamental/#route-tables","text":"You can influence the routing for your VPC by editing the main route table or creating custom route tables for each of your subnets.","title":"Route Tables"},{"location":"AWS/networking/Fundamental/#internet-gatewayigw","text":"public to private IP address mapping Resource must have a public IP. There must be a router entry for the IGW.","title":"Internet Gateway(IGW)"},{"location":"AWS/networking/Fundamental/#egress-only-internet-gateway","text":"acts like a NAT gateway for IPv6 compute","title":"Egress-Only Internet Gateway"},{"location":"AWS/networking/Fundamental/#nat-instance-legacy","text":"Lives in a public subnet; Non-public subnets use it as a default rule Disable check source/destination flag","title":"Nat Instance (legacy)"},{"location":"AWS/networking/Fundamental/#nat-gateway","text":"up to 45 GBit/s Zone-independent architectures Need one per AZ","title":"NAT Gateway"},{"location":"AWS/networking/Fundamental/#private-nat-gateway","text":"No elastic IPs Private traffic is routed from a private NAT gateway through a VGW or TGW Use case: Non-routable IP Ranges (Overlapping)","title":"Private NAT Gateway"},{"location":"AWS/networking/Fundamental/#transit-gateway-tgw","text":"","title":"Transit gateway (TGW)"},{"location":"AWS/networking/Fundamental/#virtual-private-gatewayvgw","text":"Works with AWS site to site VPN connections or AWS direct connect * Managed AWS Service * Acts as a router between your VPC and non-AWS-managed networks * Can be associated with multiple external connections * Can attach to only one VPC at a time Limitations: * VGW are NOT VPC transitive * VGW VPN throughput capped at 1.25 Gbps over ALL VPN connections * VGW Single Tunnel Use * VPN Tunnel Supports Single SA Pair * AWS S2S VPN only supports the IPSec protocol Only private ASNs may be used for VGW configuration. * AWS default ASN is 64512 in all regions * 7224 in most regions prior to June 30, 2018","title":"Virtual Private Gateway(VGW)"},{"location":"AWS/networking/Fundamental/#site-to-site-vpn","text":"Tunnels are established by traffic flowing from on-prem to AWS (by default) AWS VPN tunnels can only support a single pair of IPSec security associations. AWS creates two (Active/passive) tunnel endpoints in different AZs per VPN connection","title":"Site-to-Site VPN"},{"location":"AWS/networking/Fundamental/#accelerated-site-to-site-vpn","text":"Uses AWS Global Accelerator (GA) to route traffic Uses congestion-free AWS global network to route traffic to the best endpoint NOT compatible with VGW or CX (Cross Connect), must be attached to TGW Offers the absolute best application network performance AWS creates and manages two accelerators for you (One for each VPN tunnel)","title":"Accelerated Site-to-Site VPN"},{"location":"AWS/networking/Fundamental/#vpn-anywhere","text":"AWS-managed version of well-known OpenVPN The endpoint gets added to only one VPC","title":"VPN Anywhere"},{"location":"AWS/networking/Fundamental/#vpn-cloudhub","text":"Hub-and-Spoke VPN AWS solution for IPsec VPN tunnels. Up to 10 customer gateways. Each cgw must have a unique BGP ASN! VPN Gateway deployed via AWS VPN CloudHub with no VPC attachment (can also be attached to VPC).","title":"VPN CloudHub"},{"location":"AWS/networking/Fundamental/#customer-gateway-device-cgd","text":"Represents a customer device in AWS configuration.","title":"Customer Gateway Device (CGD)"},{"location":"AWS/networking/Fundamental/#eni-elastic-network-interface","text":"","title":"ENI (Elastic Network Interface)"},{"location":"AWS/networking/Fundamental/#eip-elastic-ip","text":"5 IPs limit","title":"EIP (Elastic IP)"},{"location":"AWS/networking/Fundamental/#ip","text":"169.254.0.0/16 - link-local non-redistributed addresses","title":"IP"},{"location":"AWS/networking/Fundamental/#classless-inter-domain-routing-cidr","text":"How to cal the number of available IPs e.g.: * /27 subnet = 2^(32-27)-5(reserved IPs) = 27 * /24 subnet = 2^(32-24)-5 = 2^8-5 = 256-5 = 251","title":"Classless Inter-Domain Routing (CIDR)"},{"location":"AWS/networking/Fundamental/#ipv4-va-ipv6","text":"IPv4: * required for all VPCs. * You have a choice between a size of /16 to /28 CIDR block. * You can choose the private CIDR block. * Public and private addresses are a thing. IPv6: * Fixed size of /56 CIDR blocks (when using AWS-assigned ranges). * Using AWS-owned ranges means you cannot select the assigned CIDR block. * You associate a /64 block to each subnet. * No difference in public and private addresses. Security is controlled with routing and security. Address spaces are binary!","title":"IPv4 va IPv6"},{"location":"AWS/networking/Fundamental/#subnets","text":"Public, Private or VPN only Private ranges: * 192.168.0.0 - 192.168.255.255 (offers 1 /16 range) * 172.16.0.0 - 172.31.255.255 (offers 2 /16 ranges) * 10.0.0.0 - 10.255.255.255 (offers 256 /16 ranges) Ephemeral ports: * Linux 32768-61000 * Windows 49152-65535 * AWS NAT Gateway 1024-65535","title":"Subnets"},{"location":"AWS/networking/Fundamental/#network-access-control-listsnacls","text":"Applied at the subnet level; Stateless; Can specify both allow and deny rules; Traffic is processed before it enters or leaves the subnet.","title":"Network Access Control Lists(NACLs)"},{"location":"AWS/networking/Fundamental/#security-groups","text":"also works like an IP list Applied at the resource level (ENI) Stateful; Can specify allow rules, but not deny rules; Scoped to VPC; Can be applied to: * EC2 instances * RDS instances * Elastic load balancers * etc Supports self-referencing feature","title":"Security Groups"},{"location":"AWS/networking/Fundamental/#vpc-endpoints-vpce","text":"VPC-EID: identifier of the endpoint (vpce-xyz) pl-id s3 (prefix list) ???? limits: * Endpoints are a reginal service * Endpoints are not extendable across VPC boundaries * DNS resolution is Required * Default VPC Endpoint Policy is unrestricted","title":"VPC Endpoints (VPCE)"},{"location":"AWS/networking/Fundamental/#interface-endpoints","text":"ENI with a private IP address. Powered by AWS PrivateLink. Services: API Gateway, Athena, Kinesis, etc.","title":"Interface Endpoints"},{"location":"AWS/networking/Fundamental/#gateway-endpoints","text":"Gateway specify as a target in a route table Gateway endpoints do NOT support hybrid connectivity! Leverage interface endpoints for this. Services: S3, DynamoDB Free??","title":"Gateway Endpoints"},{"location":"AWS/networking/Fundamental/#vpc-peering","text":"pcx-abc limits: * no transitive peering allowed * You can reference security group IDs across peering connections only if the VPCs are in the same AWS Region!","title":"VPC Peering"},{"location":"AWS/networking/Fundamental/#vpc-flow-logs","text":"Flow logs may be defined for: VPCs Subnets ENIs Flow log definitions apply to all ENIs within scope. Traffic will be logged separately for each definition. Flow log data may be sent to: CloudWatch log group 1 log stream per ENI S3 bucket 1 log file object per-publication limitations: * Does not capture all IP traffic * EC2 DNS requests to Route 53 * Amazon Windows license activation * Instance metadata * Amazon Time Sync Service * DHCP traffic * Default VPC router (only source and destination ENI) * Endpoint services created by AWS customers * Does not capture application data (operates at l3 and l4 of OSI)","title":"VPC Flow Logs"},{"location":"AWS/networking/Fundamental/#flow-logs-formats","text":"version 2: version account-id interface-id srcaddr dstaddr srcport dstport protocol packets bytes start end action log-status version 3: vpc-id subnet-id instance-id tcp-flags type pkt-srcaddr pkt-dstaddr version 4: region az-id sublocation-type sublocation-id version 5: pkt-src-aws-service pkt-dst-aws-service flow-direction traffic-path","title":"Flow logs formats"},{"location":"AWS/networking/Fundamental/#vpc-traffic-mirroring","text":"","title":"VPC Traffic Mirroring"},{"location":"AWS/networking/Fundamental/#reachability-analyzer-aws-network-manager","text":"VPC tool for performing connectivity testing between a source and destination involving your AWS VPC resources. Create paths for analysis.","title":"Reachability Analyzer (AWS Network Manager)"},{"location":"AWS/networking/Fundamental/#byoip","text":"Import your IP space into AWS and then grant AWS the ability to advertise via BGP using their ASNs of 16509 and 14618. Use cases: * Hardcoded Dependencies * Allow List * Compliance Requirements * On-Premises IPv6 Policy * IP Reputations limits: * the most specific IPv4 address range that you can bring is a /24 * the most specific publicly advertised IPv6 range that you can bring is /48. Non-publicly advertised IPv6 range can be /56. * Each address range can only belong to one Region at a time. * You are allowed five BYOIP ranges per Region per AWS account. * No support for Local Zones, Wavelength Zone, AWS Outposts.","title":"BYOIP"},{"location":"AWS/networking/Fundamental/#bastion-host-jump-server","text":"Server running at the edge of a network Bastion and their firewalls can be leveraged to perform port forwarding Bastion hosts should be locked down to a small set of allowed users.","title":"Bastion host (Jump Server)"},{"location":"AWS/networking/Fundamental/#aws-systems-manager-session-manager","text":"Agent-based (SSM agent) management of all managed instances, edge devices, and managed on-premises VM. No more SSH or RDP required, port forwarding still supported. Centralized access control via IAM policies.","title":"AWS Systems Manager - Session Manager"},{"location":"AWS/networking/Fundamental/#hpc-high-performance-connectivity","text":"Enhanced networking uses single root I/O virtualization Clustering placement groups packs instances close together Enabling Enhanced Networking - jumbo frames Instance Types: * Elastic Network Adapter (ENA) - up to 100 Gbps * Intel 82599 Virtual Function (VF) interface - up to 10 Gbps","title":"HPC (high performance connectivity)"},{"location":"AWS/networking/Fundamental/#aws-cloud-wan","text":"Helps centrally build, manage, and monitor global networks. Core network policy: Declarative document applied to core network capturing intent. Defines all segments, regional routing, and attachments. Attachments: Any connection or resource added to your core network. VPCs, VPNs, and TGW attachments. Core network edge: Regional connection point managed by AWS in each of your Regions. AWS Transit Gateway behind scenes. Network segments: Dedicated routing domains. Segment actions: Define how routing works between segments.","title":"AWS Cloud WAN"},{"location":"AWS/networking/Fundamental/#global-network","text":"Single, private network serving as high-level container for network objects","title":"Global Network"},{"location":"AWS/networking/Fundamental/#core-network","text":"Part of it is manged by AWS, including regional connections (VPNs, VPC, TGWs)","title":"Core Network"},{"location":"AWS/networking/Fundamental/#guardduty","text":"GuardDuty is a security service that continuously monitors your AWS accounts for malicious activity. It uses threat intelligence and machine learning to analyze various data sources like logs and network traffic. GuardDuty helps you identify and respond to potential threats in your AWS environment.","title":"GuardDuty"},{"location":"AWS/networking/Fundamental/#aws-shield","text":"AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.","title":"AWS Shield"},{"location":"AWS/networking/Fundamental/#aws-shield-standard","text":"AWS Shield Standard primarily mitigates attacks that occur at Layer 3 of the OSI model.","title":"AWS Shield Standard"},{"location":"AWS/networking/Fundamental/#aws-web-application-firewall-waf","text":"AWS Web Application Firewall mitigates attacks that primarily occur at Layer 7 (the Application Layer) of the OSI model.","title":"AWS Web Application Firewall (WAF)"},{"location":"AWS/networking/Fundamental/#aws-config","text":"AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.","title":"AWS Config"},{"location":"AWS/networking/Fundamental/#aws-firewall-manager","text":"AWS Firewall Manager enforces deployment of Web Application Firewall ACLs, fundamental","title":"AWS Firewall Manager"},{"location":"AWS/networking/Route53/","text":"Route 53 CNAME vs AWS Alias CNAME DNS record type Referred to as an \"alias\" Redirects to other DNS record names Returns FQDN to client Charge per-query Name cannot be re-used in zone Not used to represent zone apex(top) Alias Not a DNS record type AWS Route 53 extension Redirects to AWS service object FQDNs Returns IP address to client No query charges Can re-use name Can be used to represent zone apex","title":"Route 53"},{"location":"AWS/networking/Route53/#route-53","text":"","title":"Route 53"},{"location":"AWS/networking/Route53/#cname-vs-aws-alias","text":"CNAME DNS record type Referred to as an \"alias\" Redirects to other DNS record names Returns FQDN to client Charge per-query Name cannot be re-used in zone Not used to represent zone apex(top) Alias Not a DNS record type AWS Route 53 extension Redirects to AWS service object FQDNs Returns IP address to client No query charges Can re-use name Can be used to represent zone apex","title":"CNAME vs AWS Alias"},{"location":"C%23101/","text":"Links Tour-of-csharp Branches and Loops Collections Conditional operators Console Dotnet cli Exceptions Interfaces OOP strings and chars variables.md C# in Depth 4th edition - Jon Skeet C# and .Net versions Target framework version C# language version default .NET 6.x C# 10 .NET 5.x C# 9.0 .NET Core 3.x C# 8.0 .NET Core 2.x C# 7.3 .NET Standard 2.1 C# 8.0 .NET Standard 2.0 C# 7.3 .NET Standard 1.x C# 7.3 .NET Framework all C# 7.3","title":"C#"},{"location":"C%23101/#links","text":"Tour-of-csharp Branches and Loops Collections Conditional operators Console Dotnet cli Exceptions Interfaces OOP strings and chars variables.md C# in Depth 4th edition - Jon Skeet","title":"Links"},{"location":"C%23101/#c-and-net-versions","text":"Target framework version C# language version default .NET 6.x C# 10 .NET 5.x C# 9.0 .NET Core 3.x C# 8.0 .NET Core 2.x C# 7.3 .NET Standard 2.1 C# 8.0 .NET Standard 2.0 C# 7.3 .NET Standard 1.x C# 7.3 .NET Framework all C# 7.3","title":"C# and .Net versions"},{"location":"C%23101/OOP/","text":"namespace Acme.Collections; public class Stack<T> { Entry _top; public void Push(T data) { _top = new Entry(_top, data); } public T Pop() { if (_top == null) { throw new InvalidOperationException(); } T result = _top.Data; _top = _top.Next; return result; } class Entry { public Entry Next { get; set; } public T Data { get; set; } public Entry(Entry next, T data) { Next = next; Data = data; } } } public class Stack: BaseStack { // fields private int _value; private string _message; private List<string> _stack; //constructs private Stack(int value) { _value = value; } public Stack(string[] values) { _stack = new List<string>(); foreach (value in values) { } _value = values[1]; } public Stack(string inputString, int value1) : this(value1) { } public Stack(decimal decInput): base() { } //Properties - wrapped methods public int GetValue { get { var temp = _value; return temp; } } public int SetValue { set { _value = value; } } public string Message1 { get { return _message; } set { _message = value; } } public string Value { get; set; } //Methods public string Message() { return _value.ToString(); } }","title":"OOP"},{"location":"C%23101/branches_loops/","text":"Branches == - tests for quality && - represents and || - represents or Conditional operators if classify = (input >= 0) ? \"nonnegative\" : \"negative\"; int a = 5; int b = 6; if (a + b > 10) Console.WriteLine(\"The answer is greater than 10.\"); int a = 5; int b = 3; if (a + b > 10) Console.WriteLine(\"The answer is greater than 10\"); else Console.WriteLine(\"The answer is not greater than 10\"); int a = 5; int b = 3; int c = 4; if ((a + b + c > 10) && (a == b)) { Console.WriteLine(\"The answer is greater than 10\"); Console.WriteLine(\"And the first number is equal to the second\"); } else { Console.WriteLine(\"The answer is not greater than 10\"); Console.WriteLine(\"Or the first number is not equal to the second\"); } case switch (i) { case 1: //top border case var value1 when value1 == n * 2: //intermediate border case var value2 when value2 == n * 4: //intermediate border2 Console.WriteLine(new String('+', tableWidth)); break; case var value when value == n: //text string (String1) Console.WriteLine('+' + new string(' ', n - 1) + textString + new string(' ', n - 1) + '+'); break; case var value when value > n * 2 && value < n * 4: //String2 Console.WriteLine('+' + String2(i % 2, tableWidth) + '+'); break; default: Console.WriteLine('+' + new string(' ', tableWidth - 2) + '+'); //blank strings (String1) break; } Loops while int counter = 0; while (counter < 10) { Console.WriteLine($\"Hello World! The counter is {counter}\"); counter++; } do int counter = 0; do { Console.WriteLine($\"Hello World! The counter is {counter}\"); counter++; } while (counter < 10); foraech var names = new List<string> { \"<name>\", \"Ana\", \"Felipe\" }; foreach (var name in names) { Console.WriteLine($\"Hello {name.ToUpper()}!\"); } for for (int i = 0; i <= 10; i += 2) { Console.WriteLine(i); }","title":"Branches"},{"location":"C%23101/branches_loops/#branches","text":"== - tests for quality && - represents and || - represents or Conditional operators","title":"Branches"},{"location":"C%23101/branches_loops/#if","text":"classify = (input >= 0) ? \"nonnegative\" : \"negative\"; int a = 5; int b = 6; if (a + b > 10) Console.WriteLine(\"The answer is greater than 10.\"); int a = 5; int b = 3; if (a + b > 10) Console.WriteLine(\"The answer is greater than 10\"); else Console.WriteLine(\"The answer is not greater than 10\"); int a = 5; int b = 3; int c = 4; if ((a + b + c > 10) && (a == b)) { Console.WriteLine(\"The answer is greater than 10\"); Console.WriteLine(\"And the first number is equal to the second\"); } else { Console.WriteLine(\"The answer is not greater than 10\"); Console.WriteLine(\"Or the first number is not equal to the second\"); }","title":"if"},{"location":"C%23101/branches_loops/#case","text":"switch (i) { case 1: //top border case var value1 when value1 == n * 2: //intermediate border case var value2 when value2 == n * 4: //intermediate border2 Console.WriteLine(new String('+', tableWidth)); break; case var value when value == n: //text string (String1) Console.WriteLine('+' + new string(' ', n - 1) + textString + new string(' ', n - 1) + '+'); break; case var value when value > n * 2 && value < n * 4: //String2 Console.WriteLine('+' + String2(i % 2, tableWidth) + '+'); break; default: Console.WriteLine('+' + new string(' ', tableWidth - 2) + '+'); //blank strings (String1) break; }","title":"case"},{"location":"C%23101/branches_loops/#loops","text":"","title":"Loops"},{"location":"C%23101/branches_loops/#while","text":"int counter = 0; while (counter < 10) { Console.WriteLine($\"Hello World! The counter is {counter}\"); counter++; }","title":"while"},{"location":"C%23101/branches_loops/#do","text":"int counter = 0; do { Console.WriteLine($\"Hello World! The counter is {counter}\"); counter++; } while (counter < 10);","title":"do"},{"location":"C%23101/branches_loops/#foraech","text":"var names = new List<string> { \"<name>\", \"Ana\", \"Felipe\" }; foreach (var name in names) { Console.WriteLine($\"Hello {name.ToUpper()}!\"); }","title":"foraech"},{"location":"C%23101/branches_loops/#for","text":"for (int i = 0; i <= 10; i += 2) { Console.WriteLine(i); }","title":"for"},{"location":"C%23101/collections/","text":"Array string[] ar = new string[10]; ar.Length - \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c int[,] ar2 = new int[10,20] // two-dimensional array 10 x 20 List\\ Class var names = new List<string> { \"<name>\", \"Ana\", \"Felipe\" }; foreach (var name in names) { Console.WriteLine($\"Hello {name.ToUpper()}!\"); } Console.WriteLine($\"The list has {names.Count} people in it\"); Console.WriteLine($\"My name is {names[0]}.\"); Properties l.Count; Methods l.Sort(); l.Add(\"newname\"); l.Remove(3); // remove first 3 l.RemoveAt(3); // remove element index 3 l.RemoveAll(x => x == 3); // remove all 3 l.RemoveRange(2,3) l.AddRange(new[] {88, 333, 1123123}); l.Clear(); ArrayList - \u0421\u043f\u0438\u0441\u043e\u043a \u0438\u0437 \u0440\u0430\u0437\u043d\u043e\u0442\u0438\u043f\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 var f = new ArrayList(); temp = (int)MyArryList[n]; LinkedList ```C# val ll = new LinkedList (); ll.AddLast(1); ll.AddFirst(3); var last = ll.AddLast(4); ll.AddBefore(last, 8);","title":"Array"},{"location":"C%23101/collections/#array","text":"string[] ar = new string[10]; ar.Length - \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c int[,] ar2 = new int[10,20] // two-dimensional array 10 x 20","title":"Array"},{"location":"C%23101/collections/#list-class","text":"var names = new List<string> { \"<name>\", \"Ana\", \"Felipe\" }; foreach (var name in names) { Console.WriteLine($\"Hello {name.ToUpper()}!\"); } Console.WriteLine($\"The list has {names.Count} people in it\"); Console.WriteLine($\"My name is {names[0]}.\");","title":"List\\ Class"},{"location":"C%23101/collections/#properties","text":"l.Count;","title":"Properties"},{"location":"C%23101/collections/#methods","text":"l.Sort(); l.Add(\"newname\"); l.Remove(3); // remove first 3 l.RemoveAt(3); // remove element index 3 l.RemoveAll(x => x == 3); // remove all 3 l.RemoveRange(2,3) l.AddRange(new[] {88, 333, 1123123}); l.Clear();","title":"Methods"},{"location":"C%23101/collections/#arraylist-","text":"var f = new ArrayList(); temp = (int)MyArryList[n];","title":"ArrayList - \u0421\u043f\u0438\u0441\u043e\u043a \u0438\u0437 \u0440\u0430\u0437\u043d\u043e\u0442\u0438\u043f\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432"},{"location":"C%23101/collections/#linkedlist","text":"```C# val ll = new LinkedList (); ll.AddLast(1); ll.AddFirst(3); var last = ll.AddLast(4); ll.AddBefore(last, 8);","title":"LinkedList"},{"location":"C%23101/conditional-operator/","text":"Conditional operators classify = (input >= 0) ? \"nonnegative\" : \"negative\"; string GetWeatherDisplay(double tempInCelsius) => tempInCelsius < 20.0 ? \"Cold.\" : \"Perfect!\"; Selection-statements DisplayMeasurement(-4); // Output: Measured value is -4; too low. DisplayMeasurement(5); // Output: Measured value is 5. DisplayMeasurement(30); // Output: Measured value is 30; too high. DisplayMeasurement(double.NaN); // Output: Failed measurement. void DisplayMeasurement(double measurement) { switch (measurement) { case < 0.0: Console.WriteLine($\"Measured value is {measurement}; too low.\"); break; case > 15.0: Console.WriteLine($\"Measured value is {measurement}; too high.\"); break; case double.NaN: Console.WriteLine(\"Failed measurement.\"); break; default: Console.WriteLine($\"Measured value is {measurement}.\"); break; } }","title":"Conditional operator"},{"location":"C%23101/conditional-operator/#conditional-operators","text":"classify = (input >= 0) ? \"nonnegative\" : \"negative\"; string GetWeatherDisplay(double tempInCelsius) => tempInCelsius < 20.0 ? \"Cold.\" : \"Perfect!\";","title":"Conditional operators"},{"location":"C%23101/conditional-operator/#selection-statements","text":"DisplayMeasurement(-4); // Output: Measured value is -4; too low. DisplayMeasurement(5); // Output: Measured value is 5. DisplayMeasurement(30); // Output: Measured value is 30; too high. DisplayMeasurement(double.NaN); // Output: Failed measurement. void DisplayMeasurement(double measurement) { switch (measurement) { case < 0.0: Console.WriteLine($\"Measured value is {measurement}; too low.\"); break; case > 15.0: Console.WriteLine($\"Measured value is {measurement}; too high.\"); break; case double.NaN: Console.WriteLine(\"Failed measurement.\"); break; default: Console.WriteLine($\"Measured value is {measurement}.\"); break; } }","title":"Selection-statements"},{"location":"C%23101/console/","text":"Console.Write(2); Console.WriteLine($HEX {243:X}\"); int Console.Read() ConsoleKeyInfo Console.ReadKey() string Console.ReadLine() int.Parse(\"12345\"); bool.TryParse(\"true\"); Console.Clear() Console.SetCursorPosition(left, top) Console.BackgroundColor, Console.ForegroundColor do { Console.Write(\"Enter the table size: \"); var readSize = Convert.ToString(Console.ReadKey().KeyChar); if ((int.TryParse(readSize, out n)) && ((n > 0) && (n <= 6))) break; else Console.WriteLine(\"\\nTable size must be an integer from 1 to 6\"); } while (true);","title":"Console"},{"location":"C%23101/dotnet_cli/","text":"dotnet cli list install versions dotnet --list-sdks dotnet --list-runtimes add project to existing solution dotnet new console --use-program-main --output newProjectDir --framework net6.0 // create a new project dotnet sln OtusLABs.sln add stack/stack.csproj // add a project to the solution dotnet new sln dotnet new console --output folder1/folder2/myapp dotnet new console --framework net5.0 // add project to a solution dotnet sln add folder1/folder2/myapp dotnet sln todo.sln list dotnet new sln -n mysolution dotnet new console -o myapp dotnet new classlib -o mylib1 dotnet new classlib -o mylib2 dotnet sln mysolution.sln add myapp\\myapp.csproj dotnet sln mysolution.sln add mylib1\\mylib1.csproj --solution-folder mylibs dotnet sln mysolution.sln add mylib2\\mylib2.csproj --solution-folder mylibs dotnet sln todo.sln remove **/*.csproj dotnet run","title":"dotnet cli"},{"location":"C%23101/dotnet_cli/#dotnet-cli","text":"","title":"dotnet cli"},{"location":"C%23101/dotnet_cli/#list-install-versions","text":"dotnet --list-sdks dotnet --list-runtimes","title":"list install versions"},{"location":"C%23101/dotnet_cli/#add-project-to-existing-solution","text":"dotnet new console --use-program-main --output newProjectDir --framework net6.0 // create a new project dotnet sln OtusLABs.sln add stack/stack.csproj // add a project to the solution dotnet new sln dotnet new console --output folder1/folder2/myapp dotnet new console --framework net5.0 // add project to a solution dotnet sln add folder1/folder2/myapp dotnet sln todo.sln list dotnet new sln -n mysolution dotnet new console -o myapp dotnet new classlib -o mylib1 dotnet new classlib -o mylib2 dotnet sln mysolution.sln add myapp\\myapp.csproj dotnet sln mysolution.sln add mylib1\\mylib1.csproj --solution-folder mylibs dotnet sln mysolution.sln add mylib2\\mylib2.csproj --solution-folder mylibs dotnet sln todo.sln remove **/*.csproj dotnet run","title":"add project to existing solution"},{"location":"C%23101/exceptions/","text":"try { // statements causing exception } catch( ExceptionName e1 ) { // error handling code } catch( ExceptionName e2 ) { // error handling code } catch( ExceptionName eN ) { // error handling code } finally { // statements to be executed } class GreenException : Exception { public GreenException(string message) : base(message) { } }","title":"Exceptions"},{"location":"C%23101/interfaces/","text":"namespace interfaces; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello, World!\"); var person = new Alexander(); //var p2 = new Human(); person.MyAge(); if (person is IContract) { var contract = person as IContract; var contract = person as IContract; var contract2 = (IContract)person; } } public interface IContract { string MyName(); int MyAge(); } public interface IContract2 { string AA(); int BB(); } public abstract class Human { public Human Father1 { get; set; } public Human Father2 { get; set; } public abstract int MyAge(); //{ // return 10; //} } public class Gender : Human, IContract, IContract2 { public bool IsBoy { get; set; } public bool IsGirl { get; set; } public string AA() { throw new NotImplementedException(); } public int BB() { throw new NotImplementedException(); } public override int MyAge() { return 10; } public string MyName() { throw new NotImplementedException(); } public virtual int SomeMethod() { return 15; } } public class Alexander : Gender { public override int MyAge() { return base.MyAge(); } public override int SomeMethod() { return base.SomeMethod(); } } }","title":"Interfaces"},{"location":"C%23101/strings_n_char/","text":"Strings string - immutable ReferenceEquals(s1, s2) s3 = string.Intern(s3) $\" text {variable} \" - [string interpolation] Properties s.Length; Methods - return new string objects s.Trim(); s.TrimStart(); s.Replace(\"Hello\", \"Greetings\"); s.ToUpper(); s.Contains(\"goodbye\"); Interpolation { [, ][: ]} string firstFriend = \"Maria\"; string secondFriend = \"Sage\"; Console.WriteLine($\"My friends are {firstFriend} and {secondFriend}\"); //Aligment Console.WriteLine($\"|{\"Left\",-7}|{\"Right\",7}|\"); //verbatim Console.WriteLine(@\"Text C:\\Windows\\System32\") ## Concatenation ```C# var str = new String('+', tableWidth); var Hello = string.Concat(\"Hello\", \" Otus\") StringBuilder var sb = new StringBuilder(); sb.Append(\"Hello\"); sb.Append(\" Otus\"); var res = sb.ToString(); str[..(strWight - 2)] char 2 bytes char c = 'j'; char c = '\\u006A'; //hex unicode","title":"Strings"},{"location":"C%23101/strings_n_char/#strings","text":"string - immutable ReferenceEquals(s1, s2) s3 = string.Intern(s3) $\" text {variable} \" - [string interpolation]","title":"Strings"},{"location":"C%23101/strings_n_char/#properties","text":"s.Length;","title":"Properties"},{"location":"C%23101/strings_n_char/#methods-return-new-string-objects","text":"s.Trim(); s.TrimStart(); s.Replace(\"Hello\", \"Greetings\"); s.ToUpper(); s.Contains(\"goodbye\");","title":"Methods - return new string objects"},{"location":"C%23101/strings_n_char/#interpolation","text":"{ [, ][: ]} string firstFriend = \"Maria\"; string secondFriend = \"Sage\"; Console.WriteLine($\"My friends are {firstFriend} and {secondFriend}\"); //Aligment Console.WriteLine($\"|{\"Left\",-7}|{\"Right\",7}|\"); //verbatim Console.WriteLine(@\"Text C:\\Windows\\System32\") ## Concatenation ```C# var str = new String('+', tableWidth); var Hello = string.Concat(\"Hello\", \" Otus\")","title":"Interpolation"},{"location":"C%23101/strings_n_char/#stringbuilder","text":"var sb = new StringBuilder(); sb.Append(\"Hello\"); sb.Append(\" Otus\"); var res = sb.ToString(); str[..(strWight - 2)]","title":"StringBuilder"},{"location":"C%23101/strings_n_char/#char","text":"2 bytes char c = 'j'; char c = '\\u006A'; //hex unicode","title":"char"},{"location":"C%23101/variables/","text":"string aFriend = \"Bill\"; int a = 18; int e = (a + b) % c; //% - remainder int max = int.MaxValue; int min = int.MinValue; Console.WriteLine($\"The range of integers is {min} to {max}\"); Predefined data types C# Mono Signed Memory Range sbyte System.Sbyte Yes 1 byte \u2013128 to 127 short System.Int16 Yes 2 bytes \u201332768 to 32767 int System.Int32 Yes 4 bytes \u20132147483648 to 2147483647 long System.Int64 Yes 8 bytes \u20139223372036854775808 to 9223372036854775807 byte System.Byte No 1 byte 0 to 255 ushort System.Uint16 No 2 bytes 0 to 65535 uint System.Uint32 No 4 bytes 0 to 4294967295 ulong System.Uint64 No 8 bytes 0 to 18446744073709551615 float System.Single Yes 4 bytes \u20131.5x10-45 to 3.4 x x1038 double System.Double Yes 8 bytes \u20135.0x10-324 to 1.7x10308 decimal System.Decimal Yes 12 bytes 1.0x10-28 to 7.9x1028 char System.Char 2 bytes Unicode characters boolean System.Boolean 1 byte True or false","title":"Variables"},{"location":"C%23101/variables/#predefined-data-types","text":"C# Mono Signed Memory Range sbyte System.Sbyte Yes 1 byte \u2013128 to 127 short System.Int16 Yes 2 bytes \u201332768 to 32767 int System.Int32 Yes 4 bytes \u20132147483648 to 2147483647 long System.Int64 Yes 8 bytes \u20139223372036854775808 to 9223372036854775807 byte System.Byte No 1 byte 0 to 255 ushort System.Uint16 No 2 bytes 0 to 65535 uint System.Uint32 No 4 bytes 0 to 4294967295 ulong System.Uint64 No 8 bytes 0 to 18446744073709551615 float System.Single Yes 4 bytes \u20131.5x10-45 to 3.4 x x1038 double System.Double Yes 8 bytes \u20135.0x10-324 to 1.7x10308 decimal System.Decimal Yes 12 bytes 1.0x10-28 to 7.9x1028 char System.Char 2 bytes Unicode characters boolean System.Boolean 1 byte True or false","title":"Predefined data types"},{"location":"ansible/","text":"Ansible Automation Platform Collections Automation Execution Environment EX294","title":"Ansible"},{"location":"ansible/#ansible-automation-platform","text":"","title":"Ansible Automation Platform"},{"location":"ansible/#collections","text":"","title":"Collections"},{"location":"ansible/#automation-execution-environment","text":"","title":"Automation Execution Environment"},{"location":"ansible/#ex294","text":"","title":"EX294"},{"location":"ansible/AAP/","text":"Ansible Automation Platform 2.2 Components: Ansible Core 2.13 - shipped with some modules, plugins (battery included) Ansible Content Collections - additional modules, plugins and roles not included in Ansible Core Automation Content Navigator - ansible-navigator ; this tool replaced ansible-playbook , ansible-galaxy etc. Automation Execution Environment - a container image that include Ansible Core, Ansible Content Collections, any Python libraries and other dependencies. Automation Controller - formerly Ansible Tower; (private) Automation Hub - non-public version of Ansible galaxy","title":"Ansible Automation Platform 2.2 Components:"},{"location":"ansible/AAP/#ansible-automation-platform-22-components","text":"Ansible Core 2.13 - shipped with some modules, plugins (battery included) Ansible Content Collections - additional modules, plugins and roles not included in Ansible Core Automation Content Navigator - ansible-navigator ; this tool replaced ansible-playbook , ansible-galaxy etc. Automation Execution Environment - a container image that include Ansible Core, Ansible Content Collections, any Python libraries and other dependencies. Automation Controller - formerly Ansible Tower; (private) Automation Hub - non-public version of Ansible galaxy","title":"Ansible Automation Platform 2.2 Components:"},{"location":"ansible/AEE/","text":"Automation Execution Environment Building a New Automation Execution Environment yum install ansible-builder ansible-builder searches for execution-environment.yml file in the current directory example of execution-environment.yml file: --- version: 1 build_arg_defaults: EE_BASE_IMAGE: registry.redhat.io/ansible-automation-platform-22/ee-minimal-rhel8:latest EE_BUILDER_IMAGE: registry.redhat.io/ansible-automation-platform-22/ansible-builder-rhel8:latest ansible_config: ansible.cfg dependencies: galaxy: requirements.yml python: requirements.txt system: bindep.txt EE_BASE_IMAGE - the base image for the execution environment (quay.io/ansible/ansible-runner:stable-2.12-latest) EE_BUILDER_IMAGE - the image used to build the execution environment (quay.io/ansible/ansible-builder:stable-2.12-latest) requirements.yml: --- collections: - community.aws - community.general - name: redhat.insights version: 1.0.5 source: https://console.redhat.com/api/automation-hub/ - name: ansible.posix source: https://hub.example.com/api/galaxy/content/rh-certified/ requirements.txt: sh==1.13.1 jsonschema>=3.2.0,<4.0.1 textfsm ttp xmltodict dnspython bindep.txt: rsync [platform:rpm] kubernetes-client [platform:rpm] Simple build ansible-builder build --tag ee-demo:v1.0 Interactive build Step1. Creating the context/ directory in the current directory. ansible-builder create # /home/user/demo/context \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 bindep.txt \u251c\u2500\u2500 context \u2502 \u251c\u2500\u2500 _build \u2502 \u2502 \u251c\u2500\u2500 ansible.cfg \u2502 \u2502 \u251c\u2500\u2500 bindep.txt \u2502 \u2502 \u251c\u2500\u2500 requirements.txt \u2502 \u2502 \u2514\u2500\u2500 requirements.yml \u2502 \u2514\u2500\u2500 Containerfile \u251c\u2500\u2500 execution-environment.yml \u251c\u2500\u2500 requirements.txt \u2514\u2500\u2500 requirements.yml Step2. Adjusting adjust execution-environment.yml file Containerfile file that defines the build process ARG EE_BASE_IMAGE=registry.redhat.io/ansible-automation-platform-22/ee-minimal-rhel8:latest ARG EE_BUILDER_IMAGE=registry.redhat.io/ansible-automation-platform-22/ansible-builder-rhel8:latest FROM $EE_BASE_IMAGE as galaxy ARG ANSIBLE_GALAXY_CLI_COLLECTION_OPTS= USER root COPY my-company-ca.pem /etc/pki/ca-trust/source/anchors RUN update-ca-trust ADD _build/ansible.cfg ~/.ansible.cfg ...output omitted... run the command to complete the build process: podman build -f context/Containerfile -t ee-demo:v2.0 context","title":"Automation Execution Environment"},{"location":"ansible/AEE/#automation-execution-environment","text":"","title":"Automation Execution Environment"},{"location":"ansible/AEE/#building-a-new-automation-execution-environment","text":"yum install ansible-builder ansible-builder searches for execution-environment.yml file in the current directory example of execution-environment.yml file: --- version: 1 build_arg_defaults: EE_BASE_IMAGE: registry.redhat.io/ansible-automation-platform-22/ee-minimal-rhel8:latest EE_BUILDER_IMAGE: registry.redhat.io/ansible-automation-platform-22/ansible-builder-rhel8:latest ansible_config: ansible.cfg dependencies: galaxy: requirements.yml python: requirements.txt system: bindep.txt EE_BASE_IMAGE - the base image for the execution environment (quay.io/ansible/ansible-runner:stable-2.12-latest) EE_BUILDER_IMAGE - the image used to build the execution environment (quay.io/ansible/ansible-builder:stable-2.12-latest) requirements.yml: --- collections: - community.aws - community.general - name: redhat.insights version: 1.0.5 source: https://console.redhat.com/api/automation-hub/ - name: ansible.posix source: https://hub.example.com/api/galaxy/content/rh-certified/ requirements.txt: sh==1.13.1 jsonschema>=3.2.0,<4.0.1 textfsm ttp xmltodict dnspython bindep.txt: rsync [platform:rpm] kubernetes-client [platform:rpm]","title":"Building a New Automation Execution Environment"},{"location":"ansible/AEE/#simple-build","text":"ansible-builder build --tag ee-demo:v1.0","title":"Simple build"},{"location":"ansible/AEE/#interactive-build","text":"Step1. Creating the context/ directory in the current directory. ansible-builder create # /home/user/demo/context \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 bindep.txt \u251c\u2500\u2500 context \u2502 \u251c\u2500\u2500 _build \u2502 \u2502 \u251c\u2500\u2500 ansible.cfg \u2502 \u2502 \u251c\u2500\u2500 bindep.txt \u2502 \u2502 \u251c\u2500\u2500 requirements.txt \u2502 \u2502 \u2514\u2500\u2500 requirements.yml \u2502 \u2514\u2500\u2500 Containerfile \u251c\u2500\u2500 execution-environment.yml \u251c\u2500\u2500 requirements.txt \u2514\u2500\u2500 requirements.yml Step2. Adjusting adjust execution-environment.yml file Containerfile file that defines the build process ARG EE_BASE_IMAGE=registry.redhat.io/ansible-automation-platform-22/ee-minimal-rhel8:latest ARG EE_BUILDER_IMAGE=registry.redhat.io/ansible-automation-platform-22/ansible-builder-rhel8:latest FROM $EE_BASE_IMAGE as galaxy ARG ANSIBLE_GALAXY_CLI_COLLECTION_OPTS= USER root COPY my-company-ca.pem /etc/pki/ca-trust/source/anchors RUN update-ca-trust ADD _build/ansible.cfg ~/.ansible.cfg ...output omitted... run the command to complete the build process: podman build -f context/Containerfile -t ee-demo:v2.0 context","title":"Interactive build"},{"location":"ansible/Collections/","text":"Ansible Collections Ansible supports collections since Ansible 2.8 RedHat recommends moving roles , modules , plugins and filters inside collections. namespace - is the first part of a collection name, e.g. the namespace of the amazon.aws is amazon . ansible.cfg ansible.cfg [defaults] collections_paths = ~/.ansible/collections:/usr/share/ansible/collections. Install collections By default ansible-galaxy installs the collections in the first directory that the collections_paths directive points to ansible-galaxy collection install community.crypto ansible-galaxy collection install /tmp/community-dns-1.2.0.tar.gz ansible-galaxy collection install \\ http://www.example.com/redhat-insights-1.0.5.tar.gz ansible-galaxy collection install \\ git@github.com:ansible-collections/community.mysql.git \\ -p ~/myproject/collections/ ansible-galaxy collection install -r requirements.yml File examples collections/requirements.yml --- collections: - name: community.crypto - name: ansible.posix version: 1.2.0 - name: /tmp/community-dns-1.2.0.tar.gz - name: http://www.example.com/redhat-insights-1.0.5.tar.gz - name: git@github.com:ansible-collections/community.mysql.git Configuring Collection Sources By default ansible-galaxy uses https://galaxy.ansible.com to download collections. ansible.cfg [galaxy] server_list = automation_hub, galaxy [galaxy_server.automation_hub] url=https://console.redhat.com/api/automation-hub/ auth_url=https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token token=draz...Usnx [galaxy_server.galaxy] url=https://galaxy.ansible.com/ Create a new collection create directory structure for a new collection ansible-galaxy collection init mynamespace.mycollection mynamespace/ \u2514\u2500\u2500 mycollection \u251c\u2500\u2500 docs # documentation for your modules, plugins, etc \u251c\u2500\u2500 meta # isn't created by init command \u2502 \u2514\u2500\u2500 runtime.yml # \u251c\u2500\u2500 galaxy.yml # metadata for Ansible to build and publish the collection \u251c\u2500\u2500 plugins # your modules, plugins, and filters \u2502 \u2514\u2500\u2500 README.md \u2502 \u2514\u2500\u2500 modules \u2502 \u2514\u2500\u2500 inventory \u251c\u2500\u2500 README.md # file describes your collection \u251c\u2500\u2500 roles # directory stores your roles \u251c\u2500\u2500 requirements.txt # python dependencies \u2514\u2500\u2500 bindep.txt # binary dependencies e.g. rsync File examples galaxy.yml --- namespace: mynamespace name: mycollection version: 1.0.0 readme: README.md authors: - your name <example@domain.com> description: Ansible modules to manage my company's custom software license: - GPL-3.0-or-later repository: https://git.example.com/training/my-collection # The URL to any online docs documentation: https://git.example.com/training/my-collection/tree/main/docs # The URL to the homepage of the collection/project homepage: https://git.example.com/training/my-collection # The URL to the collection issue tracker issues: https://git.example.com/training/my-collection/issues dependencies: community.general: '>=1.0.0' ansible.posix: '>=1.0.0' requirements.txt (Python dependencies) botocore>=1.18.0 boto3>=1.15.0 boto>=2.49.0 bindep.txt rsync [platform:centos-8 platform:rhel-8] meta/runtime.yml --- requires_ansible: \">=2.10\" Create a role in the new collection cd mynamespace/mycollection/roles/ && ansible-galaxy init myrole Building Collections # from inside the collection directory ansible-galaxy collection build Testing Collections ansible-lint ansible-test Publishing Collections ansible-galaxy collection publish mynamespace-mycollection-2.0.0.tar.gz","title":"Ansible Collections"},{"location":"ansible/Collections/#ansible-collections","text":"Ansible supports collections since Ansible 2.8 RedHat recommends moving roles , modules , plugins and filters inside collections. namespace - is the first part of a collection name, e.g. the namespace of the amazon.aws is amazon .","title":"Ansible Collections"},{"location":"ansible/Collections/#ansiblecfg","text":"ansible.cfg [defaults] collections_paths = ~/.ansible/collections:/usr/share/ansible/collections.","title":"ansible.cfg"},{"location":"ansible/Collections/#install-collections","text":"By default ansible-galaxy installs the collections in the first directory that the collections_paths directive points to ansible-galaxy collection install community.crypto ansible-galaxy collection install /tmp/community-dns-1.2.0.tar.gz ansible-galaxy collection install \\ http://www.example.com/redhat-insights-1.0.5.tar.gz ansible-galaxy collection install \\ git@github.com:ansible-collections/community.mysql.git \\ -p ~/myproject/collections/ ansible-galaxy collection install -r requirements.yml","title":"Install collections"},{"location":"ansible/Collections/#file-examples","text":"collections/requirements.yml --- collections: - name: community.crypto - name: ansible.posix version: 1.2.0 - name: /tmp/community-dns-1.2.0.tar.gz - name: http://www.example.com/redhat-insights-1.0.5.tar.gz - name: git@github.com:ansible-collections/community.mysql.git","title":"File examples"},{"location":"ansible/Collections/#configuring-collection-sources","text":"By default ansible-galaxy uses https://galaxy.ansible.com to download collections. ansible.cfg [galaxy] server_list = automation_hub, galaxy [galaxy_server.automation_hub] url=https://console.redhat.com/api/automation-hub/ auth_url=https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token token=draz...Usnx [galaxy_server.galaxy] url=https://galaxy.ansible.com/","title":"Configuring Collection Sources"},{"location":"ansible/Collections/#create-a-new-collection","text":"create directory structure for a new collection ansible-galaxy collection init mynamespace.mycollection mynamespace/ \u2514\u2500\u2500 mycollection \u251c\u2500\u2500 docs # documentation for your modules, plugins, etc \u251c\u2500\u2500 meta # isn't created by init command \u2502 \u2514\u2500\u2500 runtime.yml # \u251c\u2500\u2500 galaxy.yml # metadata for Ansible to build and publish the collection \u251c\u2500\u2500 plugins # your modules, plugins, and filters \u2502 \u2514\u2500\u2500 README.md \u2502 \u2514\u2500\u2500 modules \u2502 \u2514\u2500\u2500 inventory \u251c\u2500\u2500 README.md # file describes your collection \u251c\u2500\u2500 roles # directory stores your roles \u251c\u2500\u2500 requirements.txt # python dependencies \u2514\u2500\u2500 bindep.txt # binary dependencies e.g. rsync","title":"Create a new collection"},{"location":"ansible/Collections/#file-examples_1","text":"","title":"File examples"},{"location":"ansible/Collections/#galaxyyml","text":"--- namespace: mynamespace name: mycollection version: 1.0.0 readme: README.md authors: - your name <example@domain.com> description: Ansible modules to manage my company's custom software license: - GPL-3.0-or-later repository: https://git.example.com/training/my-collection # The URL to any online docs documentation: https://git.example.com/training/my-collection/tree/main/docs # The URL to the homepage of the collection/project homepage: https://git.example.com/training/my-collection # The URL to the collection issue tracker issues: https://git.example.com/training/my-collection/issues dependencies: community.general: '>=1.0.0' ansible.posix: '>=1.0.0'","title":"galaxy.yml"},{"location":"ansible/Collections/#requirementstxt-python-dependencies","text":"botocore>=1.18.0 boto3>=1.15.0 boto>=2.49.0","title":"requirements.txt (Python dependencies)"},{"location":"ansible/Collections/#bindeptxt","text":"rsync [platform:centos-8 platform:rhel-8]","title":"bindep.txt"},{"location":"ansible/Collections/#metaruntimeyml","text":"--- requires_ansible: \">=2.10\"","title":"meta/runtime.yml"},{"location":"ansible/Collections/#create-a-role-in-the-new-collection","text":"cd mynamespace/mycollection/roles/ && ansible-galaxy init myrole","title":"Create a role in the new collection"},{"location":"ansible/Collections/#building-collections","text":"# from inside the collection directory ansible-galaxy collection build","title":"Building Collections"},{"location":"ansible/Collections/#testing-collections","text":"ansible-lint ansible-test","title":"Testing Collections"},{"location":"ansible/Collections/#publishing-collections","text":"ansible-galaxy collection publish mynamespace-mycollection-2.0.0.tar.gz","title":"Publishing Collections"},{"location":"ansible/EX294/","text":"ex294 exam notes Ansible CLI ad hoc ansible commands ansible host-pattern -m module [-a 'module arguments'] [-i inventory] ansible -i inventory localhost -m setup # default module is command -a argument localhost is the host ansible -a /bin/date localhost # ansible_facts ansible -m setup localhost # -k prompt for password ansible -m ping -i inventory all -u 4esnok -k checks ansible-playbook -i inventory site.yml --syntax-check ansible-playbook -i inventory site.yml --check doc ansible-doc -l ansible-doc -s ansible.builtin.copy ansible-navigator doc -t lookup -m lookup -l -m stdout inventory ansible-navigator inventory -m stdout --list ansible-inventory --list ansible-inventory --graph all ansible --list all ansible --list 'webservers:rhel' # intersection ansible --list 'webservers:&rhel' # conjunction ansible --list 'webservers:!rhel' # negation vault ansible-vault create secret.yml ansible-vault create --vault-password-file=vault-pass secret.yml ansible-vault view secret1.yml ansible-vault edit secret.yml ansible-vault encrypt secret1.yml secret2.yml ansible-vault decrypt secret1.yml --output=secret1-decrypted.yml ansible-vault rekey secret.yml # prompt for the vault password ansible-navigator run -m stdout --playbook-artifact-enable false create_users.yml --vault-id @prompt ansible-navigator run -m stdout create_users.yml --vault-password-file=vault-pass galaxy ansible-galaxy collection install fedora.linux_system_roles -p ./collections ansible-galaxy search 'redis' --platforms EL ansible-galaxy info geerlingguy.redis ansible-galaxy list ansible-galaxy remove nginx ansible-navigator # ee is execution environment, playbook runs on the host ansible-navigator run --ee false -m stdout playbook.yml config path /etc/ansible/ansible.cfg # system wide ~/.ansible.cfg # home directory ./ansible.cfg # current directory ansible.cfg example [defaults] inventory = ./inventory remote_user = root host_key_checking = False retry_files_enabled = False roles_path = ./roles library = ./library module_utils = ./module_utils callback_whitelist = profile_tasks [privilege_escalation] become = True become_method = sudo become_user = root become_ask_pass = False ansible-config show configurations that have changed from the default ansible-config dump --only-changed generate ansible.cfg from a template ansible-config init Syntax Conditionals --- - name: Simple Boolean Task Demo hosts: all vars: run_my_task: true tasks: - name: httpd package is installed ansible.builtin.dnf: name: httpd when: run_my_task | bool --- - name: Test Variable is Defined Demo hosts: all vars: my_service: httpd tasks: - name: \"{{ my_service }} package is installed\" ansible.builtin.dnf: name: \"{{ my_service }}\" when: my_service is defined greater-thn character (>) used to split the long conditionals over multiple lines when: > ( ansible_facts['distribution'] == \"RedHat\" and ansible_facts['distribution_major_version'] == \"9\" ) or ( ansible_facts['distribution'] == \"Fedora\" and ansible_facts['distribution_major_version'] == \"34\" ) these two conditions are equal when: ansible_facts['distribution_version'] == \"9.0\" and ansible_facts['kernel'] == \"5.14.0-70.13.1.el9_0.x86_64\" when: - ansible_facts['distribution_version'] == \"9.0\" - ansible_facts['kernel'] == \"5.14.0-70.13.1.el9_0.x86_64\" When examples when: ansible_facts['distribution'] == \"RedHat\" when: \"'RedHat' in ansible_facts['distribution']\" when: my_var is defined When help ansible-doc -t keyword when Loops - name: Postfix and Dovecot are running ansible.builtin.service: name: \"{{ item }}\" state: started loop: - postfix - dovecot - name: Users exist and are in the correct groups user: name: \"{{ item['name'] }}\" state: present groups: \"{{ item['groups'] }}\" loop: - name: jane groups: wheel - name: joe groups: root vars: mail_services: - postfix - dovecot tasks: - name: Postfix and Dovecot are running ansible.builtin.service: name: \"{{ item }}\" state: started loop: \"{{ mail_services }}\" The register keyword can also capture the output of a task that loops. The following snippet shows the structure of the register variable from a task that loops: --- - name: Loop Register Test gather_facts: no hosts: localhost tasks: - name: Looping Echo Task ansible.builtin.shell: \"echo This is my item: {{ item }}\" loop: - one - two register: echo_results - name: Show echo_results variable ansible.builtin.debug: var: echo_results playbook output {\"echo_results\": { \"changed\": true, \"msg\": \"All items completed\", \"results\": [ { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/local/bin/python3.11\" }, \"ansible_loop_var\": \"item\", \"changed\": true, \"cmd\": \"echo This is my item: one\", \"delta\": \"0:00:00.014493\", \"end\": \"2023-03-22 22:14:19.949881\", \"failed\": false, \"invocation\": {}}]}} Earlier-style Ansible loops Loop keyword Description with_items Behaves the same as the loop keyword for simple lists, such as a list of strings or a list of dictionaries. Unlike loop, if lists of lists are provided to with_items, they are flattened into a single-level list. The item loop variable holds the list item used during each iteration. with_file Requires a list of control node file names. The item loop variable holds the content of a corresponding file from the file list during each iteration. with_sequence Requires parameters to generate a list of values based on a numeric sequence. The item loop variable holds the value of one of the generated items in the generated sequence during each iteration. Loop help Loops are facilitated by lookup plugin ansible-doc -t lookup -l ansible-doc -t keyword loop Handlers and Failure tasks: - name: copy demo.example.conf configuration template ansible.builtin.template: src: /var/lib/templates/demo.example.conf.template dest: /etc/httpd/conf.d/demo.example.conf notify: - restart apache handlers: - name: restart apache ansible.builtin.service: name: httpd state: restarted ignore_errors - name: Install {{ web_package }} package ansible.builtin.dnf: name: \"{{ web_package }}\" state: present ignore_errors: yes change_when change the status of the task - name: Check local time ansible.builtin.command: date register: command_result changed_when: false failed_when - define what \u201cfailure\u201d means in the task - name: Fail task when the command error output prints FAILED ansible.builtin.command: /usr/bin/example-command -x -y -z register: command_result failed_when: \"'FAILED' in command_result.stderr\" check_mode ansible-navigator run --check tasks: - name: task always runs even in check mode ansible.builtin.shell: uname -a check_mode: no Block block - a way to group tasks together and execute them as a single unit tasks: - name: Attempt to set up a webserver block: - name: Install {{ web_package }} package ansible.builtin.dnf: name: \"{{ web_package }}\" state: present rescue: - name: Install {{ db_package }} package ansible.builtin.dnf: name: \"{{ db_package }}\" state: present always: - name: Start {{ db_service }} service ansible.builtin.service: name: \"{{ db_service }}\" state: started Filters Filter help ansible-doc -t filter -l Filter default and password_hash password: \"{{ my_password | default('redhat') | password_hash('sha512') }}\" Filter dict2items vars: my_users: fred: groups: - flintstones - wheel password: yabadabadoo wilma: groups: flintstones password: yabadabadoo barney: groups: rubbles password: flimflom tasks: - name: Ensure users are in their appropriate groups loop: \"{{ my_users | dict2items }}\" ansible.builtin.user: name: \"{{ item['key'] }}\" state: present groups: \"{{ item['value']['groups'] }}\" password: \"{{ item['value']['password'] | password_hash('sha512') }}\" update_password: on_create generate_ssh_key: yes Filter product and list vars: beatles: - John - Paul - George - Ringo category_db: - lyric - concerts - instruments tasks: - name: Ensure Beatles access to their databases loop: \"{{ beatles | product(category_db) | list }}\" ansible.mysql.mysql_user: name: \"{{ item[0] }}\" priv: \"{{ item[1] }}.*:ALL\" append_privs: yes password: \"{{ 'db_pass' | password_hash('sha512') }}\" The list filter is used here to convert the result of the product filter into a list, which is then used to loop over. Filter lookup('dict', ...) # ansible-doc -t lookup dict vars: config_files: vim: file: vimrc dest: ~/.vimrc yamllint: file: yamllint dest: ~/.config/yamllint/config tasks: - name: Ensure config files are copied loop: \"{{ lookup('dict', config_files) }}\" ansible.builtin.copy: src: \"{{ item['value']['file'] }}\" dest: \"{{ item['value']['dest'] }}\" Include, Import and Roles Include Includes are considered dynamic operation. Ansible will process the instruction as it gets to the appropriate line inside of your playbook. include_vars include_tasks --- - name: Install web server hosts: webservers tasks: - include_tasks: webserver_tasks.yml include_role One key difference between include_role and import_role is how they handle task-level keywords, conditionals, and loops: ansible.builtin.import_role applies the task's conditionals and loops to each of the tasks being imported. ansible.builtin.include_role applies the task's conditionals and loops to the statement that determines whether the role is included or not. Import Imports are considered static operation. Ansible will pre-processed at the time the playbook is parsed. (can't use loop, variables are limited) import_playbook example of a master playbook - name: Prepare the web server import_playbook: web.yml - name: Prepare the database server import_playbook: db.yml import_tasks [admin@node ~]$ cat webserver_tasks.yml - name: Installs the httpd package ansible.builtin.dnf: name: httpd state: latest - name: Starts the httpd service ansible.builtin.service: name: httpd state: started --- - name: Install web server hosts: webservers tasks: - import_tasks: webserver_tasks.yml ...output omitted... tasks: - name: Import task file and set variables import_tasks: task.yml vars: package: httpd service: httpd import_role ansible treats the role as a static import With the ansible.builtin.import_role module, Ansible treats the role as a static import and parses it during initial playbook processing. In the preceding example, when the playbook is parsed: * If roles/role2/tasks/main.yml exists, Ansible adds the tasks in that file to the play. * If roles/role2/handlers/main.yml exists, Ansible adds the handlers in that file to the play. * If roles/role2/defaults/main.yml exists, Ansible adds the default variables in that file to the play. * If roles/role2/vars/main.yml exists, Ansible adds the variables in that file to the play (possibly overriding values from role default variables due to precedence). > Because ansible.builtin.import_role is processed when the playbook is parsed, the role's handlers, default variables, and role variables are all exposed to all the tasks and roles in the play, and can be accessed by tasks and roles that precede it in the play (even though the role has not run yet). - name: Run a role as a task hosts: remote.example.com tasks: - name: A normal task ansible.builtin.debug: msg: 'first task' - name: A task to import role2 here ansible.builtin.import_role: name: role2 vars: var1: val1 var2: val2 Roles Role section in a Play --- - name: A play that only has roles hosts: remote.example.com roles: - role: role1 - role: role2 Install a role from Ansible Galaxy # roles/requirments.yml - src: https://git.example.com/someuser/someuser.myrole scm: git version: \"1.5.0\" - src: https://www.example.com/role-archive/someuser.otherrole.tar name: someuser.otherrole # from Ansible Galaxy, using the latest version - src: geerlingguy.redis # from Ansible Galaxy, overriding the name and using a specific version - src: geerlingguy.redis version: \"1.5.0\" name: redis_prod # from any Git based repository, using HTTPS - src: https://github.com/geerlingguy/ansible-role-nginx.git scm: git version: master name: nginx # from a role tar ball, given a URL; # supports 'http', 'https', or 'file' protocols - src: file:///opt/local/roles/myrole.tar name: myrole ansible-galaxy role install -r roles/requirements.yml -p roles Variables and Facts Variables Variables priority from top to bottom * Group variables defined in the inventory * Group variables defined in files in a group_vars subdirectory in the same directory as the inventory or the playbook * Host variables defined in the inventory * Host variables defined in files in a host_vars subdirectory in the same directory as the inventory or the playbook * Host facts, discovered at runtime * Play variables in the playbook (vars and vars_files) * Task variables * Extra variables defined on the command line Inventory variables You can define variables for hosts and host groups by creating two directories, group_vars and host_vars , in the same working directory as the inventory file or playbook. project \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 group_vars \u2502 \u251c\u2500\u2500 datacenters \u2502 \u251c\u2500\u2500 datacenters1 \u2502 \u2514\u2500\u2500 datacenters2 \u251c\u2500\u2500 host_vars \u2502 \u251c\u2500\u2500 demo1.example.com \u2502 \u251c\u2500\u2500 demo2.example.com \u2502 \u251c\u2500\u2500 demo3.example.com \u2502 \u2514\u2500\u2500 demo4.example.com \u251c\u2500\u2500 inventory \u2514\u2500\u2500 playbook.yml Command line variables set a variable on the command line ansible-navigator run main.yml -e \"package=apache\" Playbook variables define a variable in a playbook or vars_files user1_first_name: Bob user1_last_name: Jones user1_home_dir: /users/bjones user2_first_name: Anne user2_last_name: Cook user2_home_dir: /users/acook users: bjones: first_name: Bob last_name: Jones home_dir: /users/bjones acook: first_name: Anne last_name: Cook home_dir: /users/acook Var files - hosts: all vars_files: - vars/users.yml Work with dictionaries # Ansible way # Returns 'Bob' users.bjones.first_name # Returns '/users/acook' users.acook.home_dir # Python way # Returns 'Bob' users['bjones']['first_name'] # Returns '/users/acook' users['acook']['home_dir'] Magic Variables Hostvars A dictionary with all the hosts in inventory and variables assigned to them hostvars['demo2.example.com']['ansible_facts']['interfaces'] group_names A list of groups the current host is part of groups A dictionary with all the groups in inventory and each group has the list of hosts that belong to it inventory_hostname The inventory name for the \u2018current\u2019 host being iterated over in the play Facts # Display all facts - name: Fact dump hosts: all tasks: - name: Print all facts ansible.builtin.debug: var: ansible_facts Ansible Facts Injected as Variables Before Ansible 2.5, facts were always injected as individual variables prefixed with the string ansible_ instead of being part of the ansible_facts variable. For example, the ansible_facts['distribution'] fact was called ansible_distribution. ansible_facts.* name ansible_* name ansible_facts['hostname'] ansible_hostname ansible_facts['fqdn'] ansible_fqdn ansible_facts['default_ipv4']['address'] ansible_default_ipv4['address'] Custom Facts By default, the ansible.builtin.setup module loads custom facts from files and scripts in the etc/ansible/facts.d. File shoudl end with \".fact\". The ansible.builtin.setup module stores custom facts in the ansible_facts['ansible_local'] variable. # file name should end with .fact [packages] web_package = httpd db_package = mariadb-server [users] user1 = joe user2 = jane Packages Facts - name: Insure package facts gathered ansible.builtin.package_facts: - name: print package facts ansible.builtin.debug: var: ansible_facts.packages Modifying and Copying Files to Hosts ansible.builtin Module name Description blockinfile Insert/update/remove a text block surrounded by marker lines in a file copy Copy a file from the local or remote machine to a location on the managed hosts fetch Works like the copy module, but in reverse file Set attributes such as permissions, ownership, SELinux contexts, and time stamps of regular files, symlinks, hard links, and directories. This module can also create or remove regular files, symlinks, hard links, and directories. A number of other file-related modules support the same options to set attributes as the file module, including the copy module. lineinfile Ensure that a particular line is in a file, or replace an existing line using a back-reference regular expression. This module is primarily useful when you want to change a single line in a file. stat Retrieve status information for a file, similar to the Linux stat command. ansible.posix Module name Description patch Apply a patch using GNU patch. synchronize A wrapper around the rsync command to simplify common tasks. The synchronize module is not intended to provide access to the full power of the rsync command, but does make the most common invocations easier to implement. You might still need to call the rsync command directly via the run command module depending on your use case. Troubleshooting Ansible ansible.builtin.uri The ansible.builtin.uri module provides a way to verify that a RESTful API is returning the required content. tasks: - ansible.builtin.uri: url: http://api.myapp.example.com return_content: yes register: apiresponse - ansible.builtin.fail: msg: 'version was not provided' when: \"'version' not in apiresponse.content\" ansible.builtin.stat You can use it to register a variable and then test to determine if the file exists or to get other information about the file tasks: - name: Check if /var/run/app.lock exists ansible.builtin.stat: path: /var/run/app.lock register: lock - name: Fail if the application is running ansible.builtin.fail: when: lock['stat']['exists'] ansible.builtin.assert tasks: - name: Check if /var/run/app.lock exists ansible.builtin.stat: path: /var/run/app.lock register: lock - name: Fail if the application is running ansible.builtin.assert: that: - not lock['stat']['exists'] YAML YAML in a nutshell Special characters Character Description ( | ) vertical ber to denote a new line characters ( > ) greater-than new line characters to be converted to spaces include_newlines: | Example Company 123 Main Street Atlanta, GA 30303 fold_newlines: > This is an example of a long string, that will become a single sentence once folded. YAML Dictionaries name: svcrole svcservice: httpd svcport: 80 {name: svcrole, svcservice: httpd, svcport: 80} YAML Lists hosts: - servera - serverb - serverc hosts: [servera, serverb, serverc] Jinja2 for statement {# for statement - this is comment #} {% for user in users %} {{ user }} {% endfor %} {% for myhost in groups['myhosts'] %} {{ myhost }} {% endfor %} Conditional statements {% if finished %} {{ result }} {% endif %} /etc/hosts file - name: /etc/hosts is up to date hosts: all gather_facts: yes tasks: - name: Deploy /etc/hosts ansible.builtin.template: src: templates/hosts.j2 dest: /etc/hosts {{ ansible_managed }} {% for host in groups['all'] %} {{ hostvars[host]['ansible_facts']['default_ipv4']['address'] }} {{ hostvars[host]['ansible_facts']['fqdn'] }} {{ hostvars[host]['ansible_facts']['hostname'] }} {% endfor %} jinja2 filters {{ 'hello world' | capitalize }} {{ output | to_json }} {{ output | to_yaml }}","title":"ex294 exam notes"},{"location":"ansible/EX294/#ex294-exam-notes","text":"","title":"ex294 exam notes"},{"location":"ansible/EX294/#ansible-cli","text":"","title":"Ansible CLI"},{"location":"ansible/EX294/#ad-hoc-ansible-commands","text":"ansible host-pattern -m module [-a 'module arguments'] [-i inventory] ansible -i inventory localhost -m setup # default module is command -a argument localhost is the host ansible -a /bin/date localhost # ansible_facts ansible -m setup localhost # -k prompt for password ansible -m ping -i inventory all -u 4esnok -k","title":"ad hoc ansible commands"},{"location":"ansible/EX294/#checks","text":"ansible-playbook -i inventory site.yml --syntax-check ansible-playbook -i inventory site.yml --check","title":"checks"},{"location":"ansible/EX294/#doc","text":"ansible-doc -l ansible-doc -s ansible.builtin.copy ansible-navigator doc -t lookup -m lookup -l -m stdout","title":"doc"},{"location":"ansible/EX294/#inventory","text":"ansible-navigator inventory -m stdout --list ansible-inventory --list ansible-inventory --graph all ansible --list all ansible --list 'webservers:rhel' # intersection ansible --list 'webservers:&rhel' # conjunction ansible --list 'webservers:!rhel' # negation","title":"inventory"},{"location":"ansible/EX294/#vault","text":"ansible-vault create secret.yml ansible-vault create --vault-password-file=vault-pass secret.yml ansible-vault view secret1.yml ansible-vault edit secret.yml ansible-vault encrypt secret1.yml secret2.yml ansible-vault decrypt secret1.yml --output=secret1-decrypted.yml ansible-vault rekey secret.yml # prompt for the vault password ansible-navigator run -m stdout --playbook-artifact-enable false create_users.yml --vault-id @prompt ansible-navigator run -m stdout create_users.yml --vault-password-file=vault-pass","title":"vault"},{"location":"ansible/EX294/#galaxy","text":"ansible-galaxy collection install fedora.linux_system_roles -p ./collections ansible-galaxy search 'redis' --platforms EL ansible-galaxy info geerlingguy.redis ansible-galaxy list ansible-galaxy remove nginx","title":"galaxy"},{"location":"ansible/EX294/#ansible-navigator","text":"# ee is execution environment, playbook runs on the host ansible-navigator run --ee false -m stdout playbook.yml","title":"ansible-navigator"},{"location":"ansible/EX294/#config-path","text":"/etc/ansible/ansible.cfg # system wide ~/.ansible.cfg # home directory ./ansible.cfg # current directory","title":"config path"},{"location":"ansible/EX294/#ansiblecfg-example","text":"[defaults] inventory = ./inventory remote_user = root host_key_checking = False retry_files_enabled = False roles_path = ./roles library = ./library module_utils = ./module_utils callback_whitelist = profile_tasks [privilege_escalation] become = True become_method = sudo become_user = root become_ask_pass = False","title":"ansible.cfg example"},{"location":"ansible/EX294/#ansible-config","text":"show configurations that have changed from the default ansible-config dump --only-changed generate ansible.cfg from a template ansible-config init","title":"ansible-config"},{"location":"ansible/EX294/#syntax","text":"","title":"Syntax"},{"location":"ansible/EX294/#conditionals","text":"--- - name: Simple Boolean Task Demo hosts: all vars: run_my_task: true tasks: - name: httpd package is installed ansible.builtin.dnf: name: httpd when: run_my_task | bool --- - name: Test Variable is Defined Demo hosts: all vars: my_service: httpd tasks: - name: \"{{ my_service }} package is installed\" ansible.builtin.dnf: name: \"{{ my_service }}\" when: my_service is defined greater-thn character (>) used to split the long conditionals over multiple lines when: > ( ansible_facts['distribution'] == \"RedHat\" and ansible_facts['distribution_major_version'] == \"9\" ) or ( ansible_facts['distribution'] == \"Fedora\" and ansible_facts['distribution_major_version'] == \"34\" ) these two conditions are equal when: ansible_facts['distribution_version'] == \"9.0\" and ansible_facts['kernel'] == \"5.14.0-70.13.1.el9_0.x86_64\" when: - ansible_facts['distribution_version'] == \"9.0\" - ansible_facts['kernel'] == \"5.14.0-70.13.1.el9_0.x86_64\"","title":"Conditionals"},{"location":"ansible/EX294/#when-examples","text":"when: ansible_facts['distribution'] == \"RedHat\" when: \"'RedHat' in ansible_facts['distribution']\" when: my_var is defined","title":"When examples"},{"location":"ansible/EX294/#when-help","text":"ansible-doc -t keyword when","title":"When help"},{"location":"ansible/EX294/#loops","text":"- name: Postfix and Dovecot are running ansible.builtin.service: name: \"{{ item }}\" state: started loop: - postfix - dovecot - name: Users exist and are in the correct groups user: name: \"{{ item['name'] }}\" state: present groups: \"{{ item['groups'] }}\" loop: - name: jane groups: wheel - name: joe groups: root vars: mail_services: - postfix - dovecot tasks: - name: Postfix and Dovecot are running ansible.builtin.service: name: \"{{ item }}\" state: started loop: \"{{ mail_services }}\" The register keyword can also capture the output of a task that loops. The following snippet shows the structure of the register variable from a task that loops: --- - name: Loop Register Test gather_facts: no hosts: localhost tasks: - name: Looping Echo Task ansible.builtin.shell: \"echo This is my item: {{ item }}\" loop: - one - two register: echo_results - name: Show echo_results variable ansible.builtin.debug: var: echo_results playbook output {\"echo_results\": { \"changed\": true, \"msg\": \"All items completed\", \"results\": [ { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/local/bin/python3.11\" }, \"ansible_loop_var\": \"item\", \"changed\": true, \"cmd\": \"echo This is my item: one\", \"delta\": \"0:00:00.014493\", \"end\": \"2023-03-22 22:14:19.949881\", \"failed\": false, \"invocation\": {}}]}}","title":"Loops"},{"location":"ansible/EX294/#earlier-style-ansible-loops","text":"Loop keyword Description with_items Behaves the same as the loop keyword for simple lists, such as a list of strings or a list of dictionaries. Unlike loop, if lists of lists are provided to with_items, they are flattened into a single-level list. The item loop variable holds the list item used during each iteration. with_file Requires a list of control node file names. The item loop variable holds the content of a corresponding file from the file list during each iteration. with_sequence Requires parameters to generate a list of values based on a numeric sequence. The item loop variable holds the value of one of the generated items in the generated sequence during each iteration.","title":"Earlier-style Ansible loops"},{"location":"ansible/EX294/#loop-help","text":"Loops are facilitated by lookup plugin ansible-doc -t lookup -l ansible-doc -t keyword loop","title":"Loop help"},{"location":"ansible/EX294/#handlers-and-failure","text":"tasks: - name: copy demo.example.conf configuration template ansible.builtin.template: src: /var/lib/templates/demo.example.conf.template dest: /etc/httpd/conf.d/demo.example.conf notify: - restart apache handlers: - name: restart apache ansible.builtin.service: name: httpd state: restarted","title":"Handlers and Failure"},{"location":"ansible/EX294/#ignore_errors","text":"- name: Install {{ web_package }} package ansible.builtin.dnf: name: \"{{ web_package }}\" state: present ignore_errors: yes","title":"ignore_errors"},{"location":"ansible/EX294/#change_when","text":"change the status of the task - name: Check local time ansible.builtin.command: date register: command_result changed_when: false","title":"change_when"},{"location":"ansible/EX294/#failed_when-define-what-failure-means-in-the-task","text":"- name: Fail task when the command error output prints FAILED ansible.builtin.command: /usr/bin/example-command -x -y -z register: command_result failed_when: \"'FAILED' in command_result.stderr\"","title":"failed_when - define what \u201cfailure\u201d means in the task"},{"location":"ansible/EX294/#check_mode","text":"ansible-navigator run --check tasks: - name: task always runs even in check mode ansible.builtin.shell: uname -a check_mode: no","title":"check_mode"},{"location":"ansible/EX294/#block","text":"block - a way to group tasks together and execute them as a single unit tasks: - name: Attempt to set up a webserver block: - name: Install {{ web_package }} package ansible.builtin.dnf: name: \"{{ web_package }}\" state: present rescue: - name: Install {{ db_package }} package ansible.builtin.dnf: name: \"{{ db_package }}\" state: present always: - name: Start {{ db_service }} service ansible.builtin.service: name: \"{{ db_service }}\" state: started","title":"Block"},{"location":"ansible/EX294/#filters","text":"","title":"Filters"},{"location":"ansible/EX294/#filter-help","text":"ansible-doc -t filter -l","title":"Filter help"},{"location":"ansible/EX294/#filter-default-and-password_hash","text":"password: \"{{ my_password | default('redhat') | password_hash('sha512') }}\"","title":"Filter default and password_hash"},{"location":"ansible/EX294/#filter-dict2items","text":"vars: my_users: fred: groups: - flintstones - wheel password: yabadabadoo wilma: groups: flintstones password: yabadabadoo barney: groups: rubbles password: flimflom tasks: - name: Ensure users are in their appropriate groups loop: \"{{ my_users | dict2items }}\" ansible.builtin.user: name: \"{{ item['key'] }}\" state: present groups: \"{{ item['value']['groups'] }}\" password: \"{{ item['value']['password'] | password_hash('sha512') }}\" update_password: on_create generate_ssh_key: yes","title":"Filter dict2items"},{"location":"ansible/EX294/#filter-product-and-list","text":"vars: beatles: - John - Paul - George - Ringo category_db: - lyric - concerts - instruments tasks: - name: Ensure Beatles access to their databases loop: \"{{ beatles | product(category_db) | list }}\" ansible.mysql.mysql_user: name: \"{{ item[0] }}\" priv: \"{{ item[1] }}.*:ALL\" append_privs: yes password: \"{{ 'db_pass' | password_hash('sha512') }}\" The list filter is used here to convert the result of the product filter into a list, which is then used to loop over.","title":"Filter product and list"},{"location":"ansible/EX294/#filter-lookupdict","text":"# ansible-doc -t lookup dict vars: config_files: vim: file: vimrc dest: ~/.vimrc yamllint: file: yamllint dest: ~/.config/yamllint/config tasks: - name: Ensure config files are copied loop: \"{{ lookup('dict', config_files) }}\" ansible.builtin.copy: src: \"{{ item['value']['file'] }}\" dest: \"{{ item['value']['dest'] }}\"","title":"Filter lookup('dict', ...)"},{"location":"ansible/EX294/#include-import-and-roles","text":"","title":"Include, Import and Roles"},{"location":"ansible/EX294/#include","text":"Includes are considered dynamic operation. Ansible will process the instruction as it gets to the appropriate line inside of your playbook.","title":"Include"},{"location":"ansible/EX294/#include_vars","text":"","title":"include_vars"},{"location":"ansible/EX294/#include_tasks","text":"--- - name: Install web server hosts: webservers tasks: - include_tasks: webserver_tasks.yml","title":"include_tasks"},{"location":"ansible/EX294/#include_role","text":"One key difference between include_role and import_role is how they handle task-level keywords, conditionals, and loops: ansible.builtin.import_role applies the task's conditionals and loops to each of the tasks being imported. ansible.builtin.include_role applies the task's conditionals and loops to the statement that determines whether the role is included or not.","title":"include_role"},{"location":"ansible/EX294/#import","text":"Imports are considered static operation. Ansible will pre-processed at the time the playbook is parsed. (can't use loop, variables are limited)","title":"Import"},{"location":"ansible/EX294/#import_playbook","text":"example of a master playbook - name: Prepare the web server import_playbook: web.yml - name: Prepare the database server import_playbook: db.yml","title":"import_playbook"},{"location":"ansible/EX294/#import_tasks","text":"[admin@node ~]$ cat webserver_tasks.yml - name: Installs the httpd package ansible.builtin.dnf: name: httpd state: latest - name: Starts the httpd service ansible.builtin.service: name: httpd state: started --- - name: Install web server hosts: webservers tasks: - import_tasks: webserver_tasks.yml ...output omitted... tasks: - name: Import task file and set variables import_tasks: task.yml vars: package: httpd service: httpd","title":"import_tasks"},{"location":"ansible/EX294/#import_role","text":"ansible treats the role as a static import With the ansible.builtin.import_role module, Ansible treats the role as a static import and parses it during initial playbook processing. In the preceding example, when the playbook is parsed: * If roles/role2/tasks/main.yml exists, Ansible adds the tasks in that file to the play. * If roles/role2/handlers/main.yml exists, Ansible adds the handlers in that file to the play. * If roles/role2/defaults/main.yml exists, Ansible adds the default variables in that file to the play. * If roles/role2/vars/main.yml exists, Ansible adds the variables in that file to the play (possibly overriding values from role default variables due to precedence). > Because ansible.builtin.import_role is processed when the playbook is parsed, the role's handlers, default variables, and role variables are all exposed to all the tasks and roles in the play, and can be accessed by tasks and roles that precede it in the play (even though the role has not run yet). - name: Run a role as a task hosts: remote.example.com tasks: - name: A normal task ansible.builtin.debug: msg: 'first task' - name: A task to import role2 here ansible.builtin.import_role: name: role2 vars: var1: val1 var2: val2","title":"import_role"},{"location":"ansible/EX294/#roles","text":"","title":"Roles"},{"location":"ansible/EX294/#role-section-in-a-play","text":"--- - name: A play that only has roles hosts: remote.example.com roles: - role: role1 - role: role2","title":"Role section in a Play"},{"location":"ansible/EX294/#install-a-role-from-ansible-galaxy","text":"# roles/requirments.yml - src: https://git.example.com/someuser/someuser.myrole scm: git version: \"1.5.0\" - src: https://www.example.com/role-archive/someuser.otherrole.tar name: someuser.otherrole # from Ansible Galaxy, using the latest version - src: geerlingguy.redis # from Ansible Galaxy, overriding the name and using a specific version - src: geerlingguy.redis version: \"1.5.0\" name: redis_prod # from any Git based repository, using HTTPS - src: https://github.com/geerlingguy/ansible-role-nginx.git scm: git version: master name: nginx # from a role tar ball, given a URL; # supports 'http', 'https', or 'file' protocols - src: file:///opt/local/roles/myrole.tar name: myrole ansible-galaxy role install -r roles/requirements.yml -p roles","title":"Install a role from Ansible Galaxy"},{"location":"ansible/EX294/#variables-and-facts","text":"","title":"Variables and Facts"},{"location":"ansible/EX294/#variables","text":"Variables priority from top to bottom * Group variables defined in the inventory * Group variables defined in files in a group_vars subdirectory in the same directory as the inventory or the playbook * Host variables defined in the inventory * Host variables defined in files in a host_vars subdirectory in the same directory as the inventory or the playbook * Host facts, discovered at runtime * Play variables in the playbook (vars and vars_files) * Task variables * Extra variables defined on the command line","title":"Variables"},{"location":"ansible/EX294/#inventory-variables","text":"You can define variables for hosts and host groups by creating two directories, group_vars and host_vars , in the same working directory as the inventory file or playbook. project \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 group_vars \u2502 \u251c\u2500\u2500 datacenters \u2502 \u251c\u2500\u2500 datacenters1 \u2502 \u2514\u2500\u2500 datacenters2 \u251c\u2500\u2500 host_vars \u2502 \u251c\u2500\u2500 demo1.example.com \u2502 \u251c\u2500\u2500 demo2.example.com \u2502 \u251c\u2500\u2500 demo3.example.com \u2502 \u2514\u2500\u2500 demo4.example.com \u251c\u2500\u2500 inventory \u2514\u2500\u2500 playbook.yml","title":"Inventory variables"},{"location":"ansible/EX294/#command-line-variables","text":"set a variable on the command line ansible-navigator run main.yml -e \"package=apache\"","title":"Command line variables"},{"location":"ansible/EX294/#playbook-variables","text":"define a variable in a playbook or vars_files user1_first_name: Bob user1_last_name: Jones user1_home_dir: /users/bjones user2_first_name: Anne user2_last_name: Cook user2_home_dir: /users/acook users: bjones: first_name: Bob last_name: Jones home_dir: /users/bjones acook: first_name: Anne last_name: Cook home_dir: /users/acook","title":"Playbook variables"},{"location":"ansible/EX294/#var-files","text":"- hosts: all vars_files: - vars/users.yml","title":"Var files"},{"location":"ansible/EX294/#work-with-dictionaries","text":"# Ansible way # Returns 'Bob' users.bjones.first_name # Returns '/users/acook' users.acook.home_dir # Python way # Returns 'Bob' users['bjones']['first_name'] # Returns '/users/acook' users['acook']['home_dir']","title":"Work with dictionaries"},{"location":"ansible/EX294/#magic-variables","text":"","title":"Magic Variables"},{"location":"ansible/EX294/#hostvars","text":"A dictionary with all the hosts in inventory and variables assigned to them hostvars['demo2.example.com']['ansible_facts']['interfaces']","title":"Hostvars"},{"location":"ansible/EX294/#group_names","text":"A list of groups the current host is part of","title":"group_names"},{"location":"ansible/EX294/#groups","text":"A dictionary with all the groups in inventory and each group has the list of hosts that belong to it","title":"groups"},{"location":"ansible/EX294/#inventory_hostname","text":"The inventory name for the \u2018current\u2019 host being iterated over in the play","title":"inventory_hostname"},{"location":"ansible/EX294/#facts","text":"# Display all facts - name: Fact dump hosts: all tasks: - name: Print all facts ansible.builtin.debug: var: ansible_facts Ansible Facts Injected as Variables Before Ansible 2.5, facts were always injected as individual variables prefixed with the string ansible_ instead of being part of the ansible_facts variable. For example, the ansible_facts['distribution'] fact was called ansible_distribution. ansible_facts.* name ansible_* name ansible_facts['hostname'] ansible_hostname ansible_facts['fqdn'] ansible_fqdn ansible_facts['default_ipv4']['address'] ansible_default_ipv4['address']","title":"Facts"},{"location":"ansible/EX294/#custom-facts","text":"By default, the ansible.builtin.setup module loads custom facts from files and scripts in the etc/ansible/facts.d. File shoudl end with \".fact\". The ansible.builtin.setup module stores custom facts in the ansible_facts['ansible_local'] variable. # file name should end with .fact [packages] web_package = httpd db_package = mariadb-server [users] user1 = joe user2 = jane","title":"Custom Facts"},{"location":"ansible/EX294/#packages-facts","text":"- name: Insure package facts gathered ansible.builtin.package_facts: - name: print package facts ansible.builtin.debug: var: ansible_facts.packages","title":"Packages Facts"},{"location":"ansible/EX294/#modifying-and-copying-files-to-hosts","text":"","title":"Modifying and Copying Files to Hosts"},{"location":"ansible/EX294/#ansiblebuiltin","text":"Module name Description blockinfile Insert/update/remove a text block surrounded by marker lines in a file copy Copy a file from the local or remote machine to a location on the managed hosts fetch Works like the copy module, but in reverse file Set attributes such as permissions, ownership, SELinux contexts, and time stamps of regular files, symlinks, hard links, and directories. This module can also create or remove regular files, symlinks, hard links, and directories. A number of other file-related modules support the same options to set attributes as the file module, including the copy module. lineinfile Ensure that a particular line is in a file, or replace an existing line using a back-reference regular expression. This module is primarily useful when you want to change a single line in a file. stat Retrieve status information for a file, similar to the Linux stat command.","title":"ansible.builtin"},{"location":"ansible/EX294/#ansibleposix","text":"Module name Description patch Apply a patch using GNU patch. synchronize A wrapper around the rsync command to simplify common tasks. The synchronize module is not intended to provide access to the full power of the rsync command, but does make the most common invocations easier to implement. You might still need to call the rsync command directly via the run command module depending on your use case.","title":"ansible.posix"},{"location":"ansible/EX294/#troubleshooting-ansible","text":"","title":"Troubleshooting Ansible"},{"location":"ansible/EX294/#ansiblebuiltinuri","text":"The ansible.builtin.uri module provides a way to verify that a RESTful API is returning the required content. tasks: - ansible.builtin.uri: url: http://api.myapp.example.com return_content: yes register: apiresponse - ansible.builtin.fail: msg: 'version was not provided' when: \"'version' not in apiresponse.content\"","title":"ansible.builtin.uri"},{"location":"ansible/EX294/#ansiblebuiltinstat","text":"You can use it to register a variable and then test to determine if the file exists or to get other information about the file tasks: - name: Check if /var/run/app.lock exists ansible.builtin.stat: path: /var/run/app.lock register: lock - name: Fail if the application is running ansible.builtin.fail: when: lock['stat']['exists']","title":"ansible.builtin.stat"},{"location":"ansible/EX294/#ansiblebuiltinassert","text":"tasks: - name: Check if /var/run/app.lock exists ansible.builtin.stat: path: /var/run/app.lock register: lock - name: Fail if the application is running ansible.builtin.assert: that: - not lock['stat']['exists']","title":"ansible.builtin.assert"},{"location":"ansible/EX294/#yaml","text":"YAML in a nutshell","title":"YAML"},{"location":"ansible/EX294/#special-characters","text":"Character Description ( | ) vertical ber to denote a new line characters ( > ) greater-than new line characters to be converted to spaces include_newlines: | Example Company 123 Main Street Atlanta, GA 30303 fold_newlines: > This is an example of a long string, that will become a single sentence once folded.","title":"Special characters"},{"location":"ansible/EX294/#yaml-dictionaries","text":"name: svcrole svcservice: httpd svcport: 80 {name: svcrole, svcservice: httpd, svcport: 80}","title":"YAML Dictionaries"},{"location":"ansible/EX294/#yaml-lists","text":"hosts: - servera - serverb - serverc hosts: [servera, serverb, serverc]","title":"YAML Lists"},{"location":"ansible/EX294/#jinja2","text":"","title":"Jinja2"},{"location":"ansible/EX294/#for-statement","text":"{# for statement - this is comment #} {% for user in users %} {{ user }} {% endfor %} {% for myhost in groups['myhosts'] %} {{ myhost }} {% endfor %}","title":"for statement"},{"location":"ansible/EX294/#conditional-statements","text":"{% if finished %} {{ result }} {% endif %}","title":"Conditional statements"},{"location":"ansible/EX294/#etchosts-file","text":"- name: /etc/hosts is up to date hosts: all gather_facts: yes tasks: - name: Deploy /etc/hosts ansible.builtin.template: src: templates/hosts.j2 dest: /etc/hosts {{ ansible_managed }} {% for host in groups['all'] %} {{ hostvars[host]['ansible_facts']['default_ipv4']['address'] }} {{ hostvars[host]['ansible_facts']['fqdn'] }} {{ hostvars[host]['ansible_facts']['hostname'] }} {% endfor %}","title":"/etc/hosts file"},{"location":"ansible/EX294/#jinja2-filters","text":"{{ 'hello world' | capitalize }} {{ output | to_json }} {{ output | to_yaml }}","title":"jinja2 filters"},{"location":"ansible/do374-1/","text":"Structure Ansible Project in Git inventories/ prod/ group_vars/ host_vars/ inventory/ stage/ group_vars/ host_vars/ inventory/ site.yml # main playbook webservers.yml # playbook for webserver tier dbservers.yml # playbook for dbserver tier collections/ # holds Ansible Content Collections in directories requirements.yml # specifies collections needed by this project roles/ common/ # this hierarchy represents a \"role\" tasks/ # main.yml # <-- tasks file can include smaller files if warranted handlers/ # main.yml # <-- handlers file templates/ # <-- files for use with the template resource ntp.conf.j2 # <------- templates end in .j2 files/ # bar.txt # <-- files for use with the copy resource myscript.sh # <-- script files for use with the script resource vars/ # main.yml # <-- variables associated with this role defaults/ # main.yml # <-- default lower priority variables for this role meta/ # main.yml # <-- role dependencies ...additional roles... .gitingore roles/** !roles/requirements.yml collections/ansible_collections ansible-navigator.~~log~~ *-artifact-* .ssh Testing - name: Start web server ansible.builtin.service: name: httpd status: started - name: Check web site from web server ansible.builtin.uri: url: http://{{ ansible_fqdn }} return_content: true register: example_webpage failed_when: example_webpage.status != 200","title":"Structure Ansible Project in Git"},{"location":"ansible/do374-1/#structure-ansible-project-in-git","text":"inventories/ prod/ group_vars/ host_vars/ inventory/ stage/ group_vars/ host_vars/ inventory/ site.yml # main playbook webservers.yml # playbook for webserver tier dbservers.yml # playbook for dbserver tier collections/ # holds Ansible Content Collections in directories requirements.yml # specifies collections needed by this project roles/ common/ # this hierarchy represents a \"role\" tasks/ # main.yml # <-- tasks file can include smaller files if warranted handlers/ # main.yml # <-- handlers file templates/ # <-- files for use with the template resource ntp.conf.j2 # <------- templates end in .j2 files/ # bar.txt # <-- files for use with the copy resource myscript.sh # <-- script files for use with the script resource vars/ # main.yml # <-- variables associated with this role defaults/ # main.yml # <-- default lower priority variables for this role meta/ # main.yml # <-- role dependencies ...additional roles... .gitingore roles/** !roles/requirements.yml collections/ansible_collections ansible-navigator.~~log~~ *-artifact-* .ssh","title":"Structure Ansible Project in Git"},{"location":"ansible/do374-1/#testing","text":"- name: Start web server ansible.builtin.service: name: httpd status: started - name: Check web site from web server ansible.builtin.uri: url: http://{{ ansible_fqdn }} return_content: true register: example_webpage failed_when: example_webpage.status != 200","title":"Testing"},{"location":"ansible/do374-2/","text":"Ch2 Managing Content Collections and Execution Environments module map Install collection","title":"Ch2 Managing Content Collections and Execution Environments"},{"location":"ansible/do374-2/#ch2-managing-content-collections-and-execution-environments","text":"module map","title":"Ch2 Managing Content Collections and Execution Environments"},{"location":"ansible/do374-2/#install-collection","text":"","title":"Install collection"},{"location":"ansible/do374-3/","text":"outstanding","title":"Do374 3"},{"location":"ansible/do374-4/","text":"Ansible Configuration ansible-navigator config Name Default Source Current 0\u2502Action warnings True default True 1\u2502Agnostic become prompt True default True ...output omitted... 44\u2502Default ask pass True default False 45\u2502Default ask vault pass True default False 46\u2502Default become False /home/.../ansible.cfg True 47\u2502Default become ask pass False /home/.../ansible.cfg False 48\u2502Default become exe True default None ...output omitted... 61\u2502Default forks False env 100 62\u2502Default gathering True default implicit ...output omitted... Configuration settings can come from: Specific environment variables A config file specified by the ANSIBLE_CONFIG env var Hard-coded default values Configuring Automation Content Navigator Automation content navigator looks for a settings file in the following order: ANSIBLE_NAVIGATOR_CONFIG env ansible-navigator.yml in the current directory ~/.ansible-navigator.yml in your home directory ansible-navigator.yml --- ansible-navigator: ...output omitted... execution-environment: container-engine: podman enabled: true image: ee-supported-rhel8:latest pull: policy: missing mode: stdout playbook-artifact: enable: false ...output omitted... # output a sample config ansible-navigator settings --sample # output the effective config ansible-navigator settings --effective","title":"Ansible Configuration"},{"location":"ansible/do374-4/#ansible-configuration","text":"ansible-navigator config Name Default Source Current 0\u2502Action warnings True default True 1\u2502Agnostic become prompt True default True ...output omitted... 44\u2502Default ask pass True default False 45\u2502Default ask vault pass True default False 46\u2502Default become False /home/.../ansible.cfg True 47\u2502Default become ask pass False /home/.../ansible.cfg False 48\u2502Default become exe True default None ...output omitted... 61\u2502Default forks False env 100 62\u2502Default gathering True default implicit ...output omitted... Configuration settings can come from: Specific environment variables A config file specified by the ANSIBLE_CONFIG env var Hard-coded default values","title":"Ansible Configuration"},{"location":"ansible/do374-4/#configuring-automation-content-navigator","text":"Automation content navigator looks for a settings file in the following order: ANSIBLE_NAVIGATOR_CONFIG env ansible-navigator.yml in the current directory ~/.ansible-navigator.yml in your home directory ansible-navigator.yml --- ansible-navigator: ...output omitted... execution-environment: container-engine: podman enabled: true image: ee-supported-rhel8:latest pull: policy: missing mode: stdout playbook-artifact: enable: false ...output omitted... # output a sample config ansible-navigator settings --sample # output the effective config ansible-navigator settings --effective","title":"Configuring Automation Content Navigator"},{"location":"ansible/do374-6/","text":"Controlling Privilege Escalation Privilege escalation directives: * become * become_user * become_method * become_flags Privilege Escalation by Configuration ansible.cfg [privilege_escalation] become = true become_user = true Directive Command-line option become --become or -b become_method --become-method=BECOME_METHOD become_user --become-user=BECOME_USER become_password --ask-become-pass or -k Privilege Escalation in Play, Task, Block and Role - name: Example play with one role hosts: localhost roles: - role: role-name become: true Listing Privilege Escalation with Connection Variables Connection variables override the become settings in the configuration file, as well as in plays, tasks, blocks, and roles. Directive Connection variable become ansible_become become_method ansible_become_method become_user ansible_become_user become_password ansible_become_pass webservers: hosts: servera.lab.example.com: serverb.lab.example.com: vars: ansible_become: true Controlling Task Execution In a play, Ansible always runs the tasks from roles, called by the roles statement, before the tasks that you define under the tasks section. Importing or Including Roles as a Task import_role - statically imports a role include_role - dynamically includes a role Pre- and Post-tasks pre_tasks is a tasks section that runs before the roles section. post_tasks is a tasks section that runs after the task section and handlers notified by tasks Reviewing the Order of Execution Ansible runs the play sections in the following order: pre_tasks Handlers that are notified in the pre_tasks section roles tasks Handlers that are notified in the roles and tasks sections post_tasks Handlers that are notified in the post_tasks section To immediately run any handlers that have been notified by a particular task in the play, add a task that uses the meta module with the flush_handlers parameter. This enables you to define specific points during task execution when all notified handlers are run. --- - name: Updating the application configuration and cache hosts: app_servers tasks: - name: Deploying the configuration file ansible.builtin.template: src: api-server.cfg.j2 dest: /etc/api-server.cfg notify: Restart api server - name: Running all notified handlers ansible.builtin.meta: flush_handlers - name: Asking the API server to rebuild its internal cache ansible.builtin.uri: url: \"https://{{ inventory_hostname }}/rest/api/2/cache/\" method: POST force_basic_auth: true user: admin password: redhat body_format: json body: type: data delay: 0 status_code: 201 handlers: - name: Restart api server ansible.builtin.service: name: api-server state: restarted enabled: true Listening to Handlers A task can notify multiple handlers in at least two ways: * It can notify a list of handlers individually by name. * It can notify one name for which multiple handlers are configured to listen. --- - name: Testing the listen directive hosts: localhost gather_facts: false become: false tasks: - name: Trigger handlers ansible.builtin.debug: msg: Trigerring the handlers notify: My handlers changed_when: true handlers: # Listening to the \"My handlers\" event - name: Listening to a notification ansible.builtin.debug: msg: First handler was notified listen: My handlers # As an example, this handler is also triggered because # its name matches the notification, but no two handlers # can have the same name. - name: My handlers ansible.builtin.debug: msg: Second handler was notified Controlling the Order of Host Execution By default, Ansible runs the play against hosts in the order in which they are listed in the inventory. You can change that order on a play-by-play basis by using the order directive. This playbook alphabetically sorts the hosts in the web_servers group before running the task: --- - name: Testing host order hosts: web_servers order: sorted # inventory(default) | sorted | reverse_sorted | reverse_inventory | shuffle tasks: - name: Creating a test file in /tmp ansible.builtin.copy: content: 'This is a sample file' dest: /tmp/test.out Tagging Ansible Resources Tags are available for the following resources: * plays * tasks - the most common ways that tags are used * role * blocks Options: * --tags * --skip-tags - skips the tasks that are tagged with the specified tags * --list-tags - lists all tags that are defined in the play Special tags always - a resource that is tagged with always is always run, regardless if it doesn't match the list of tags passed to the --tags option. The only exception is when it is explicitly skipped with --skip-tags . never - a task that you tag with the never tag does not run, unless you run the playbook with the --tags option set to never or to one of the other tags associated with the task. tagged - runs any resource with an explicit tag. untagged - runs any resource that does not have an explicit tag. all - runs all resources, regardless of their tags. This is default behavior of Ansible Optimizing Playbook Execution a way to measure playbook execution time: time ansible-navigator run -m stdout speed_facts.yml -i inventory If gathering facts isn't run substitute with magic variables: ansible_facts['hostname'] or ansible_hostname - inventory_hostname ansible_facts['nodename'] or ansible_nodename - inventory_hostname_short Reusing Gathered Facts with Fact Caching Ansible uses cache plug-ins to store gathered facts or inventory source data gathered by a play The memory cache plug-in is enabled by default This play book illustrates how cache works: - name: Gather facts for everyone hosts: all gather_facts: true # any tasks we might want for the first play # if you do not have tasks, \"setup\" will still run - name: The next play, does not gather facts hosts: all gather_facts: false tasks: - name: Show that we still know the facts ansible.builtin.debug: var: ansible_facts Another way to use fact caching is to use smart gathering. When enabled, smart gathering gathers facts on each new host in a playbook run, but if the same host is used across multiple plays, then the host is not contacted for fact gathering again in the run. [defaults] gathering=smart Limiting Fact Gathering - name: A play that gathers some facts hosts: all gather_facts: false tasks: - name: Collect only network-related facts ansible.builtin.setup: gather_subset: - '!all' - '!min' - network - hardware - virtual - ohai - facter Parallelism defaults] forks=100 -f option can be used to override the default value Avoiding Loops with the Package Manager Modules Efficiently Copying Files to Managed Hosts Using Templates When used with a loop, the ansible.builtin.lineinfile module is inefficient and can be error-prone, use either the ansible.builtin.template or ansible.builtin.copy module instead. Enabling Pipelining To run a task on a remote node, Ansible performs several SSH operations to copy the module and all its data to the remote node and run the module. To increase the performance of your playbook, you can activate the pipelining feature. With pipelining, Ansible establish fewer SSH connections. --- ansible-navigator: ansible: config: ./ansible.cfg execution-environment: image: ee-supported-rhel8:latest pull-policy: missing environment-variables: set: ANSIBLE_PIPELINING: true Ansible does not use pipelining by default because the feature requires that the requiretty sudo option on the remote node be disabled. Profiling Playbooks Execution with Callback Plug-ins Callback plug-ins extend Ansible by adjusting how it responds to various events. Some of these plug-ins modify the output of the command-line tools. ansible.cfg [defaults] callbacks_enabled=timer, profile_tasks, cgroup_perf_recap ansible-navigator doc -t callback -l -m stdout ansible-navigator doc -t callback cgroup_perf_recap -m stdout","title":"Controlling Privilege Escalation"},{"location":"ansible/do374-6/#controlling-privilege-escalation","text":"Privilege escalation directives: * become * become_user * become_method * become_flags","title":"Controlling Privilege Escalation"},{"location":"ansible/do374-6/#privilege-escalation-by-configuration","text":"ansible.cfg [privilege_escalation] become = true become_user = true Directive Command-line option become --become or -b become_method --become-method=BECOME_METHOD become_user --become-user=BECOME_USER become_password --ask-become-pass or -k","title":"Privilege Escalation by Configuration"},{"location":"ansible/do374-6/#privilege-escalation-in-play-task-block-and-role","text":"- name: Example play with one role hosts: localhost roles: - role: role-name become: true","title":"Privilege Escalation in Play, Task, Block and Role"},{"location":"ansible/do374-6/#listing-privilege-escalation-with-connection-variables","text":"Connection variables override the become settings in the configuration file, as well as in plays, tasks, blocks, and roles. Directive Connection variable become ansible_become become_method ansible_become_method become_user ansible_become_user become_password ansible_become_pass webservers: hosts: servera.lab.example.com: serverb.lab.example.com: vars: ansible_become: true","title":"Listing Privilege Escalation with Connection Variables"},{"location":"ansible/do374-6/#controlling-task-execution","text":"In a play, Ansible always runs the tasks from roles, called by the roles statement, before the tasks that you define under the tasks section.","title":"Controlling Task Execution"},{"location":"ansible/do374-6/#importing-or-including-roles-as-a-task","text":"import_role - statically imports a role include_role - dynamically includes a role","title":"Importing or Including Roles as a Task"},{"location":"ansible/do374-6/#pre-and-post-tasks","text":"pre_tasks is a tasks section that runs before the roles section. post_tasks is a tasks section that runs after the task section and handlers notified by tasks","title":"Pre- and Post-tasks"},{"location":"ansible/do374-6/#reviewing-the-order-of-execution","text":"Ansible runs the play sections in the following order: pre_tasks Handlers that are notified in the pre_tasks section roles tasks Handlers that are notified in the roles and tasks sections post_tasks Handlers that are notified in the post_tasks section To immediately run any handlers that have been notified by a particular task in the play, add a task that uses the meta module with the flush_handlers parameter. This enables you to define specific points during task execution when all notified handlers are run. --- - name: Updating the application configuration and cache hosts: app_servers tasks: - name: Deploying the configuration file ansible.builtin.template: src: api-server.cfg.j2 dest: /etc/api-server.cfg notify: Restart api server - name: Running all notified handlers ansible.builtin.meta: flush_handlers - name: Asking the API server to rebuild its internal cache ansible.builtin.uri: url: \"https://{{ inventory_hostname }}/rest/api/2/cache/\" method: POST force_basic_auth: true user: admin password: redhat body_format: json body: type: data delay: 0 status_code: 201 handlers: - name: Restart api server ansible.builtin.service: name: api-server state: restarted enabled: true","title":"Reviewing the Order of Execution"},{"location":"ansible/do374-6/#listening-to-handlers","text":"A task can notify multiple handlers in at least two ways: * It can notify a list of handlers individually by name. * It can notify one name for which multiple handlers are configured to listen. --- - name: Testing the listen directive hosts: localhost gather_facts: false become: false tasks: - name: Trigger handlers ansible.builtin.debug: msg: Trigerring the handlers notify: My handlers changed_when: true handlers: # Listening to the \"My handlers\" event - name: Listening to a notification ansible.builtin.debug: msg: First handler was notified listen: My handlers # As an example, this handler is also triggered because # its name matches the notification, but no two handlers # can have the same name. - name: My handlers ansible.builtin.debug: msg: Second handler was notified","title":"Listening to Handlers"},{"location":"ansible/do374-6/#controlling-the-order-of-host-execution","text":"By default, Ansible runs the play against hosts in the order in which they are listed in the inventory. You can change that order on a play-by-play basis by using the order directive. This playbook alphabetically sorts the hosts in the web_servers group before running the task: --- - name: Testing host order hosts: web_servers order: sorted # inventory(default) | sorted | reverse_sorted | reverse_inventory | shuffle tasks: - name: Creating a test file in /tmp ansible.builtin.copy: content: 'This is a sample file' dest: /tmp/test.out","title":"Controlling the Order of Host Execution"},{"location":"ansible/do374-6/#tagging-ansible-resources","text":"Tags are available for the following resources: * plays * tasks - the most common ways that tags are used * role * blocks Options: * --tags * --skip-tags - skips the tasks that are tagged with the specified tags * --list-tags - lists all tags that are defined in the play","title":"Tagging Ansible Resources"},{"location":"ansible/do374-6/#special-tags","text":"always - a resource that is tagged with always is always run, regardless if it doesn't match the list of tags passed to the --tags option. The only exception is when it is explicitly skipped with --skip-tags . never - a task that you tag with the never tag does not run, unless you run the playbook with the --tags option set to never or to one of the other tags associated with the task. tagged - runs any resource with an explicit tag. untagged - runs any resource that does not have an explicit tag. all - runs all resources, regardless of their tags. This is default behavior of Ansible","title":"Special tags"},{"location":"ansible/do374-6/#optimizing-playbook-execution","text":"a way to measure playbook execution time: time ansible-navigator run -m stdout speed_facts.yml -i inventory If gathering facts isn't run substitute with magic variables: ansible_facts['hostname'] or ansible_hostname - inventory_hostname ansible_facts['nodename'] or ansible_nodename - inventory_hostname_short","title":"Optimizing Playbook Execution"},{"location":"ansible/do374-6/#reusing-gathered-facts-with-fact-caching","text":"Ansible uses cache plug-ins to store gathered facts or inventory source data gathered by a play The memory cache plug-in is enabled by default This play book illustrates how cache works: - name: Gather facts for everyone hosts: all gather_facts: true # any tasks we might want for the first play # if you do not have tasks, \"setup\" will still run - name: The next play, does not gather facts hosts: all gather_facts: false tasks: - name: Show that we still know the facts ansible.builtin.debug: var: ansible_facts Another way to use fact caching is to use smart gathering. When enabled, smart gathering gathers facts on each new host in a playbook run, but if the same host is used across multiple plays, then the host is not contacted for fact gathering again in the run. [defaults] gathering=smart","title":"Reusing Gathered Facts with Fact Caching"},{"location":"ansible/do374-6/#limiting-fact-gathering","text":"- name: A play that gathers some facts hosts: all gather_facts: false tasks: - name: Collect only network-related facts ansible.builtin.setup: gather_subset: - '!all' - '!min' - network - hardware - virtual - ohai - facter","title":"Limiting Fact Gathering"},{"location":"ansible/do374-6/#parallelism","text":"defaults] forks=100 -f option can be used to override the default value","title":"Parallelism"},{"location":"ansible/do374-6/#avoiding-loops-with-the-package-manager-modules","text":"","title":"Avoiding Loops with the Package Manager Modules"},{"location":"ansible/do374-6/#efficiently-copying-files-to-managed-hosts","text":"","title":"Efficiently Copying Files to Managed Hosts"},{"location":"ansible/do374-6/#using-templates","text":"When used with a loop, the ansible.builtin.lineinfile module is inefficient and can be error-prone, use either the ansible.builtin.template or ansible.builtin.copy module instead.","title":"Using Templates"},{"location":"ansible/do374-6/#enabling-pipelining","text":"To run a task on a remote node, Ansible performs several SSH operations to copy the module and all its data to the remote node and run the module. To increase the performance of your playbook, you can activate the pipelining feature. With pipelining, Ansible establish fewer SSH connections. --- ansible-navigator: ansible: config: ./ansible.cfg execution-environment: image: ee-supported-rhel8:latest pull-policy: missing environment-variables: set: ANSIBLE_PIPELINING: true Ansible does not use pipelining by default because the feature requires that the requiretty sudo option on the remote node be disabled.","title":"Enabling Pipelining"},{"location":"ansible/do374-6/#profiling-playbooks-execution-with-callback-plug-ins","text":"Callback plug-ins extend Ansible by adjusting how it responds to various events. Some of these plug-ins modify the output of the command-line tools. ansible.cfg [defaults] callbacks_enabled=timer, profile_tasks, cgroup_perf_recap ansible-navigator doc -t callback -l -m stdout ansible-navigator doc -t callback cgroup_perf_recap -m stdout","title":"Profiling Playbooks Execution with Callback Plug-ins"},{"location":"ansible/do374-7-1/","text":"Ch.7 Transforming Data with Filters and Plug-ins Ansible data types Type Description String A sequence of characters. Number A numeric value. Booleans True or false values. Dates ISO-8601 calendar date. Null The variable become undefined. Lists or Arrays A sorted collection of values. Dictionaries A collection of key-value pairs. Processing Variables with Filters Ansible applies variable values to playbooks and templates by using Jinja2 expressions. Some filters are provided by the Jinja2 language; others are included with Red Hat Ansible Automation Platform as plug-ins. Get help for filters from ansible collections ansible-doc -t filter -l ansible-doc -t filter dict2items Default - name: Manage user ansible.builtin.user: name: \"{{ item['name'] }}\" groups: \"{{ item['groups'] | default(omit) }}\" # if the groups key is not defined for the item var system: \"{{ item['system'] | default(false) }}\" shell: \"{{ item['shell'] | default('/bin/bash') }}\" state: \"{{ item['state'] | default('present') }}\" remove: \"{{ item['remove'] | default(false) }}\" loop: \"{{ user_list }}\" The default filter only provides a value if a variable in not defined. Passing true as the second parameter will return the default value if the value is defined but blank or evaluates to the false boolean value. --- - name: Default filter examples hosts: localhost tasks: - name: Default filter examples vars: pattern: \"some text\" ansible.builtin.debug: msg: \"{{ item }}\" loop: - \"{{ pattern | regex_search('test') | default('MESSAGE') }}\" # Because the regular expression is not found in the variable, the regex_search filter returns an empty string. The default filter is not used. - \"{{ pattern | regex_search('test') | default('MESSAGE', true) }}\" # Although the regex_search filter returns an empty string, the default filter is used because it includes true. - \"{{ pattern | bool | default('MESSAGE') }}\" # Because the string evaluates to the false Boolean, the default filter is not used. - \"{{ pattern | bool | default('MESSAGE', true) }}\" # Although the string evaluates to the false Boolean, the default filter is used because it includes true. - name: Ensure httpd packages are installed ansible.builtin.yum: name: \"{{ apache_package_list }}\" state: present enablerepo: \"{{ apache_enablerepos_list | default(omit, true) }}\" int and float the following Jinja2 expression increments the current hour value, which is collected as a fact and stored as a string, not an integer: {{ ( ansible_facts['date_time']['hour'] | int ) + 1 }} Mathematical operations log , pow , root , abs , round root - square root of the variable or value. {{ 1764 | root }} Manipulating Lists max , min or sum {{ [2, 4, 6, 8, 10, 12] | sum }} extracting list elements - name: All three of these assertions are true ansible.builtin.assert: that: - \"{{ [ 2, 4, 6, 8, 10, 12 ] | length }} is eq( 6 )\" - \"{{ [ 2, 4, 6, 8, 10, 12 ] | first }} is eq( 2 )\" - \"{{ [ 2, 4, 6, 8, 10, 12 ] | last }} is eq( 12 )\" random - returns a random element {{ ['Douglas', 'Marvin', 'Arthur'] | random }} modifying the order of list elements shuffle - returns a list with the elements in a random order - name: reversing and sorting lists ansible.builtin.assert: that: - \"{{ [ 2, 4, 6, 8, 10 ] | reverse }} is eq( [ 10, 8, 6, 4, 2] )\" - \"{{ [ 4, 8, 10, 6, 2 ] | sort }} is eq( [ 2, 4, 6, 8, 10 ] )\" merging lists flatten - recursively takes any inner list in the input list value, and adds the inner values to the outer list. - name: Flatten turns nested lists on the left to list on the right ansible.builtin.assert: that: - \"{{ [ 2, [4, [6, 8]], 10 ] | flatten }} is eq( [ 2, 4, 6, 8, 10] )\" Operating on Lists as Sets - name: The 'unique' filter leaves unique elements ansible.builtin.assert: that: - \"{{ [ 1, 1, 2, 2, 2, 3, 4, 4 ] | unique }} is eq( [ 1, 2, 3, 4 ] )\" unique - ensure that a list has no duplicate elements. union - returns a set with elements form both input sets. intersect - returns a set with elements common to both sets. difference - returns a set with elements from the first set that are not present in the second set - name: The 'difference' filter provides elements not in specified set ansible.builtin.assert: that: - \"{{ [2, 4, 6, 8, 10] | difference([2, 4, 6, 16]) }} is eq( [8, 10] )\" Manipulating Dictionaries joining Dictionaries combine - joins two dictionaries. - name: The 'combine' filter combines two dictionaries into one vars: expected: A: 1 B: 4 C: 5 ansible.builtin.assert: that: - \"{{ {'A':1,'B':2} | combine({'B':4,'C':5}) }} is eq( expected )\" # Entries from the second dict have higher priority converting Dictionaries dict2items - filter to convert a dictionary to a list. items2dict - convert a list to a dictionary. - name: converting between dictionaries and lists vars: characters_dict: Douglas: Human Marvin: Robot Arthur: Human characters_items: - key: Douglas value: Human - key: Marvin value: Robot - key: Arthur value: Human ansible.builtin.assert: that: - \"{{ characters_dict | dict2items }} is eq( characters_items )\" - \"{{ characters_items | items2dict }} is eq( characters_dict )\" Hashing, Encoding, and Manipulating Strings hash - returns the hash value of the input string - name: the string's SHA-1 hash vars: expected: '8bae3f7d0a461488ced07b3e10ab80d018eb1d8c' ansible.builtin.assert: that: - \"'{{ 'Arthur' | hash('sha1') }}' is eq( expected )\" password_hash - generates password hashes {{ 'secret_password' | password_hash('sha512') }} b64encode - translate binary data to Base64 b64decode - encoded data back to binary data - name: Base64 encoding and decoding of values ansible.builtin.assert: that: - \"'{{ '\u00e2\u00c9\u00ef\u00f4\u00fa' | b64encode }}' is eq( 'w6LDicOvw7TDug==' )\" - \"'{{ 'w6LDicOvw7TDug==' | b64decode }}' is eq( '\u00e2\u00c9\u00ef\u00f4\u00fa' )\" quote - sanitize a string by using quote - name: Put quotes around 'my_string' shell: echo {{ my_string | quote }} formatting Text - name: Change case of characters ansible.builtin.assert: that: - \"'{{ 'Marvin' | lower }}' is eq( 'marvin' )\" - \"'{{ 'Marvin' | upper }}' is eq( 'MARVIN' )\" - \"'{{ 'marvin' | capitalize }}' is eq( 'Marvin' )\" replacing Text - name: Replace 'ar' with asterisks ansible.builtin.assert: that: - \"'{{ 'marvin, arthur' | replace('ar','**') }}' is eq( 'm**vin, **thur' )\" regex_search - complex search regex_replace - complex replacements - name: Test results of regex search and search-and-replace ansible.builtin.assert: that: - \"'{{ 'marvin, arthur' | regex_search('ar\\S*r') }}' is eq( 'arthur' )\" - \"'{{ 'arthur up' | regex_replace('ar(\\S*)r','\\\\1mb') }}' is eq( 'thumb up' )\" Manipulating Data Structures Many data structures used by Ansible are in JSON format. selectattr - selects a sequence of objects based on attributes of the objects in the list. map - turns a list of dictionaries into a simple list based on a given attribute. Although the community.general collection provides the json_query filter, you can usually achieve the same functionality using the selectattr and map filters. --- - name: Query automation controller execution environments hosts: localhost gather_facts: false tasks: - name: Query EEs vars: username_password: \"admin:redhat\" ansible.builtin.uri: url: https://controller.lab.example.com/api/v2/execution_environments/ method: GET headers: Authorization: Basic {{ username_password | string | b64encode }} validate_certs: false register: query_results - name: Show execution environment ID ansible.builtin.debug: msg: \"{{ query_results['json']['results'] | selectattr('name', '==', 'Control Plane Execution Environment') | map(attribute='id') | first }}\" --- - name: Find deployed webapp files ansible.builtin.find: paths: \"{{ webapp_content_root_dir }}\" recurse: true register: webapp_find_files - name: Compute the webapp file list ansible.builtin.set_fact: webapp_deployed_files: \"{{ webapp_find_files['files'] | map(attribute='path') | list }}\" # Alternatively you can let it invoke a filter by passing the name of the filter and the arguments afterwards - name: Compute the relative webapp file list ansible.builtin.set_fact: webapp_rel_deployed_files: \"{{ webapp_deployed_files | map('relpath', webapp_content_root_dir) | list }}\" Parsing and Encoding Data Structures to_json to_yaml to_nice_json to_nice_yaml - name: Convert between JSON and YAML format vars: hosts: - name: bastion ip: - 172.25.250.254 - 172.25.252.1 hosts_json: '[{\"name\": \"bastion\", \"ip\": [\"172.25.250.254\", \"172.25.252.1\"]}]' ansible.builtin.assert: that: - \"'{{ hosts | to_json }}' is eq( hosts_json )\"","title":"Ch.7 Transforming Data with Filters and Plug-ins"},{"location":"ansible/do374-7-1/#ch7-transforming-data-with-filters-and-plug-ins","text":"","title":"Ch.7 Transforming Data with Filters and Plug-ins"},{"location":"ansible/do374-7-1/#ansible-data-types","text":"Type Description String A sequence of characters. Number A numeric value. Booleans True or false values. Dates ISO-8601 calendar date. Null The variable become undefined. Lists or Arrays A sorted collection of values. Dictionaries A collection of key-value pairs.","title":"Ansible data types"},{"location":"ansible/do374-7-1/#processing-variables-with-filters","text":"Ansible applies variable values to playbooks and templates by using Jinja2 expressions. Some filters are provided by the Jinja2 language; others are included with Red Hat Ansible Automation Platform as plug-ins. Get help for filters from ansible collections ansible-doc -t filter -l ansible-doc -t filter dict2items","title":"Processing Variables with Filters"},{"location":"ansible/do374-7-1/#default","text":"- name: Manage user ansible.builtin.user: name: \"{{ item['name'] }}\" groups: \"{{ item['groups'] | default(omit) }}\" # if the groups key is not defined for the item var system: \"{{ item['system'] | default(false) }}\" shell: \"{{ item['shell'] | default('/bin/bash') }}\" state: \"{{ item['state'] | default('present') }}\" remove: \"{{ item['remove'] | default(false) }}\" loop: \"{{ user_list }}\" The default filter only provides a value if a variable in not defined. Passing true as the second parameter will return the default value if the value is defined but blank or evaluates to the false boolean value. --- - name: Default filter examples hosts: localhost tasks: - name: Default filter examples vars: pattern: \"some text\" ansible.builtin.debug: msg: \"{{ item }}\" loop: - \"{{ pattern | regex_search('test') | default('MESSAGE') }}\" # Because the regular expression is not found in the variable, the regex_search filter returns an empty string. The default filter is not used. - \"{{ pattern | regex_search('test') | default('MESSAGE', true) }}\" # Although the regex_search filter returns an empty string, the default filter is used because it includes true. - \"{{ pattern | bool | default('MESSAGE') }}\" # Because the string evaluates to the false Boolean, the default filter is not used. - \"{{ pattern | bool | default('MESSAGE', true) }}\" # Although the string evaluates to the false Boolean, the default filter is used because it includes true. - name: Ensure httpd packages are installed ansible.builtin.yum: name: \"{{ apache_package_list }}\" state: present enablerepo: \"{{ apache_enablerepos_list | default(omit, true) }}\"","title":"Default"},{"location":"ansible/do374-7-1/#int-and-float","text":"the following Jinja2 expression increments the current hour value, which is collected as a fact and stored as a string, not an integer: {{ ( ansible_facts['date_time']['hour'] | int ) + 1 }}","title":"int and float"},{"location":"ansible/do374-7-1/#mathematical-operations","text":"log , pow , root , abs , round root - square root of the variable or value. {{ 1764 | root }}","title":"Mathematical operations"},{"location":"ansible/do374-7-1/#manipulating-lists","text":"max , min or sum {{ [2, 4, 6, 8, 10, 12] | sum }} extracting list elements - name: All three of these assertions are true ansible.builtin.assert: that: - \"{{ [ 2, 4, 6, 8, 10, 12 ] | length }} is eq( 6 )\" - \"{{ [ 2, 4, 6, 8, 10, 12 ] | first }} is eq( 2 )\" - \"{{ [ 2, 4, 6, 8, 10, 12 ] | last }} is eq( 12 )\" random - returns a random element {{ ['Douglas', 'Marvin', 'Arthur'] | random }} modifying the order of list elements shuffle - returns a list with the elements in a random order - name: reversing and sorting lists ansible.builtin.assert: that: - \"{{ [ 2, 4, 6, 8, 10 ] | reverse }} is eq( [ 10, 8, 6, 4, 2] )\" - \"{{ [ 4, 8, 10, 6, 2 ] | sort }} is eq( [ 2, 4, 6, 8, 10 ] )\" merging lists flatten - recursively takes any inner list in the input list value, and adds the inner values to the outer list. - name: Flatten turns nested lists on the left to list on the right ansible.builtin.assert: that: - \"{{ [ 2, [4, [6, 8]], 10 ] | flatten }} is eq( [ 2, 4, 6, 8, 10] )\" Operating on Lists as Sets - name: The 'unique' filter leaves unique elements ansible.builtin.assert: that: - \"{{ [ 1, 1, 2, 2, 2, 3, 4, 4 ] | unique }} is eq( [ 1, 2, 3, 4 ] )\" unique - ensure that a list has no duplicate elements. union - returns a set with elements form both input sets. intersect - returns a set with elements common to both sets. difference - returns a set with elements from the first set that are not present in the second set - name: The 'difference' filter provides elements not in specified set ansible.builtin.assert: that: - \"{{ [2, 4, 6, 8, 10] | difference([2, 4, 6, 16]) }} is eq( [8, 10] )\"","title":"Manipulating Lists"},{"location":"ansible/do374-7-1/#manipulating-dictionaries","text":"joining Dictionaries combine - joins two dictionaries. - name: The 'combine' filter combines two dictionaries into one vars: expected: A: 1 B: 4 C: 5 ansible.builtin.assert: that: - \"{{ {'A':1,'B':2} | combine({'B':4,'C':5}) }} is eq( expected )\" # Entries from the second dict have higher priority converting Dictionaries dict2items - filter to convert a dictionary to a list. items2dict - convert a list to a dictionary. - name: converting between dictionaries and lists vars: characters_dict: Douglas: Human Marvin: Robot Arthur: Human characters_items: - key: Douglas value: Human - key: Marvin value: Robot - key: Arthur value: Human ansible.builtin.assert: that: - \"{{ characters_dict | dict2items }} is eq( characters_items )\" - \"{{ characters_items | items2dict }} is eq( characters_dict )\"","title":"Manipulating Dictionaries"},{"location":"ansible/do374-7-1/#hashing-encoding-and-manipulating-strings","text":"hash - returns the hash value of the input string - name: the string's SHA-1 hash vars: expected: '8bae3f7d0a461488ced07b3e10ab80d018eb1d8c' ansible.builtin.assert: that: - \"'{{ 'Arthur' | hash('sha1') }}' is eq( expected )\" password_hash - generates password hashes {{ 'secret_password' | password_hash('sha512') }} b64encode - translate binary data to Base64 b64decode - encoded data back to binary data - name: Base64 encoding and decoding of values ansible.builtin.assert: that: - \"'{{ '\u00e2\u00c9\u00ef\u00f4\u00fa' | b64encode }}' is eq( 'w6LDicOvw7TDug==' )\" - \"'{{ 'w6LDicOvw7TDug==' | b64decode }}' is eq( '\u00e2\u00c9\u00ef\u00f4\u00fa' )\" quote - sanitize a string by using quote - name: Put quotes around 'my_string' shell: echo {{ my_string | quote }} formatting Text - name: Change case of characters ansible.builtin.assert: that: - \"'{{ 'Marvin' | lower }}' is eq( 'marvin' )\" - \"'{{ 'Marvin' | upper }}' is eq( 'MARVIN' )\" - \"'{{ 'marvin' | capitalize }}' is eq( 'Marvin' )\" replacing Text - name: Replace 'ar' with asterisks ansible.builtin.assert: that: - \"'{{ 'marvin, arthur' | replace('ar','**') }}' is eq( 'm**vin, **thur' )\" regex_search - complex search regex_replace - complex replacements - name: Test results of regex search and search-and-replace ansible.builtin.assert: that: - \"'{{ 'marvin, arthur' | regex_search('ar\\S*r') }}' is eq( 'arthur' )\" - \"'{{ 'arthur up' | regex_replace('ar(\\S*)r','\\\\1mb') }}' is eq( 'thumb up' )\"","title":"Hashing, Encoding, and Manipulating Strings"},{"location":"ansible/do374-7-1/#manipulating-data-structures","text":"Many data structures used by Ansible are in JSON format. selectattr - selects a sequence of objects based on attributes of the objects in the list. map - turns a list of dictionaries into a simple list based on a given attribute. Although the community.general collection provides the json_query filter, you can usually achieve the same functionality using the selectattr and map filters. --- - name: Query automation controller execution environments hosts: localhost gather_facts: false tasks: - name: Query EEs vars: username_password: \"admin:redhat\" ansible.builtin.uri: url: https://controller.lab.example.com/api/v2/execution_environments/ method: GET headers: Authorization: Basic {{ username_password | string | b64encode }} validate_certs: false register: query_results - name: Show execution environment ID ansible.builtin.debug: msg: \"{{ query_results['json']['results'] | selectattr('name', '==', 'Control Plane Execution Environment') | map(attribute='id') | first }}\" --- - name: Find deployed webapp files ansible.builtin.find: paths: \"{{ webapp_content_root_dir }}\" recurse: true register: webapp_find_files - name: Compute the webapp file list ansible.builtin.set_fact: webapp_deployed_files: \"{{ webapp_find_files['files'] | map(attribute='path') | list }}\" # Alternatively you can let it invoke a filter by passing the name of the filter and the arguments afterwards - name: Compute the relative webapp file list ansible.builtin.set_fact: webapp_rel_deployed_files: \"{{ webapp_deployed_files | map('relpath', webapp_content_root_dir) | list }}\"","title":"Manipulating Data Structures"},{"location":"ansible/do374-7-1/#parsing-and-encoding-data-structures","text":"to_json to_yaml to_nice_json to_nice_yaml - name: Convert between JSON and YAML format vars: hosts: - name: bastion ip: - 172.25.250.254 - 172.25.252.1 hosts_json: '[{\"name\": \"bastion\", \"ip\": [\"172.25.250.254\", \"172.25.252.1\"]}]' ansible.builtin.assert: that: - \"'{{ hosts | to_json }}' is eq( hosts_json )\"","title":"Parsing and Encoding Data Structures"},{"location":"ansible/do374-7-2/","text":"Lookups (External Data) A lookup plug-in (keyword) is an Ansible extension to the Jinja2 templating language. These plug-ins enable Ansible to use data from external sources, such as files and the shell environment. You can call lookup plug-ins with one of two Jinja2 template functions, lookup or query . Both methods have a syntax that is similar to filters. Specify the name of the function, and in parentheses the name of the lookup plug-in that you want to call and any arguments that the plug-in needs. vars: hosts: \"{{ lookup('ansible.builtin.file', '/etc/hosts', '/etc/issue') }}\" the result hosts: \"127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4\\n::1 localhost localhost.localdomain localhost6 localhost6.localdomain6\\n\\n,\\\\S\\nKernel \\\\r on an \\\\m (\\\\l)\" query or q - a function that returns a list ( lookup returns values being comma-separated) vars: hosts: \"{{ query('ansible.builtin.file', '/etc/hosts', '/etc/issue') }}\" the result hosts: - \"127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4\\n::1 localhost localhost.localdomain localhost6 localhost6.localdomain6\\n\\n\" - \"\\\\S\\nKernel \\\\r on an \\\\m (\\\\l)\" list lookup plugins ansible-doc -t lookup -l print details about specific plug-in ansible-doc -t lookup onepassword Lookup plug-ins ansible.builtin.file - loads the contents of a local file into a variable - name: Add authorized keys hosts: all vars: users: - fred - naoko tasks: - name: Add authorized keys ansible.posix.authorized_key: user: \"{{ item }}\" key: \"{{ lookup('ansible.builtin.file', item + '.key.pub') }}\" loop: \"{{ users }}\" an example of reading a yaml file into a variable my_yaml: \"{{ lookup('ansible.builtin.file', '/path/to/my.yaml') | from_yaml }}\" ansible.builtin.template - return the contents of files # my_var.template.j2 {# Constracting the value of the powershell commands variable #} {% for module in pwsh['modules'] %} - module: {{ (module['name'] + \" \" + module['version'] | default(\"\")) | trim }} {% set params = \"-AcceptLicense -Scope AllUsers -ErrorAction Stop\"%} {% if module['version'] is defined %} {% set params = params + \" -RequiredVersion \" + module['version'] %} {% endif %} install_cmd: Install-Module {{ params + \" -Name \" + module['name']}} check_cmd: Get-InstalledModule -ErrorAction Stop -Name {{ module['name'] }} {% endfor %} # the playbook --- - name: Print \"Hello class.\" vars: pwsh: repositories: - name: jFrog-PSGallery-DEV url: https://jfrog.io/artifactory/api/nuget/cloud-dev-psgallery/ modules: - name: powershell-yaml version: 0.4.7 - name: Pester ps_commands: {{ lookup('ansible.builtin.template', 'my_var.template.j2') | from_yaml }}\" ansible.builtin.pipe - runs a command and returns the output generated by the command ansible.builtin.lines - same as pipe but splits the output of that command into lines {{ query('ansible.builtin.pipe', 'ls files') }} - name: Prints the first line of some files ansible.builtin.debug: msg: \"{{ item[0] }}\" loop: - \"{{ query('ansible.builtin.lines', 'cat files/my.file') }}\" - \"{{ query('ansible.builtin.lines', 'cat files/my.file.2') }}\" ansible.builtin.url - gets content from a URL {{ lookup('ansible.builtin.url', 'https://my.site.com/my.file') }} Custom lookup plug-ins Ansible is looking for plug-ins in: * COLLECTION-DIR/plugins/lookup directory * ROLE-DIR/filter_plugins directory * PROJECT-DIR/lookup_plugins directory Handling Lookup Errors {{ lookup('ansible.builtin.file', 'my.file', errors='warn') }}","title":"Lookups (External Data)"},{"location":"ansible/do374-7-2/#lookups-external-data","text":"A lookup plug-in (keyword) is an Ansible extension to the Jinja2 templating language. These plug-ins enable Ansible to use data from external sources, such as files and the shell environment. You can call lookup plug-ins with one of two Jinja2 template functions, lookup or query . Both methods have a syntax that is similar to filters. Specify the name of the function, and in parentheses the name of the lookup plug-in that you want to call and any arguments that the plug-in needs. vars: hosts: \"{{ lookup('ansible.builtin.file', '/etc/hosts', '/etc/issue') }}\" the result hosts: \"127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4\\n::1 localhost localhost.localdomain localhost6 localhost6.localdomain6\\n\\n,\\\\S\\nKernel \\\\r on an \\\\m (\\\\l)\" query or q - a function that returns a list ( lookup returns values being comma-separated) vars: hosts: \"{{ query('ansible.builtin.file', '/etc/hosts', '/etc/issue') }}\" the result hosts: - \"127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4\\n::1 localhost localhost.localdomain localhost6 localhost6.localdomain6\\n\\n\" - \"\\\\S\\nKernel \\\\r on an \\\\m (\\\\l)\" list lookup plugins ansible-doc -t lookup -l print details about specific plug-in ansible-doc -t lookup onepassword","title":"Lookups (External Data)"},{"location":"ansible/do374-7-2/#lookup-plug-ins","text":"ansible.builtin.file - loads the contents of a local file into a variable - name: Add authorized keys hosts: all vars: users: - fred - naoko tasks: - name: Add authorized keys ansible.posix.authorized_key: user: \"{{ item }}\" key: \"{{ lookup('ansible.builtin.file', item + '.key.pub') }}\" loop: \"{{ users }}\" an example of reading a yaml file into a variable my_yaml: \"{{ lookup('ansible.builtin.file', '/path/to/my.yaml') | from_yaml }}\" ansible.builtin.template - return the contents of files # my_var.template.j2 {# Constracting the value of the powershell commands variable #} {% for module in pwsh['modules'] %} - module: {{ (module['name'] + \" \" + module['version'] | default(\"\")) | trim }} {% set params = \"-AcceptLicense -Scope AllUsers -ErrorAction Stop\"%} {% if module['version'] is defined %} {% set params = params + \" -RequiredVersion \" + module['version'] %} {% endif %} install_cmd: Install-Module {{ params + \" -Name \" + module['name']}} check_cmd: Get-InstalledModule -ErrorAction Stop -Name {{ module['name'] }} {% endfor %} # the playbook --- - name: Print \"Hello class.\" vars: pwsh: repositories: - name: jFrog-PSGallery-DEV url: https://jfrog.io/artifactory/api/nuget/cloud-dev-psgallery/ modules: - name: powershell-yaml version: 0.4.7 - name: Pester ps_commands: {{ lookup('ansible.builtin.template', 'my_var.template.j2') | from_yaml }}\" ansible.builtin.pipe - runs a command and returns the output generated by the command ansible.builtin.lines - same as pipe but splits the output of that command into lines {{ query('ansible.builtin.pipe', 'ls files') }} - name: Prints the first line of some files ansible.builtin.debug: msg: \"{{ item[0] }}\" loop: - \"{{ query('ansible.builtin.lines', 'cat files/my.file') }}\" - \"{{ query('ansible.builtin.lines', 'cat files/my.file.2') }}\" ansible.builtin.url - gets content from a URL {{ lookup('ansible.builtin.url', 'https://my.site.com/my.file') }}","title":"Lookup plug-ins"},{"location":"ansible/do374-7-2/#custom-lookup-plug-ins","text":"Ansible is looking for plug-ins in: * COLLECTION-DIR/plugins/lookup directory * ROLE-DIR/filter_plugins directory * PROJECT-DIR/lookup_plugins directory","title":"Custom lookup plug-ins"},{"location":"ansible/do374-7-2/#handling-lookup-errors","text":"{{ lookup('ansible.builtin.file', 'my.file', errors='warn') }}","title":"Handling Lookup Errors"},{"location":"ansible/do374-7-3/","text":"Advanced Loops Iterating over a List of Lists - name: Remove build files ansible.builtin.file: path: \"{{ item }}\" state: absent loop: \"{{ list_of_lists | flatten(levels=1) }}\" vars: list_of_lists: - \"{{ app_a_tmp_files }}\" - \"{{ app_b_tmp_files }}\" - \"{{ app_c_tmp_files }}\" # app_X_tmp_files - a list of temporary files. Iterating over Nested Lists users: - name: paul password: \"{{ paul_pass }}\" authorized: - keys/paul_key1.pub - keys/paul_key2.pub mysql: hosts: - \"%\" - \"127.0.0.1\" - \"::1\" - \"localhost\" groups: - wheel - name: john password: \"{{ john_pass }}\" authorized: - keys/john_key.pub mysql: password: other-mysql-password hosts: - \"utility\" groups: - wheel - devops - name: Set authorized ssh key ansible.posix.authorized_key: user: \"{{ item[0]['name'] }}\" key: \"{{ lookup('ansible.builtin.file', item[1]) }}\" loop: \"{{ users | subelements('authorized') }}\" Iterating over a Dictionary users: demo1: name: Demo User 1 mail: demo1@example.com demo2: name: Demo User 2 mail: demo2@example.com demo200: name: Demo User 200 mail: demo200@example.com - name: Iterate over Users ansible.builtin.user: name: \"{{ item['key'] }}\" comment: \"{{ item['value']['name'] }}\" state: present loop: \"{{ users | dict2items }}\" Iterating over a File Globbing Pattern - name: Both tasks have the same result hosts: localhost gather_facts: false tasks: - name: Iteration Option One ansible.builtin.debug: msg: \"{{ item }}\" loop: \"{{ query('fileglob', '~/.bash*') }}\" Retrying a Task - name: Perform smoke test ansible.builtin.uri: url: \"https://{{ blue }}/status\" return_content: true register: smoke_test until: \"'STATUS_OK' in smoke_test['content']\" retries: 12 delay: 10","title":"Advanced Loops"},{"location":"ansible/do374-7-3/#advanced-loops","text":"Iterating over a List of Lists - name: Remove build files ansible.builtin.file: path: \"{{ item }}\" state: absent loop: \"{{ list_of_lists | flatten(levels=1) }}\" vars: list_of_lists: - \"{{ app_a_tmp_files }}\" - \"{{ app_b_tmp_files }}\" - \"{{ app_c_tmp_files }}\" # app_X_tmp_files - a list of temporary files. Iterating over Nested Lists users: - name: paul password: \"{{ paul_pass }}\" authorized: - keys/paul_key1.pub - keys/paul_key2.pub mysql: hosts: - \"%\" - \"127.0.0.1\" - \"::1\" - \"localhost\" groups: - wheel - name: john password: \"{{ john_pass }}\" authorized: - keys/john_key.pub mysql: password: other-mysql-password hosts: - \"utility\" groups: - wheel - devops - name: Set authorized ssh key ansible.posix.authorized_key: user: \"{{ item[0]['name'] }}\" key: \"{{ lookup('ansible.builtin.file', item[1]) }}\" loop: \"{{ users | subelements('authorized') }}\" Iterating over a Dictionary users: demo1: name: Demo User 1 mail: demo1@example.com demo2: name: Demo User 2 mail: demo2@example.com demo200: name: Demo User 200 mail: demo200@example.com - name: Iterate over Users ansible.builtin.user: name: \"{{ item['key'] }}\" comment: \"{{ item['value']['name'] }}\" state: present loop: \"{{ users | dict2items }}\" Iterating over a File Globbing Pattern - name: Both tasks have the same result hosts: localhost gather_facts: false tasks: - name: Iteration Option One ansible.builtin.debug: msg: \"{{ item }}\" loop: \"{{ query('fileglob', '~/.bash*') }}\"","title":"Advanced Loops"},{"location":"ansible/do374-7-3/#retrying-a-task","text":"- name: Perform smoke test ansible.builtin.uri: url: \"https://{{ blue }}/status\" return_content: true register: smoke_test until: \"'STATUS_OK' in smoke_test['content']\" retries: 12 delay: 10","title":"Retrying a Task"},{"location":"ansible/do374-7-4/","text":"Using Filters to Work with Network Addresses ansible.utils.ipaddr filter needs the netaddr Python module Testing IP Addresses {{ my_hosts_list | ansible.utils.ipaddr }} Filtering Data vars: listips: - 192.168.2.1 - 10.0.0.128/25 - 172.24.10.0/255.255.255.0 - 172.24.10.0/255.255.255.255 - ff02::1 - ::1 - 2001::1/64 - 2001::/64 - www.redhat.com msg: \"{{ listips | ansible.utils.ipaddr('netmask') }}\" # netmask, host, net, private # host \"msg\": [ \"192.168.2.1/32\", \"172.24.11.0/32\", \"ff02::1/128\", \"::1/128\", \"2001::1/64\" ] # private \"msg\": [ \"192.168.2.1\", \"10.0.0.128/25\", \"172.24.10.0/255.255.255.0\", \"172.24.10.0/255.255.255.255\" ] ansible.utils.ipwrap - puts brackets around the address part \"msg\": [ \"192.168.2.1\", \"10.0.0.128/25\", \"172.24.10.0/255.255.255.0\", \"172.24.10.0/255.255.255.255\", \"[ff02::1]\", \"[::1]\", \"[2001::1]/64\", \"[2001::]/64\", \"www.redhat.com\" ] # to remove domain www.redhat.com \"{{ listips | ansible.utils.ipaddr | ansible.utils.ipwrap }}\" Manipulating IP Addresses To return the address part, 192.0.2.1: \"{{ '192.0.2.1/24' | ansible.utils.ipaddr('address') }}\" To return the variable-length subnet mask, 255.255.255.0: \"{{ '192.0.2.1/24' | ansible.utils.ipaddr('netmask') }}\" To return the CIDR prefix, 24: \"{{ '192.0.2.1/24' | ansible.utils.ipaddr('prefix') }}\" To return the network's broadcast address, 192.0.2.255: \"{{ '192.0.2.1/24' | ansible.utils.ipaddr('broadcast') }}\" To return the network's network address, 192.0.2.0: \"{{ '192.0.2.1/24' | ansible.utils.ipaddr('network') }}\" To return the IP address in DNS PTR record format, 1.2.0.192.in-addr.arpa.: \"{{ '192.0.2.1/24' | ansible.utils.ipaddr('revdns') }}\" Reformatting or Calculating Network Information # 192.0.2.5/24 \"{{ '192.0.2.0/24' | ansible.utils.ipaddr(5) }}\" # 192.0.2.1-192.0.2.254 \"{{ '192.0.2.0/24' | ansible.utils.ipaddr('range_usable') }}\"","title":"Using Filters to Work with Network Addresses"},{"location":"ansible/do374-7-4/#using-filters-to-work-with-network-addresses","text":"ansible.utils.ipaddr filter needs the netaddr Python module Testing IP Addresses {{ my_hosts_list | ansible.utils.ipaddr }} Filtering Data vars: listips: - 192.168.2.1 - 10.0.0.128/25 - 172.24.10.0/255.255.255.0 - 172.24.10.0/255.255.255.255 - ff02::1 - ::1 - 2001::1/64 - 2001::/64 - www.redhat.com msg: \"{{ listips | ansible.utils.ipaddr('netmask') }}\" # netmask, host, net, private # host \"msg\": [ \"192.168.2.1/32\", \"172.24.11.0/32\", \"ff02::1/128\", \"::1/128\", \"2001::1/64\" ] # private \"msg\": [ \"192.168.2.1\", \"10.0.0.128/25\", \"172.24.10.0/255.255.255.0\", \"172.24.10.0/255.255.255.255\" ] ansible.utils.ipwrap - puts brackets around the address part \"msg\": [ \"192.168.2.1\", \"10.0.0.128/25\", \"172.24.10.0/255.255.255.0\", \"172.24.10.0/255.255.255.255\", \"[ff02::1]\", \"[::1]\", \"[2001::1]/64\", \"[2001::]/64\", \"www.redhat.com\" ] # to remove domain www.redhat.com \"{{ listips | ansible.utils.ipaddr | ansible.utils.ipwrap }}\" Manipulating IP Addresses To return the address part, 192.0.2.1: \"{{ '192.0.2.1/24' | ansible.utils.ipaddr('address') }}\" To return the variable-length subnet mask, 255.255.255.0: \"{{ '192.0.2.1/24' | ansible.utils.ipaddr('netmask') }}\" To return the CIDR prefix, 24: \"{{ '192.0.2.1/24' | ansible.utils.ipaddr('prefix') }}\" To return the network's broadcast address, 192.0.2.255: \"{{ '192.0.2.1/24' | ansible.utils.ipaddr('broadcast') }}\" To return the network's network address, 192.0.2.0: \"{{ '192.0.2.1/24' | ansible.utils.ipaddr('network') }}\" To return the IP address in DNS PTR record format, 1.2.0.192.in-addr.arpa.: \"{{ '192.0.2.1/24' | ansible.utils.ipaddr('revdns') }}\"","title":"Using Filters to Work with Network Addresses"},{"location":"ansible/do374-7-4/#reformatting-or-calculating-network-information","text":"# 192.0.2.5/24 \"{{ '192.0.2.0/24' | ansible.utils.ipaddr(5) }}\" # 192.0.2.1-192.0.2.254 \"{{ '192.0.2.0/24' | ansible.utils.ipaddr('range_usable') }}\"","title":"Reformatting or Calculating Network Information"},{"location":"ansible/do374-8/","text":"Ch.8 Rolling Updates Delegating Tasks In a play, you can delegate a task to run on another host instead of the current managed host . delegate_to - this directive points Ansible to the host that executes the task instead of the current managed host. --- - name: Delegation Example hosts: demo.lab.example.com become: false tasks: - name: Get system information ansible.builtin.command: uname -a register: managed_host - name: Display demo system information ansible.builtin.debug: var: managed_host - name: Get system information ansible.builtin.command: uname -a delegate_to: host.lab.example.com register: delegated - name: Display localhost system information ansible.builtin.debug: var: delegated - name: Remove the server from HAProxy community.general.haproxy: state: disabled host: \"{{ ansible_facts['fqdn'] }}\" socket: /var/lib/haproxy/stats delegate_to: \"{{ item }}\" loop: \"{{ groups['lbservers'] }}\" - name: Make sure Apache HTTPD is stopped ansible.builtin.service: name: httpd state: stopped - name: Access the web service uri: url: http://{{ ansible_facts['fqdn'] }} timeout: 5 delegate_to: test-client.example.com Delegating to Execution Environments One of the most common uses of delegation is to run tasks on a control node instead of a managed host . - name: Get information about controller instance ansible.builtin.uri: url: https://{{ ansible_facts['fqdn'] }}/api/v2/ping/ method: GET validate_certs: no return_content: yes delegate_to: localhost register: controller_ping Delegating Facts When you delegate a task, Ansible uses the host variable and facts for the managed host(the current inventory_hostname) for which the task is running. delegate_facts - assigns the facts collected by a delegated task to the host to which the task was delegated. - name: Delegate Fact Example hosts: localhost gather_facts: false tasks: - name: Set a fact in delegated task on demo ansible.builtin.set_fact: myfact: Where am I set? delegate_to: demo.lab.example.com delegate_facts: true - name: Display the facts from demo.lab.example.com ansible.builtin.debug: msg: \"{{ hostvars['demo.lab.example.com']['myfact'] }}\" Parallelism Forks When Ansible processes a playbook, it runs each play in order. After determining the list of hosts for the play, Ansible runs through each task in order. Normally, all hosts must successfully complete a task before any host starts the next task in the play. In theory, Ansible could connect to all hosts in the play simultaneously for each task. This approach works fine for small lists of hosts, but if the play targets hundreds of hosts, it can put a heavy load on the control node. forks - controls the number of hosts that Ansible connects to simultaneously. [user@host]$ ansible-navigator config dump -m stdout ...output omitted... DEFAULT_FORCE_HANDLERS(default) = False DEFAULT_FORKS(default) = 5 DEFAULT_GATHERING(default) = implicit ...output omitted... Serial Normally, when Ansible runs a play, it ensure that all managed hosts complete each task before it starts the next task. After all managed hosts have completed all tasks, then any notified handlers are run. However, running all tasks on all hosts can lead to undesirable behavior. For example, if a play updates a cluster of load-balanced web servers, it might need to take each web server out of service during the update. If all the servers are updated in the same play, they could all be out of service simultaneously. serial - controls the number of hosts that Ansible connects to simultaneously. # Ansible first runs the tasks (and handlers) on the first two managed hosts --- - name: Rolling update hosts: webservers serial: - 2 - 25% # If unprocessed hosts remain after the last batch corresponding to the previous serial directive entry, the last batch repeats until all hosts are processed. tasks: - name: Latest apache httpd package is installed ansible.builtin.yum: name: httpd state: latest notify: restart apache handlers: - name: Restart apache ansible.builtin.service: name: httpd state: restarted Aborting the Play ansible_play_batch - contains the list of hosts that Ansible is currently processing. max_fail_percentage - specifies the maximum percentage of hosts that can fail before Ansible aborts the play. To summarize the Ansible failure behavior: If the serial directive and the max_fail_percentage values are not defined, all hosts are run through the play in one batch. If all hosts fail, then the play fails. If the serial directive is defined, then hosts are run through the play in multiple batches, and the play fails if all hosts in any one batch fail. If the max_fail_percentage directive is defined, the play fails if more than that percentage of hosts in a batch fail. If a play fails, Ansible aborts all remaining plays in the playbook. Running a Task Once run_once - runs a task only once(one host per batch), even if the play targets multiple hosts. - name: Reactivate Hosts ansible.builtin.shell: /sbin/activate.sh {{ active_hosts_string }} run_once: true delegate_to: monitor.example.com vars: active_hosts_string: \"{{ ansible_play_batch | join(' ')}}\" # for a case with multiple batches, and you don't need to run a task on each batch. # when: inventory_hostname == ansible_play_hosts[0]","title":"Ch.8 Rolling Updates"},{"location":"ansible/do374-8/#ch8-rolling-updates","text":"","title":"Ch.8 Rolling Updates"},{"location":"ansible/do374-8/#delegating-tasks","text":"In a play, you can delegate a task to run on another host instead of the current managed host . delegate_to - this directive points Ansible to the host that executes the task instead of the current managed host. --- - name: Delegation Example hosts: demo.lab.example.com become: false tasks: - name: Get system information ansible.builtin.command: uname -a register: managed_host - name: Display demo system information ansible.builtin.debug: var: managed_host - name: Get system information ansible.builtin.command: uname -a delegate_to: host.lab.example.com register: delegated - name: Display localhost system information ansible.builtin.debug: var: delegated - name: Remove the server from HAProxy community.general.haproxy: state: disabled host: \"{{ ansible_facts['fqdn'] }}\" socket: /var/lib/haproxy/stats delegate_to: \"{{ item }}\" loop: \"{{ groups['lbservers'] }}\" - name: Make sure Apache HTTPD is stopped ansible.builtin.service: name: httpd state: stopped - name: Access the web service uri: url: http://{{ ansible_facts['fqdn'] }} timeout: 5 delegate_to: test-client.example.com","title":"Delegating Tasks"},{"location":"ansible/do374-8/#delegating-to-execution-environments","text":"One of the most common uses of delegation is to run tasks on a control node instead of a managed host . - name: Get information about controller instance ansible.builtin.uri: url: https://{{ ansible_facts['fqdn'] }}/api/v2/ping/ method: GET validate_certs: no return_content: yes delegate_to: localhost register: controller_ping","title":"Delegating to Execution Environments"},{"location":"ansible/do374-8/#delegating-facts","text":"When you delegate a task, Ansible uses the host variable and facts for the managed host(the current inventory_hostname) for which the task is running. delegate_facts - assigns the facts collected by a delegated task to the host to which the task was delegated. - name: Delegate Fact Example hosts: localhost gather_facts: false tasks: - name: Set a fact in delegated task on demo ansible.builtin.set_fact: myfact: Where am I set? delegate_to: demo.lab.example.com delegate_facts: true - name: Display the facts from demo.lab.example.com ansible.builtin.debug: msg: \"{{ hostvars['demo.lab.example.com']['myfact'] }}\"","title":"Delegating Facts"},{"location":"ansible/do374-8/#parallelism","text":"","title":"Parallelism"},{"location":"ansible/do374-8/#forks","text":"When Ansible processes a playbook, it runs each play in order. After determining the list of hosts for the play, Ansible runs through each task in order. Normally, all hosts must successfully complete a task before any host starts the next task in the play. In theory, Ansible could connect to all hosts in the play simultaneously for each task. This approach works fine for small lists of hosts, but if the play targets hundreds of hosts, it can put a heavy load on the control node. forks - controls the number of hosts that Ansible connects to simultaneously. [user@host]$ ansible-navigator config dump -m stdout ...output omitted... DEFAULT_FORCE_HANDLERS(default) = False DEFAULT_FORKS(default) = 5 DEFAULT_GATHERING(default) = implicit ...output omitted...","title":"Forks"},{"location":"ansible/do374-8/#serial","text":"Normally, when Ansible runs a play, it ensure that all managed hosts complete each task before it starts the next task. After all managed hosts have completed all tasks, then any notified handlers are run. However, running all tasks on all hosts can lead to undesirable behavior. For example, if a play updates a cluster of load-balanced web servers, it might need to take each web server out of service during the update. If all the servers are updated in the same play, they could all be out of service simultaneously. serial - controls the number of hosts that Ansible connects to simultaneously. # Ansible first runs the tasks (and handlers) on the first two managed hosts --- - name: Rolling update hosts: webservers serial: - 2 - 25% # If unprocessed hosts remain after the last batch corresponding to the previous serial directive entry, the last batch repeats until all hosts are processed. tasks: - name: Latest apache httpd package is installed ansible.builtin.yum: name: httpd state: latest notify: restart apache handlers: - name: Restart apache ansible.builtin.service: name: httpd state: restarted","title":"Serial"},{"location":"ansible/do374-8/#aborting-the-play","text":"ansible_play_batch - contains the list of hosts that Ansible is currently processing. max_fail_percentage - specifies the maximum percentage of hosts that can fail before Ansible aborts the play. To summarize the Ansible failure behavior: If the serial directive and the max_fail_percentage values are not defined, all hosts are run through the play in one batch. If all hosts fail, then the play fails. If the serial directive is defined, then hosts are run through the play in multiple batches, and the play fails if all hosts in any one batch fail. If the max_fail_percentage directive is defined, the play fails if more than that percentage of hosts in a batch fail. If a play fails, Ansible aborts all remaining plays in the playbook.","title":"Aborting the Play"},{"location":"ansible/do374-8/#running-a-task-once","text":"run_once - runs a task only once(one host per batch), even if the play targets multiple hosts. - name: Reactivate Hosts ansible.builtin.shell: /sbin/activate.sh {{ active_hosts_string }} run_once: true delegate_to: monitor.example.com vars: active_hosts_string: \"{{ ansible_play_batch | join(' ')}}\" # for a case with multiple batches, and you don't need to run a task on each batch. # when: inventory_hostname == ansible_play_hosts[0]","title":"Running a Task Once"},{"location":"ansible/do374-review/","text":"Git git config --global user.name \"admin\" git config --global user.email \"email@ya.ru\" git config --global push.default simple Ansible special tags always - always run the task (default) never - never run the task New collection meta/runtime.yml --- requires_ansible: \">=2.10\"","title":"Git"},{"location":"ansible/do374-review/#git","text":"git config --global user.name \"admin\" git config --global user.email \"email@ya.ru\" git config --global push.default simple","title":"Git"},{"location":"ansible/do374-review/#ansible-special-tags","text":"always - always run the task (default) never - never run the task","title":"Ansible special tags"},{"location":"ansible/do374-review/#new-collection","text":"meta/runtime.yml --- requires_ansible: \">=2.10\"","title":"New collection"},{"location":"container/","text":"Linux containers overview Containers Under the hood Container_images Compose Rootless_podman Troubleshooting","title":"Containers"},{"location":"container/#linux-containers-overview","text":"","title":"Linux containers overview"},{"location":"container/#containers","text":"","title":"Containers"},{"location":"container/#under-the-hood","text":"Container_images Compose Rootless_podman Troubleshooting","title":"Under the hood"},{"location":"container/Compose/","text":"Compose Compose spec https://github.com/compose-spec/compose-spec/blob/master/spec.md Basic commands podman-compose up # Options: # -d, --detach: Start containers in the background. # --force-recreate: Re-create containers on start. # -V, --renew-anon-volumes: Re-create anonymous volumes. # --remove-orphans: Remove containers that do not correspond to services that are defined in the current Compose file. podman-compose stop/dow podman-compose logs -n -f Podman Pods?? podman generate kube - generate a Kubernetes YAML file from a Podman pod definition podman play kube - create a pod from a Kubernetes YAML file The Compose File version (deprecated): Specifies the Compose version used. services : Defines the containers used. networks : Defines the networks used by the containers. volumes : Specifies the volumes used by the containers. configs : Specifies the configurations used by the containers. secrets : Defines the secrets used by the containers. Examples compose.yaml services: frontend: image: quay.io/example/frontend networks: - app-net ports: - \"8082:8080\" backend: image: quay.io/example/backend networks: - app-net - db-net depends_on: - db db: image: registry.redhat.io/rhel8/postgresql-13 environment: POSTGRESQL_ADMIN_PASSWORD: redhat networks: - db-net volumes: - db-vol:/var/lib/postgresql/data networks: app-net: {} db-net: {} volume: db-vol: {} Installing Podman Compose pip3 install podman-compose Links Podman Compose Vs Docker Compose","title":"Compose"},{"location":"container/Compose/#compose","text":"","title":"Compose"},{"location":"container/Compose/#compose-spec","text":"https://github.com/compose-spec/compose-spec/blob/master/spec.md","title":"Compose spec"},{"location":"container/Compose/#basic-commands","text":"podman-compose up # Options: # -d, --detach: Start containers in the background. # --force-recreate: Re-create containers on start. # -V, --renew-anon-volumes: Re-create anonymous volumes. # --remove-orphans: Remove containers that do not correspond to services that are defined in the current Compose file. podman-compose stop/dow podman-compose logs -n -f","title":"Basic commands"},{"location":"container/Compose/#podman-pods","text":"podman generate kube - generate a Kubernetes YAML file from a Podman pod definition podman play kube - create a pod from a Kubernetes YAML file","title":"Podman Pods??"},{"location":"container/Compose/#the-compose-file","text":"version (deprecated): Specifies the Compose version used. services : Defines the containers used. networks : Defines the networks used by the containers. volumes : Specifies the volumes used by the containers. configs : Specifies the configurations used by the containers. secrets : Defines the secrets used by the containers.","title":"The Compose File"},{"location":"container/Compose/#examples","text":"compose.yaml services: frontend: image: quay.io/example/frontend networks: - app-net ports: - \"8082:8080\" backend: image: quay.io/example/backend networks: - app-net - db-net depends_on: - db db: image: registry.redhat.io/rhel8/postgresql-13 environment: POSTGRESQL_ADMIN_PASSWORD: redhat networks: - db-net volumes: - db-vol:/var/lib/postgresql/data networks: app-net: {} db-net: {} volume: db-vol: {}","title":"Examples"},{"location":"container/Compose/#installing-podman-compose","text":"pip3 install podman-compose","title":"Installing Podman Compose"},{"location":"container/Compose/#links","text":"Podman Compose Vs Docker Compose","title":"Links"},{"location":"container/Container_images/","text":"Container images Basic commands Inspect podman images --format=\"{{.Names}}\\t{{.Size}}\" podman image inspect imageID podman image tree hello-server:bad Search images in registry podman search nginx Build image podman build -t localhost/my-container -f Containerfile.my-container podman build --squash -t localhost/squashed . podman build --squash-all -t localhost/squashed . Remove all images podman rmi --all # or podman image rm --all Removes dangling images. (without tags and that aren't referenced by other images) podman image prune With option --all removes all unused images podman image prune -af Save/load images to/from tarball podman save --output images.tar \\ docker.io/library/redis \\ docker.io/library/mysql podman load --input images.tar Login to registry podman login registry.redhat.io Podman stores the credentials in the ${XDG_RUNTIME_DIR}/containers/auth.json file auth.json example [user@host ~]$ cat ${XDG_RUNTIME_DIR}/containers/auth.json { \"auths\": { \"registry.redhat.io\": { \"auth\": \"dXNlcjpodW50ZXIy\" } } } [user@host ~]$ echo -n dXNlcjpodW50ZXIy | base64 -d user:hunter2 Container image naming [\\ /\\ /]\\ [:\\ ] MAJOR.MINOR.PATCH meaning: MAJOR: backward incompatible changes MINOR: backward compatible changes PATCH: bug fixes more about tags Container Registry Podman registry config podman registries config file location grep ^[^#] /etc/containers/registries.conf RedHat registries registry.access.redhat.com # requires no authentication registry.redhat.io # requires authentication registry.connect.redhat.com # requires authentication third-party products quay.io # redhat public registry Useful images RehHat UBI images UBI - universal base images Standard : This is the primary UBI, which includes DNF, systemd, and utilities such as gzip and tar. Init : Simplifies running multiple applications within a single container by managing them with systemd. Minimal : This image is smaller than the init image and still provides nice-to-have features. This image uses the microdnf minimal package manager instead of the full-sized version of DNF. Micro : This is the smallest available UBI because it only includes the bare minimum number of packages. For example, this image does not include a package manager. registry.access.redhat.com/ubi9 # RedHat Universal Base Image registry.access.redhat.com/ubi9/python-39 # Python 3.9 on UBI9 Run your own registry Quay container registry - container image podman pull registry.redhat.io/quay/quay-rhel8 Nexus - the artifact repository podman pull dockette/nexus:latest Docker registry # can proxy other registries docker run -d -p 5000:5000 --name registry docker.io/registry:latest Build Basic commands podman build -t localhost/not-squashed . # squash CoW layers podman build --squash -t localhost/squashed . podman build --squash-all -t localhost/squashed . Containerfile Instructions Containerfiles use a small domain-specific language (DSL) FROM FROM registry.access.redhat.com/ubi8/ubi-minimal:latest ARG (Defines build-time variables, typically to make a customizable container build) ENV (You can declare multiple ENV instructions within the Containerfile) ARG VERSION=\"1.16.8\" BIN_DIR=/usr/local/bin/ ENV VERSION=${VERSION} \\ BIN_DIR=${BIN_DIR} RUN curl \"https://dl.example.io/${VERSION}/example-linux-amd64\" \\ -o ${BIN_DIR}/example WORKDIR (Sets the working directory for subsequent instructions) WORKDIR /opt/app-root/src COPY and ADD COPY --chown=1001:0 app.js /opt/app-root/src/ ADD https://example-linux-amd64 /usr/local/bin/example # The ADD instruction adds the following functionality: # * Copying files from URLs. # * Unpacking tar archives in the destination image. # Because the ADD instruction adds functionality that might not be obvious, developers tend to prefer the COPY instruction for copying local files into the container image. RUN (Executes a command and creates a new layer) RUN yum install -y httpd USER (Instructions that follow the USER instruction run as this user, including the CMD instruction.) USER 1001 LABEL (Adds a key-value pair to the metadata of the image for organization and image selection) LABEL name=\"example\" \\ version=\"1.0\" \\ release=\"1\" \\ summary=\"Example application\" \\ description=\"Example application for demonstrating Containerfiles\" EXPOSE (This instruction does not bind the port on the host and is for documentation purposes) EXPOSE 8080 VOLUME (A data volume is a specially-designated directory within one or more containers that bypasses the Union File System.) VOLUME /var/lib/mysql ENTRYPOINT (Sets the executable to run when the container is started) ENTRYPOINT [\"executable\", \"param1\", ... \"paramN\"] CMD (Runs a command when the container is started. This command is passed to the executable defined by ENTRYPOINT. Base images define a default ENTRYPOINT, which is usually a shell executable, such as Bash.) CMD [\"echo\", \"Hello\", \"Red Hat\"] Neither ENTRYPOINT nor CMD run when building a container image. Podman executes them when you start a container from the image. Multistage Builds # First stage FROM registry.access.redhat.com/ubi8/nodejs-14:1 as builder COPY ./ /opt/app-root/src/ RUN npm install RUN npm run build # Second stage FROM registry.access.redhat.com/ubi8/nginx-120 COPY --from=builder /opt/app-root/src/ /usr/share/nginx/html Utility scopeo is a command line utility that allows you to inspect and manage container images. skopeo copy --dest-tls-verify=false \\ docker://${RHOCP_REGISTRY}/default/python:3.9-ubi8 \\ docker://registry.ocp4.example.com:8443/developer/python:3.9-ubi8 buildah - build container images from Dockerfiles","title":"Container images"},{"location":"container/Container_images/#container-images","text":"","title":"Container images"},{"location":"container/Container_images/#basic-commands","text":"","title":"Basic commands"},{"location":"container/Container_images/#inspect","text":"podman images --format=\"{{.Names}}\\t{{.Size}}\" podman image inspect imageID podman image tree hello-server:bad","title":"Inspect"},{"location":"container/Container_images/#search-images-in-registry","text":"podman search nginx","title":"Search images in registry"},{"location":"container/Container_images/#build-image","text":"podman build -t localhost/my-container -f Containerfile.my-container podman build --squash -t localhost/squashed . podman build --squash-all -t localhost/squashed .","title":"Build image"},{"location":"container/Container_images/#remove-all-images","text":"podman rmi --all # or podman image rm --all Removes dangling images. (without tags and that aren't referenced by other images) podman image prune With option --all removes all unused images podman image prune -af","title":"Remove all images"},{"location":"container/Container_images/#saveload-images-tofrom-tarball","text":"podman save --output images.tar \\ docker.io/library/redis \\ docker.io/library/mysql podman load --input images.tar","title":"Save/load images to/from tarball"},{"location":"container/Container_images/#login-to-registry","text":"podman login registry.redhat.io Podman stores the credentials in the ${XDG_RUNTIME_DIR}/containers/auth.json file auth.json example [user@host ~]$ cat ${XDG_RUNTIME_DIR}/containers/auth.json { \"auths\": { \"registry.redhat.io\": { \"auth\": \"dXNlcjpodW50ZXIy\" } } } [user@host ~]$ echo -n dXNlcjpodW50ZXIy | base64 -d user:hunter2","title":"Login to registry"},{"location":"container/Container_images/#container-image-naming","text":"[\\ /\\ /]\\ [:\\ ] MAJOR.MINOR.PATCH meaning: MAJOR: backward incompatible changes MINOR: backward compatible changes PATCH: bug fixes more about tags","title":"Container image naming"},{"location":"container/Container_images/#container-registry","text":"","title":"Container Registry"},{"location":"container/Container_images/#podman-registry-config","text":"podman registries config file location grep ^[^#] /etc/containers/registries.conf","title":"Podman registry config"},{"location":"container/Container_images/#redhat-registries","text":"registry.access.redhat.com # requires no authentication registry.redhat.io # requires authentication registry.connect.redhat.com # requires authentication third-party products quay.io # redhat public registry","title":"RedHat registries"},{"location":"container/Container_images/#useful-images","text":"RehHat UBI images UBI - universal base images Standard : This is the primary UBI, which includes DNF, systemd, and utilities such as gzip and tar. Init : Simplifies running multiple applications within a single container by managing them with systemd. Minimal : This image is smaller than the init image and still provides nice-to-have features. This image uses the microdnf minimal package manager instead of the full-sized version of DNF. Micro : This is the smallest available UBI because it only includes the bare minimum number of packages. For example, this image does not include a package manager. registry.access.redhat.com/ubi9 # RedHat Universal Base Image registry.access.redhat.com/ubi9/python-39 # Python 3.9 on UBI9","title":"Useful images"},{"location":"container/Container_images/#run-your-own-registry","text":"Quay container registry - container image podman pull registry.redhat.io/quay/quay-rhel8 Nexus - the artifact repository podman pull dockette/nexus:latest Docker registry # can proxy other registries docker run -d -p 5000:5000 --name registry docker.io/registry:latest","title":"Run your own registry"},{"location":"container/Container_images/#build","text":"","title":"Build"},{"location":"container/Container_images/#basic-commands_1","text":"podman build -t localhost/not-squashed . # squash CoW layers podman build --squash -t localhost/squashed . podman build --squash-all -t localhost/squashed .","title":"Basic commands"},{"location":"container/Container_images/#containerfile-instructions","text":"Containerfiles use a small domain-specific language (DSL) FROM FROM registry.access.redhat.com/ubi8/ubi-minimal:latest ARG (Defines build-time variables, typically to make a customizable container build) ENV (You can declare multiple ENV instructions within the Containerfile) ARG VERSION=\"1.16.8\" BIN_DIR=/usr/local/bin/ ENV VERSION=${VERSION} \\ BIN_DIR=${BIN_DIR} RUN curl \"https://dl.example.io/${VERSION}/example-linux-amd64\" \\ -o ${BIN_DIR}/example WORKDIR (Sets the working directory for subsequent instructions) WORKDIR /opt/app-root/src COPY and ADD COPY --chown=1001:0 app.js /opt/app-root/src/ ADD https://example-linux-amd64 /usr/local/bin/example # The ADD instruction adds the following functionality: # * Copying files from URLs. # * Unpacking tar archives in the destination image. # Because the ADD instruction adds functionality that might not be obvious, developers tend to prefer the COPY instruction for copying local files into the container image. RUN (Executes a command and creates a new layer) RUN yum install -y httpd USER (Instructions that follow the USER instruction run as this user, including the CMD instruction.) USER 1001 LABEL (Adds a key-value pair to the metadata of the image for organization and image selection) LABEL name=\"example\" \\ version=\"1.0\" \\ release=\"1\" \\ summary=\"Example application\" \\ description=\"Example application for demonstrating Containerfiles\" EXPOSE (This instruction does not bind the port on the host and is for documentation purposes) EXPOSE 8080 VOLUME (A data volume is a specially-designated directory within one or more containers that bypasses the Union File System.) VOLUME /var/lib/mysql ENTRYPOINT (Sets the executable to run when the container is started) ENTRYPOINT [\"executable\", \"param1\", ... \"paramN\"] CMD (Runs a command when the container is started. This command is passed to the executable defined by ENTRYPOINT. Base images define a default ENTRYPOINT, which is usually a shell executable, such as Bash.) CMD [\"echo\", \"Hello\", \"Red Hat\"] Neither ENTRYPOINT nor CMD run when building a container image. Podman executes them when you start a container from the image.","title":"Containerfile Instructions"},{"location":"container/Container_images/#multistage-builds","text":"# First stage FROM registry.access.redhat.com/ubi8/nodejs-14:1 as builder COPY ./ /opt/app-root/src/ RUN npm install RUN npm run build # Second stage FROM registry.access.redhat.com/ubi8/nginx-120 COPY --from=builder /opt/app-root/src/ /usr/share/nginx/html","title":"Multistage Builds"},{"location":"container/Container_images/#utility","text":"scopeo is a command line utility that allows you to inspect and manage container images. skopeo copy --dest-tls-verify=false \\ docker://${RHOCP_REGISTRY}/default/python:3.9-ubi8 \\ docker://registry.ocp4.example.com:8443/developer/python:3.9-ubi8 buildah - build container images from Dockerfiles","title":"Utility"},{"location":"container/Containers/","text":"Podman/Docker (OCI - Open Container Initiative) conmon conmon is a monitoring tool for OCI containers. It is used by Podman and CRI-O to monitor the state of containers. podman run -d nginx ps -ef | grep conmon runc list kaniko kaniko vs buildah Get Help Inline help podman --help Man pages are available for each command. For example: man podman-run man podman-build Basic command examples Run = Create + Start podman create ... podman run ... podman start ... podman stop sends SIGTERM to the container and waits 10 sec, then send SIGKILL podman stop $(podman ps -aq) podman kill sends SIGKILL to stop the container immediately podman kill containerID podman pause/unpuase sends SIGSTOP/SIGCONT to the container to all processes in the container. it requires cgroups v2, it's not enabled on RHEL8 by default. podman pause 4f2038c05b8c # -i - interactive, -t - tty podman exec -it <ContainerId> /bin/sh podman ps -a podman logs containerID podman inspect --format='{{.State.Running}}' containerID podman pull podman image ls podman rm $(podman ps -aq) # remove all stopped containers podman rm --all # build podman build -t localhost/my-container -f Containerfile.my-container Network podman network --help Network specific commands podman network create example-net podman network ls podman network inspect example-net # port mapping podman port example-net # Containers can be connected to multiple networks by specifying network names in a comma-separated list podman run -d --name double-connector --net postgres-net,redis-net container-image:latest # If the a container is already running, the following command connects it to the example-net network podman network connect example-net my-container # Removes any networks that are not currently in use by any containers podman network prune DNS DNS is disabled in the default podman network. To enable DNS resolution between containers, create a Podman network and connect your containers to that network. Port-Forwarding -p or --publish option of the podman run command forwards a port HOST_PORT:CONTAINER_PORT podman run -p 8075:80 my-app podman run -p 127.0.0.1:8075:80 my-app # List port mappings podman port my-app # --all option lists port mappings for all containers podman port --all podman inspect my-app -f '{{.NetworkSettings.Networks.apps.IPAddress}}' Storage Storage commands podman inspect volumeID podman volume prune podman volume create volumeNAME podman volume ls --format=\"{{.Name}}\\t{{.Mountpoint}}\" diff - display the changes made to a container's filesystem since it was started podman diff elastic_maxwell cp - copy files/folders between a container and the local filesystem podman cp index.html elastic_maxwell:/var/wwww/index.html Volumes and Bind mounts Volumes - managed by Podman podman volume create http-data podman volume inspect http-data for rootless containers, Podman stores volume in the $HOME/.local/share/containers/storage/volumes directory. Bind mounts - can exist anywhere on the host filesystem Both volumes and bind mounts can use --volume or -v parameter # --volume /path/on/host:/path/in/container:OPTIONS # Bind mounts with the read-only option podman run --volume /www:/var/www/html:ro ubi8/httpd-24:latest # Volume mount into a container podman run --volume http-data:/var/www/html ubi8/httpd-24:latest ## Because Podman manages the volume, you do not need to configure SELinux permissions. Alternatively, you can use the --mount parameter --mount type=TYPE,source=/path/on/host,destination=/path/in/container,options=OPTIONS # type= bind | volume | tmpfs # options=ro | rw | z | Z Some application cannot use the default COW file system in a specific directory for performance reasons, but do not need persistence or data sharing. In this case, you can use the tmpfs mount type, which means that the data in mount is ephemeral but does not use the COW file system. podman run -e POSTGRESQL_ADMIN_PASSWORD=redhat --network lab-net \\ --mount type=tmpfs,tmpfs-size=512M,destination=/var/lib/pgsql/data \\ registry.redhat.io/rhel9/postgresql-13:1 Import/export volumes podman volume export http_data --output web_data.tar.gz podman volume import web_data.tar.gz Links Storage Overlay.go SELinux and Container File Permissions Networking Podman 4 network nsenter","title":"Podman/Docker (OCI - Open Container Initiative)"},{"location":"container/Containers/#podmandocker-oci-open-container-initiative","text":"","title":"Podman/Docker (OCI - Open Container Initiative)"},{"location":"container/Containers/#conmon","text":"conmon is a monitoring tool for OCI containers. It is used by Podman and CRI-O to monitor the state of containers. podman run -d nginx ps -ef | grep conmon runc list","title":"conmon"},{"location":"container/Containers/#kaniko","text":"kaniko vs buildah","title":"kaniko"},{"location":"container/Containers/#get-help","text":"Inline help podman --help Man pages are available for each command. For example: man podman-run man podman-build","title":"Get Help"},{"location":"container/Containers/#basic-command-examples","text":"Run = Create + Start podman create ... podman run ... podman start ... podman stop sends SIGTERM to the container and waits 10 sec, then send SIGKILL podman stop $(podman ps -aq) podman kill sends SIGKILL to stop the container immediately podman kill containerID podman pause/unpuase sends SIGSTOP/SIGCONT to the container to all processes in the container. it requires cgroups v2, it's not enabled on RHEL8 by default. podman pause 4f2038c05b8c # -i - interactive, -t - tty podman exec -it <ContainerId> /bin/sh podman ps -a podman logs containerID podman inspect --format='{{.State.Running}}' containerID podman pull podman image ls podman rm $(podman ps -aq) # remove all stopped containers podman rm --all # build podman build -t localhost/my-container -f Containerfile.my-container","title":"Basic command examples"},{"location":"container/Containers/#network","text":"podman network --help","title":"Network"},{"location":"container/Containers/#network-specific-commands","text":"podman network create example-net podman network ls podman network inspect example-net # port mapping podman port example-net # Containers can be connected to multiple networks by specifying network names in a comma-separated list podman run -d --name double-connector --net postgres-net,redis-net container-image:latest # If the a container is already running, the following command connects it to the example-net network podman network connect example-net my-container # Removes any networks that are not currently in use by any containers podman network prune","title":"Network specific commands"},{"location":"container/Containers/#dns","text":"DNS is disabled in the default podman network. To enable DNS resolution between containers, create a Podman network and connect your containers to that network.","title":"DNS"},{"location":"container/Containers/#port-forwarding","text":"-p or --publish option of the podman run command forwards a port HOST_PORT:CONTAINER_PORT podman run -p 8075:80 my-app podman run -p 127.0.0.1:8075:80 my-app # List port mappings podman port my-app # --all option lists port mappings for all containers podman port --all podman inspect my-app -f '{{.NetworkSettings.Networks.apps.IPAddress}}'","title":"Port-Forwarding"},{"location":"container/Containers/#storage","text":"","title":"Storage"},{"location":"container/Containers/#storage-commands","text":"podman inspect volumeID podman volume prune podman volume create volumeNAME podman volume ls --format=\"{{.Name}}\\t{{.Mountpoint}}\" diff - display the changes made to a container's filesystem since it was started podman diff elastic_maxwell cp - copy files/folders between a container and the local filesystem podman cp index.html elastic_maxwell:/var/wwww/index.html","title":"Storage commands"},{"location":"container/Containers/#volumes-and-bind-mounts","text":"Volumes - managed by Podman podman volume create http-data podman volume inspect http-data for rootless containers, Podman stores volume in the $HOME/.local/share/containers/storage/volumes directory. Bind mounts - can exist anywhere on the host filesystem Both volumes and bind mounts can use --volume or -v parameter # --volume /path/on/host:/path/in/container:OPTIONS # Bind mounts with the read-only option podman run --volume /www:/var/www/html:ro ubi8/httpd-24:latest # Volume mount into a container podman run --volume http-data:/var/www/html ubi8/httpd-24:latest ## Because Podman manages the volume, you do not need to configure SELinux permissions. Alternatively, you can use the --mount parameter --mount type=TYPE,source=/path/on/host,destination=/path/in/container,options=OPTIONS # type= bind | volume | tmpfs # options=ro | rw | z | Z Some application cannot use the default COW file system in a specific directory for performance reasons, but do not need persistence or data sharing. In this case, you can use the tmpfs mount type, which means that the data in mount is ephemeral but does not use the COW file system. podman run -e POSTGRESQL_ADMIN_PASSWORD=redhat --network lab-net \\ --mount type=tmpfs,tmpfs-size=512M,destination=/var/lib/pgsql/data \\ registry.redhat.io/rhel9/postgresql-13:1","title":"Volumes and Bind mounts"},{"location":"container/Containers/#importexport-volumes","text":"podman volume export http_data --output web_data.tar.gz podman volume import web_data.tar.gz","title":"Import/export volumes"},{"location":"container/Containers/#links","text":"","title":"Links"},{"location":"container/Containers/#storage_1","text":"","title":"Storage"},{"location":"container/Containers/#overlaygo","text":"","title":"Overlay.go"},{"location":"container/Containers/#selinux-and-container-file-permissions","text":"","title":"SELinux and Container File Permissions"},{"location":"container/Containers/#networking","text":"","title":"Networking"},{"location":"container/Containers/#podman-4-network","text":"","title":"Podman 4 network"},{"location":"container/Containers/#nsenter","text":"","title":"nsenter"},{"location":"container/Readme/","text":"Container A container is a set of one or more processes that are isolated from the rest of the system. Container runtime A container runtime is a lower-level software component, is responsible for the actual running and lifecycle management of containers. It interacts with the OS to manage the lower-level aspects of container management, such as cgroups, namespaces, and SELinux. Examples: runc (lightweight, portable container runtime) - used by docker cri-o (container runtime interface) - used by kubernetes Container engine A container engine is a higher-level software that processes user requests, including command line options and image pulls. The container engine uses a container runtime , also called a lower-level container runtime, to run and manage the components required to deploy and operate containers. Examples: docker podman container management: kubernetes docker swarm mesos nomad podman Linux kernel container features Namespaces A namespace isolates specific system resources usually visible to all processes. Inside a namespace, only processes that are members of that namespace can see those resources. Namespaces can include resources like network interfaces , the process ID list , mount points , IPC resources , and the system's host name information. Control groups (cgroups) Control groups partition sets of processes and their children into groups to manage and limit the resources they consume. Control groups place restrictions on the amount of system resources processes might use. Those restrictions keep one process from using too many resources on the host. Seccomp Developed in 2005 and introduced to containers circa 2014, Seccomp limits how processes could use system call s. Seccomp defines a security profile for processes that lists the system calls, parameters and file descriptors they are allowed to use. SELinux Security-Enhanced Linux (SELinux) is a mandatory access control system for processes. Linux kernel uses SELinux to protect processes from each other and to protect the host system from its running processes. Processes run as a confined SELinux type that has limited access to host system resources.","title":"Container"},{"location":"container/Readme/#container","text":"A container is a set of one or more processes that are isolated from the rest of the system.","title":"Container"},{"location":"container/Readme/#container-runtime","text":"A container runtime is a lower-level software component, is responsible for the actual running and lifecycle management of containers. It interacts with the OS to manage the lower-level aspects of container management, such as cgroups, namespaces, and SELinux. Examples: runc (lightweight, portable container runtime) - used by docker cri-o (container runtime interface) - used by kubernetes","title":"Container runtime"},{"location":"container/Readme/#container-engine","text":"A container engine is a higher-level software that processes user requests, including command line options and image pulls. The container engine uses a container runtime , also called a lower-level container runtime, to run and manage the components required to deploy and operate containers. Examples: docker podman","title":"Container engine"},{"location":"container/Readme/#container-management","text":"kubernetes docker swarm mesos nomad podman","title":"container management:"},{"location":"container/Readme/#linux-kernel-container-features","text":"","title":"Linux kernel container features"},{"location":"container/Readme/#namespaces","text":"A namespace isolates specific system resources usually visible to all processes. Inside a namespace, only processes that are members of that namespace can see those resources. Namespaces can include resources like network interfaces , the process ID list , mount points , IPC resources , and the system's host name information.","title":"Namespaces"},{"location":"container/Readme/#control-groups-cgroups","text":"Control groups partition sets of processes and their children into groups to manage and limit the resources they consume. Control groups place restrictions on the amount of system resources processes might use. Those restrictions keep one process from using too many resources on the host.","title":"Control groups (cgroups)"},{"location":"container/Readme/#seccomp","text":"Developed in 2005 and introduced to containers circa 2014, Seccomp limits how processes could use system call s. Seccomp defines a security profile for processes that lists the system calls, parameters and file descriptors they are allowed to use.","title":"Seccomp"},{"location":"container/Readme/#selinux","text":"Security-Enhanced Linux (SELinux) is a mandatory access control system for processes. Linux kernel uses SELinux to protect processes from each other and to protect the host system from its running processes. Processes run as a confined SELinux type that has limited access to host system resources.","title":"SELinux"},{"location":"container/Rootless_podman/","text":"Rootless Podman User Mapping Podman maps users inside of the container to unprivileged users on the host system by using subordinate ID ranges /etc/subuid /etc/subgid To generate the subordinate ID ranges, use the usermod command: sudo usermod --add-subuids 100000-165535 \\ --add-subgids 100000-165535 student # The /etc/subuid and /etc/subgid files must exist before you define the subordinate ID ranges # for the new subordinate ID ranges to take effect: podman system migrate To verify the mapped user podman run -it registry.access.redhat.com/ubi9/ubi id podman top e6116477c5c9 huser user When you execute a container with elevated privileges on the host machine, the root mapping does not take place even when you define subordinate ID ranges Links Shortcomings of Rootless Podman Rootless podman Understanding root inside and outside a container Container Security Workshop Podman is gaining rootless overlay support","title":"Rootless Podman"},{"location":"container/Rootless_podman/#rootless-podman","text":"","title":"Rootless Podman"},{"location":"container/Rootless_podman/#user-mapping","text":"Podman maps users inside of the container to unprivileged users on the host system by using subordinate ID ranges /etc/subuid /etc/subgid To generate the subordinate ID ranges, use the usermod command: sudo usermod --add-subuids 100000-165535 \\ --add-subgids 100000-165535 student # The /etc/subuid and /etc/subgid files must exist before you define the subordinate ID ranges # for the new subordinate ID ranges to take effect: podman system migrate To verify the mapped user podman run -it registry.access.redhat.com/ubi9/ubi id podman top e6116477c5c9 huser user When you execute a container with elevated privileges on the host machine, the root mapping does not take place even when you define subordinate ID ranges","title":"User Mapping"},{"location":"container/Rootless_podman/#links","text":"","title":"Links"},{"location":"container/Rootless_podman/#shortcomings-of-rootless-podman","text":"","title":"Shortcomings of Rootless Podman"},{"location":"container/Rootless_podman/#rootless-podman_1","text":"","title":"Rootless podman"},{"location":"container/Rootless_podman/#understanding-root-inside-and-outside-a-container","text":"","title":"Understanding root inside and outside a container"},{"location":"container/Rootless_podman/#container-security-workshop","text":"","title":"Container Security Workshop"},{"location":"container/Rootless_podman/#podman-is-gaining-rootless-overlay-support","text":"","title":"Podman is gaining rootless overlay support"},{"location":"container/Troubleshooting/","text":"Troubleshooting Container Startup podman ps -a podman logs CONTAINER_ID # podman logs -f CONTAINER_ID Container Networking podman port CONTAINER_ID To verify the application ports in use in the container podman exec -it CONTAINER ss -pant Containers usually lack many commands. Run the host system commands within the container network namespace by using the nsenter command # get the PID of the container podman inspect CONTAINER_ID --format '{{.State.Pid}}' sudo nsenter -n -t CONTAINER_PID ss -pant To verify that every container is using a specific network podman inspect CONTAINER_ID --format '{{.NetworkSettings.Networks}}' ## When containers communicate by using Podman networks, there is no port mapping involved. Troubleshoot Bind mounts podman-unshare - Run a command inside of a modified user namespace podman unshare ls -l /www/ SELinux ls -Zd /www system_u:object_r:default_t:s0:c228,c359 /www The output shows the SELinux context label system_u:object_r:default_t:s0:c228,c359 , which has the default_t type. A container must have the container_file_t SELinux type to have access to the bind mount. To fix the SELinux configuration, add the :z or :Z option to the bind mount: z lets different containers share access to a bind mount. Z provides the container with exclusive access to the bind mount podman run -p 8080:8080 --volume /www:/var/www/html:Z ubi8/httpd-24:latest After adding the corresponding option, run the ls -Zd command and notice the right SELinux type. [user@host ~]$ ls -Zd /www system_u:object_r:container_file_t:s0:c240,c717 /www Utility getent - retrieves information from various databases, including the user database, group database, and network services database, based on the configured sources, such as /etc/passwd, /etc/group, and /etc/services. getent hosts <hostname> getent passwd nsenter is a command line utility that allows you to enter a namespace. nsenter -t <pid> -n Links Application Container Security Guide Podman Setup Podman Troubleshooting namespaces","title":"Troubleshooting"},{"location":"container/Troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"container/Troubleshooting/#container-startup","text":"podman ps -a podman logs CONTAINER_ID # podman logs -f CONTAINER_ID","title":"Container Startup"},{"location":"container/Troubleshooting/#container-networking","text":"podman port CONTAINER_ID To verify the application ports in use in the container podman exec -it CONTAINER ss -pant Containers usually lack many commands. Run the host system commands within the container network namespace by using the nsenter command # get the PID of the container podman inspect CONTAINER_ID --format '{{.State.Pid}}' sudo nsenter -n -t CONTAINER_PID ss -pant To verify that every container is using a specific network podman inspect CONTAINER_ID --format '{{.NetworkSettings.Networks}}' ## When containers communicate by using Podman networks, there is no port mapping involved.","title":"Container Networking"},{"location":"container/Troubleshooting/#troubleshoot-bind-mounts","text":"podman-unshare - Run a command inside of a modified user namespace podman unshare ls -l /www/ SELinux ls -Zd /www system_u:object_r:default_t:s0:c228,c359 /www The output shows the SELinux context label system_u:object_r:default_t:s0:c228,c359 , which has the default_t type. A container must have the container_file_t SELinux type to have access to the bind mount. To fix the SELinux configuration, add the :z or :Z option to the bind mount: z lets different containers share access to a bind mount. Z provides the container with exclusive access to the bind mount podman run -p 8080:8080 --volume /www:/var/www/html:Z ubi8/httpd-24:latest After adding the corresponding option, run the ls -Zd command and notice the right SELinux type. [user@host ~]$ ls -Zd /www system_u:object_r:container_file_t:s0:c240,c717 /www","title":"Troubleshoot Bind mounts"},{"location":"container/Troubleshooting/#utility","text":"getent - retrieves information from various databases, including the user database, group database, and network services database, based on the configured sources, such as /etc/passwd, /etc/group, and /etc/services. getent hosts <hostname> getent passwd nsenter is a command line utility that allows you to enter a namespace. nsenter -t <pid> -n","title":"Utility"},{"location":"container/Troubleshooting/#links","text":"Application Container Security Guide Podman Setup Podman Troubleshooting namespaces","title":"Links"},{"location":"container/monitoring/","text":"ctop","title":"Monitoring"},{"location":"container/registry/","text":"","title":"Registry"},{"location":"container/security/","text":"shared resources: * dentry - is a directory entry in the kernel's dentry cache * inodes - shared by all containers privileged Docker run --privileged vs (--cap-drop=ALL --cap-add=setuid) Distroless FROM scratch export DOCKER_CONTENT_TRUST=1 - verify publisher - tags Links https://sysdig.com/blog/container-isolation-gone-wrong/","title":"Security"},{"location":"container/security/#shared-resources","text":"* dentry - is a directory entry in the kernel's dentry cache * inodes - shared by all containers","title":"shared resources:"},{"location":"container/security/#privileged","text":"Docker run --privileged vs (--cap-drop=ALL --cap-add=setuid) Distroless FROM scratch export DOCKER_CONTENT_TRUST=1 - verify publisher - tags","title":"privileged"},{"location":"container/security/#links","text":"https://sysdig.com/blog/container-isolation-gone-wrong/","title":"Links"},{"location":"container/under_the_hood/","text":"CGroups (Control Groups) - Linux kernel feature to limit, prioritize, and isolate resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes. Memory is incompressible resource # start container with mem limit docker run -d --name lowmem100 -m=100m monitoringartist/docker-killer:latest membomb # check out of memory events docker inspect lowmem100 | grep OOM CPU is compressible resource # start container with cpu limit docker run -d --cpu=0.01 --name slow nginx # checking that container is running slowly docker exec slow sha1sum /dev/hosts Namespaces - Linux kernel feature to isolate resources of a collection of processes. (abstraction of system resources) Cgroup IPC (Inter-Process Communication) Network Mount PID (Process ID) User UTS (Unix Time-Sharing) # list all namespaces lsns # enter into network namespace nsenter -t 91 -n ip a Capabilities - Linux kernel feature to grant a process a subset of the full set of root privileges. CAP_CHOWN - Make arbitrary changes to file UIDs and GIDs CAP_KILL - Bypass permission checks for sending signals CAP_NET_BIND_SERVICE - Bind a socket to internet domain privileged ports cat /proc/<PID>/status | grep Cap Dockerd Dockerd - Docker daemon, the persistent process that manages containers. (Build images, network, storage, logs, etc.) Containerd - High-level container runtime that manages the complete container lifecycle. (Start/Stop, network on driver level, etc.) Runc - Low-level container runtime that runs containers according to the OCI specification. (Build container, start/stop, etc.) Docker-containerd-shim - A shim process that is responsible for forwarding signals and reaping processes. Docker-proxy - A process that forwards traffic to and from the container. OCI Runtime-spec - Open Container Initiative (OCI) runtime specification. Image-spec - Open Container Initiative (OCI) image specification. Containerd ctr - Command-line client for containerd. ctr image pull docker.io/library/alpine:latest ctr run -d docker.io/library/alpine:latest alpine ctr tasks exec --exec-id 1006 alpine echo \"Hello, World!\" # stop container kill -9 1006 RunC oci bundle - A directory that contains the configuration and root filesystem for a container. oci image - A directory that contains the configuration and root filesystem for a container. runc spec runc run test mkdir rootfs skopeo copy docker://busybox:latest oci:rootfs:latest umoci unpack --image busybox:latest bundle cp -r ./bundle/rootfs/* rootfs runc run test runc list runc --root /run/docker/runtime-runc/moby list","title":"Under the hood"},{"location":"container/under_the_hood/#cgroups-control-groups-linux-kernel-feature-to-limit-prioritize-and-isolate-resource-usage-cpu-memory-disk-io-network-etc-of-a-collection-of-processes","text":"Memory is incompressible resource # start container with mem limit docker run -d --name lowmem100 -m=100m monitoringartist/docker-killer:latest membomb # check out of memory events docker inspect lowmem100 | grep OOM CPU is compressible resource # start container with cpu limit docker run -d --cpu=0.01 --name slow nginx # checking that container is running slowly docker exec slow sha1sum /dev/hosts","title":"CGroups (Control Groups) - Linux kernel feature to limit, prioritize, and isolate resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes."},{"location":"container/under_the_hood/#namespaces-linux-kernel-feature-to-isolate-resources-of-a-collection-of-processes-abstraction-of-system-resources","text":"Cgroup IPC (Inter-Process Communication) Network Mount PID (Process ID) User UTS (Unix Time-Sharing) # list all namespaces lsns # enter into network namespace nsenter -t 91 -n ip a","title":"Namespaces - Linux kernel feature to isolate resources of a collection of processes. (abstraction of system resources)"},{"location":"container/under_the_hood/#capabilities-linux-kernel-feature-to-grant-a-process-a-subset-of-the-full-set-of-root-privileges","text":"CAP_CHOWN - Make arbitrary changes to file UIDs and GIDs CAP_KILL - Bypass permission checks for sending signals CAP_NET_BIND_SERVICE - Bind a socket to internet domain privileged ports cat /proc/<PID>/status | grep Cap","title":"Capabilities - Linux kernel feature to grant a process a subset of the full set of root privileges."},{"location":"container/under_the_hood/#dockerd","text":"Dockerd - Docker daemon, the persistent process that manages containers. (Build images, network, storage, logs, etc.) Containerd - High-level container runtime that manages the complete container lifecycle. (Start/Stop, network on driver level, etc.) Runc - Low-level container runtime that runs containers according to the OCI specification. (Build container, start/stop, etc.) Docker-containerd-shim - A shim process that is responsible for forwarding signals and reaping processes. Docker-proxy - A process that forwards traffic to and from the container.","title":"Dockerd"},{"location":"container/under_the_hood/#oci","text":"Runtime-spec - Open Container Initiative (OCI) runtime specification. Image-spec - Open Container Initiative (OCI) image specification.","title":"OCI"},{"location":"container/under_the_hood/#containerd","text":"ctr - Command-line client for containerd. ctr image pull docker.io/library/alpine:latest ctr run -d docker.io/library/alpine:latest alpine ctr tasks exec --exec-id 1006 alpine echo \"Hello, World!\" # stop container kill -9 1006","title":"Containerd"},{"location":"container/under_the_hood/#runc","text":"oci bundle - A directory that contains the configuration and root filesystem for a container. oci image - A directory that contains the configuration and root filesystem for a container. runc spec runc run test mkdir rootfs skopeo copy docker://busybox:latest oci:rootfs:latest umoci unpack --image busybox:latest bundle cp -r ./bundle/rootfs/* rootfs runc run test runc list runc --root /run/docker/runtime-runc/moby list","title":"RunC"},{"location":"git/","text":"use cases git git log diff -u patch git status git config -l git log git log -p [-2] #(p for patche) - shows changes git log --stat - shows chages stats git log --graph --oneline [--all] git log -p origin/master git show fc1009896cd27f3c223af528e79b8521f1c84a21 - shows commint info git diff (= diff -u) git diff --staged git add git add -p # review the changes befor stagign them git commit -a # to stage any changes to tracked files git rm (Removing) git rm filename + git commit git mv filename Undoing git checkout \"filename\" #it reverts changes to modified files before they are staged git reset HEAD output.txt #remove changes from the staging area, opposit to git add Ammending Commits git commit --amend # overwrite the previous commit, avoid amending public commits Rollbacks git revert HEAD # revert most resent commit git revert \"hash .. \" # rever any commit Branches git branch - list all branches in the repo git branch somename - create a branch, but not swithcing to the branch git checkout branchname - change a branch git branch -d branchname - delete a branch git merge even-better-feature - merge the branch into the master brach if there is a conflict: git status - check the merge status open a conflicting file and fix it git add thefile lastly git commit one more time git merge --abort #will cancell a merge git fetch git pull = git fetch + git merge git remote show origin git remote Lists remote repos git remote -v List remote repos verbosely git remote show Describes a single remote repo git remote update Fetches the most up-to-date objects git fetch Downloads specific objects git branch -r Lists remote branches ; can be combined with other branch arguments to manage remote branches git push -u origin branchname #create and push a new branch to upstream about merge and pull requests https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/incorporating-changes-from-a-pull-request/about-pull-request-merges More Information on Code Reviews Check out the following links for more information: http://google.github.io/styleguide/ https://help.github.com/en/articles/about-pull-request-reviews https://medium.com/osedea/the-perfect-code-review-process-845e6ba5c31 https://smartbear.com/learn/code-review/what-is-code-review/ git rebase -i HEAD~3 #rebase 3 commits back Fast-forward merge Implicit merge Explicit merge Squash on merge HEAD is an alias to represetn the currently checked-out snapshot of your project why git commit id is a hash - \"You can verify the data you get back out is the exact same data you put in\" https://git-scm.com/docs links to check https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/process/submitting-patches.rst?id=HEAD http://stopwritingramblingcommitmessages.com/ https://thoughtbot.com/blog/5-useful-tips-for-a-better-commit-message git rebase -i HEAD~3 # squash last 3 commits Ssh-agent and Keychain generate a new ssh key pair ssh-keygen -t rsa -b 4096 -C \"comment\" -f ~/.ssh/id_rsa # assiminng you will specify a passphrase add the passphrase to the keychain ssh-add --apple-use-keychain ~/.ssh/id_rsa Configure SSH-agent to always use the keychain in the .ssh/config file add the following Host * # ssh client will use the key for all hosts UseKeychain yes # this is the important line IdentityFile ~/.ssh/id_rsa","title":"Git"},{"location":"git/#git","text":"","title":"git"},{"location":"git/#git-log","text":"diff -u patch git status git config -l git log git log -p [-2] #(p for patche) - shows changes git log --stat - shows chages stats git log --graph --oneline [--all] git log -p origin/master git show fc1009896cd27f3c223af528e79b8521f1c84a21 - shows commint info git diff (= diff -u) git diff --staged","title":"git log"},{"location":"git/#git-add","text":"git add -p # review the changes befor stagign them git commit -a # to stage any changes to tracked files","title":"git add"},{"location":"git/#git-rm-removing","text":"git rm filename + git commit git mv filename","title":"git rm (Removing)"},{"location":"git/#undoing","text":"git checkout \"filename\" #it reverts changes to modified files before they are staged git reset HEAD output.txt #remove changes from the staging area, opposit to git add","title":"Undoing"},{"location":"git/#ammending-commits","text":"git commit --amend # overwrite the previous commit, avoid amending public commits","title":"Ammending Commits"},{"location":"git/#rollbacks","text":"git revert HEAD # revert most resent commit git revert \"hash .. \" # rever any commit","title":"Rollbacks"},{"location":"git/#branches","text":"git branch - list all branches in the repo git branch somename - create a branch, but not swithcing to the branch git checkout branchname - change a branch git branch -d branchname - delete a branch git merge even-better-feature - merge the branch into the master brach if there is a conflict: git status - check the merge status open a conflicting file and fix it git add thefile lastly git commit one more time git merge --abort #will cancell a merge git fetch git pull = git fetch + git merge git remote show origin git remote Lists remote repos git remote -v List remote repos verbosely git remote show Describes a single remote repo git remote update Fetches the most up-to-date objects git fetch Downloads specific objects git branch -r Lists remote branches ; can be combined with other branch arguments to manage remote branches git push -u origin branchname #create and push a new branch to upstream about merge and pull requests https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/incorporating-changes-from-a-pull-request/about-pull-request-merges More Information on Code Reviews Check out the following links for more information: http://google.github.io/styleguide/ https://help.github.com/en/articles/about-pull-request-reviews https://medium.com/osedea/the-perfect-code-review-process-845e6ba5c31 https://smartbear.com/learn/code-review/what-is-code-review/ git rebase -i HEAD~3 #rebase 3 commits back Fast-forward merge Implicit merge Explicit merge Squash on merge HEAD is an alias to represetn the currently checked-out snapshot of your project why git commit id is a hash - \"You can verify the data you get back out is the exact same data you put in\" https://git-scm.com/docs links to check https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/process/submitting-patches.rst?id=HEAD http://stopwritingramblingcommitmessages.com/ https://thoughtbot.com/blog/5-useful-tips-for-a-better-commit-message git rebase -i HEAD~3 # squash last 3 commits","title":"Branches"},{"location":"git/#ssh-agent-and-keychain","text":"","title":"Ssh-agent and Keychain"},{"location":"git/#generate-a-new-ssh-key-pair","text":"ssh-keygen -t rsa -b 4096 -C \"comment\" -f ~/.ssh/id_rsa # assiminng you will specify a passphrase","title":"generate a new ssh key pair"},{"location":"git/#add-the-passphrase-to-the-keychain","text":"ssh-add --apple-use-keychain ~/.ssh/id_rsa","title":"add the passphrase to the keychain"},{"location":"git/#configure-ssh-agent-to-always-use-the-keychain","text":"in the .ssh/config file add the following Host * # ssh client will use the key for all hosts UseKeychain yes # this is the important line IdentityFile ~/.ssh/id_rsa","title":"Configure SSH-agent to always use the keychain"},{"location":"git/usecases/","text":"remove branch locally and remotely #remove local branch git branch -d feature/revertTempChanges #remove remote branch git push origin :feature/revertTempChange #or git push origin --delete feature/revertTempChanges #prune origin git fetch --prune origin restore a file from previous commits","title":"remove branch locally and remotely"},{"location":"git/usecases/#remove-branch-locally-and-remotely","text":"#remove local branch git branch -d feature/revertTempChanges #remove remote branch git push origin :feature/revertTempChange #or git push origin --delete feature/revertTempChanges #prune origin git fetch --prune origin","title":"remove branch locally and remotely"},{"location":"git/usecases/#restore-a-file-from-previous-commits","text":"","title":"restore a file from previous commits"},{"location":"k8s/","text":"Base objects Basic_commands EX180_notes EX280_notes OpenShift","title":"K8S"},{"location":"k8s/Base_objects/","text":"Intro Kubernetes uses API resource objects to represent the intended state of everything in the cluster. All administrative tasks require creating, viewing, and changing the API resources . Use the oc api-resources command to view the Kubernetes resources. every k8s resource has kind , apiVersion , spec and status fields. The status field is generated by the k8s almost every k8s object includes two nested object fields: spec and status . The spec field contains the desired state, and the status field contains the current state. Resource types RHOCP specific resource types BuildConfig DeploymentConfig - deprecated Route ImageStream Operators Operators automate the required tasks to maintain a healthy RHOCP cluster that would otherwise require human intervention. Operators are the preferred method of packaging, deploying, and managing services on the control plane. Cluster Version Operator (CVO) - kind value of clusteroperators oc get clusteroperators describe clusteroperators operator-name oc get clusteroperators dns -o yaml Operator Lifecycle Manager (OLM) Operators oc get operators Operators use one or more pods to provide cluster services oc get pods -n openshift-dns-operator","title":"Intro"},{"location":"k8s/Base_objects/#intro","text":"Kubernetes uses API resource objects to represent the intended state of everything in the cluster. All administrative tasks require creating, viewing, and changing the API resources . Use the oc api-resources command to view the Kubernetes resources. every k8s resource has kind , apiVersion , spec and status fields. The status field is generated by the k8s almost every k8s object includes two nested object fields: spec and status . The spec field contains the desired state, and the status field contains the current state.","title":"Intro"},{"location":"k8s/Base_objects/#resource-types","text":"","title":"Resource types"},{"location":"k8s/Base_objects/#rhocp-specific-resource-types","text":"BuildConfig DeploymentConfig - deprecated Route ImageStream","title":"RHOCP specific resource types"},{"location":"k8s/Base_objects/#operators","text":"Operators automate the required tasks to maintain a healthy RHOCP cluster that would otherwise require human intervention. Operators are the preferred method of packaging, deploying, and managing services on the control plane. Cluster Version Operator (CVO) - kind value of clusteroperators oc get clusteroperators describe clusteroperators operator-name oc get clusteroperators dns -o yaml Operator Lifecycle Manager (OLM) Operators oc get operators Operators use one or more pods to provide cluster services oc get pods -n openshift-dns-operator","title":"Operators"},{"location":"k8s/Basic_commands/","text":"help kubectl --help kubectl create --help Cluster info # Print the supported API versions on the server kubectl api-versions # Print the supported API resources on the server kubectl api-resources # API-Group/API-Version - API-Group is blank for Kubernetes core resources # KIND - the formal Kubernetes resource schema type # Display addresses of the control plane and services kubectl cluster-info get retrieves information about resources in the selected project oc get clusteroperator # [-o yaml | json] kubectl get all --show-kind kubectl get pods -o wide kubectl get pods \\ -o custom-columns=PodName:\".metadata.name\",\\ ContainerName:\"spec.containers[].name\",\\ explain provides detailed information about the attributes of a given resource type kubectl explain pods kubectl explain pods.spec describe provides detailed information about a given resource kubectl describe mysql-openshift-1-glgrp logs Container logs are the standard output (stdout) and standard error (stderr) output of a container kubectl logs mysql-kd13 -c container-name Other commands oc create -f pod.yaml oc delete OC specific commands oc login cluster-url oc new-project myapp oc status oc adm ```sh oc adm top pods -A --sum","title":"Basic commands"},{"location":"k8s/Basic_commands/#help","text":"kubectl --help kubectl create --help","title":"help"},{"location":"k8s/Basic_commands/#cluster-info","text":"# Print the supported API versions on the server kubectl api-versions # Print the supported API resources on the server kubectl api-resources # API-Group/API-Version - API-Group is blank for Kubernetes core resources # KIND - the formal Kubernetes resource schema type # Display addresses of the control plane and services kubectl cluster-info","title":"Cluster info"},{"location":"k8s/Basic_commands/#get","text":"retrieves information about resources in the selected project oc get clusteroperator # [-o yaml | json] kubectl get all --show-kind kubectl get pods -o wide kubectl get pods \\ -o custom-columns=PodName:\".metadata.name\",\\ ContainerName:\"spec.containers[].name\",\\","title":"get"},{"location":"k8s/Basic_commands/#explain","text":"provides detailed information about the attributes of a given resource type kubectl explain pods kubectl explain pods.spec","title":"explain"},{"location":"k8s/Basic_commands/#describe","text":"provides detailed information about a given resource kubectl describe mysql-openshift-1-glgrp","title":"describe"},{"location":"k8s/Basic_commands/#logs","text":"Container logs are the standard output (stdout) and standard error (stderr) output of a container kubectl logs mysql-kd13 -c container-name","title":"logs"},{"location":"k8s/Basic_commands/#other-commands","text":"oc create -f pod.yaml oc delete","title":"Other commands"},{"location":"k8s/Basic_commands/#oc-specific-commands","text":"oc login cluster-url oc new-project myapp oc status","title":"OC specific commands"},{"location":"k8s/Basic_commands/#oc-adm","text":"```sh","title":"oc adm"},{"location":"k8s/Basic_commands/#_1","text":"oc adm top pods -A --sum","title":""},{"location":"k8s/EX180_notes/","text":"Examining Cluster Metrics oc adm oc adm top pods -A --sum oc adm must-gather --dest-dir /home/student/must-gather oc adm inspect clusteroperator/openshift-apiserver \\ clusteroperator/kube-apiserver oc adm inspect clusteroperator/openshift-apiserver --since 10m oc adm top pods -A --sum oc adm top pods etcd-master01 -n openshift-etcd --containers Query Cluster Events and Alerts oc debug oc debug job/test --as-user=10000 oc debug node/node-name Run Applications as Containers and Pods oc run RESOURCE/NAME --image IMAGE [options] kubectl run web-server --image registry.access.redhat.com/ubi8/httpd-24 # -- arguments kubectl run RESOURCE/NAME --image IMAGE -- arg1 arg2 ... argN oc exec RESOURCE/NAME -- COMMAND [args...] [options] oc exec my-app -- date # exec in the first container of the pod kubectl exec my-app -c ruby-container -- date oc attach my-app -it Node debug ssh to a node or create a debug pod oc debug node/node-name chroot /host cri debug # get pod id crictl pods --name master01-debug # get container name from pod id crictl ps -p cb066ee76b598 -o json | jq -r .containers[0].metadata.name # get container id CID=$(crictl ps --name container-00 -o json | jq -r .containers[0].id) # get PID of the container crictl inspect $CID | grep pid # list namespaces lsns -p pid # enter the container namespace nsenter -t PID -p -r ps -ef # -t PID - target PID # -p - enter PID namespace # -r - set the top-level directory Common Resource Types Templates Templates (RHOCP feature) - a yaml manifest that contains parameterized definitions of one or more resources. Processed by the oc process command, which replaces value and generates resource definitions. The resulting resource definitions can be applied to a cluster using the oc apply command. # process and generate resource definitions from a file. oc process -f mysql-template.yaml -o yaml # display the parameters defined in a template oc process -f mysql-template.yaml --parameters You also use templates with the oc new-app command oc new-app --template=mysql-persistent Pod the smallest compute unit that can be defined. DeploymentConfig Deployment configurations define the specification of a pod. They manage pods by creating replication controllers . Deployment Similar to deployment configurations, deployments define the intended state of a replica set. ReplicaSet Replica sets define a configurable number of pods that match a specification. Project A project is a Kubernetes namespace with additional annotations, and is the primary method for managing access to resources in a cluster. Service an object for internal pod-to-pod communication. Application send requests to a service and and port. PersistentVolume Claim(PVC) Secrets Resource Management Imperative - instructs what the system does Declarative - defines the state that the cluster attempts to match.","title":"EX180 notes"},{"location":"k8s/EX180_notes/#examining-cluster-metrics","text":"","title":"Examining Cluster Metrics"},{"location":"k8s/EX180_notes/#oc-adm","text":"oc adm top pods -A --sum oc adm must-gather --dest-dir /home/student/must-gather oc adm inspect clusteroperator/openshift-apiserver \\ clusteroperator/kube-apiserver oc adm inspect clusteroperator/openshift-apiserver --since 10m oc adm top pods -A --sum oc adm top pods etcd-master01 -n openshift-etcd --containers","title":"oc adm"},{"location":"k8s/EX180_notes/#query-cluster-events-and-alerts","text":"oc debug oc debug job/test --as-user=10000 oc debug node/node-name","title":"Query Cluster Events and Alerts"},{"location":"k8s/EX180_notes/#run-applications-as-containers-and-pods","text":"oc run RESOURCE/NAME --image IMAGE [options] kubectl run web-server --image registry.access.redhat.com/ubi8/httpd-24 # -- arguments kubectl run RESOURCE/NAME --image IMAGE -- arg1 arg2 ... argN oc exec RESOURCE/NAME -- COMMAND [args...] [options] oc exec my-app -- date # exec in the first container of the pod kubectl exec my-app -c ruby-container -- date oc attach my-app -it","title":"Run Applications as Containers and Pods"},{"location":"k8s/EX180_notes/#node-debug","text":"ssh to a node or create a debug pod oc debug node/node-name chroot /host","title":"Node debug"},{"location":"k8s/EX180_notes/#cri-debug","text":"# get pod id crictl pods --name master01-debug # get container name from pod id crictl ps -p cb066ee76b598 -o json | jq -r .containers[0].metadata.name # get container id CID=$(crictl ps --name container-00 -o json | jq -r .containers[0].id) # get PID of the container crictl inspect $CID | grep pid # list namespaces lsns -p pid # enter the container namespace nsenter -t PID -p -r ps -ef # -t PID - target PID # -p - enter PID namespace # -r - set the top-level directory","title":"cri debug"},{"location":"k8s/EX180_notes/#common-resource-types","text":"","title":"Common Resource Types"},{"location":"k8s/EX180_notes/#templates","text":"Templates (RHOCP feature) - a yaml manifest that contains parameterized definitions of one or more resources. Processed by the oc process command, which replaces value and generates resource definitions. The resulting resource definitions can be applied to a cluster using the oc apply command. # process and generate resource definitions from a file. oc process -f mysql-template.yaml -o yaml # display the parameters defined in a template oc process -f mysql-template.yaml --parameters You also use templates with the oc new-app command oc new-app --template=mysql-persistent","title":"Templates"},{"location":"k8s/EX180_notes/#pod","text":"the smallest compute unit that can be defined.","title":"Pod"},{"location":"k8s/EX180_notes/#deploymentconfig","text":"Deployment configurations define the specification of a pod. They manage pods by creating replication controllers .","title":"DeploymentConfig"},{"location":"k8s/EX180_notes/#deployment","text":"Similar to deployment configurations, deployments define the intended state of a replica set.","title":"Deployment"},{"location":"k8s/EX180_notes/#replicaset","text":"Replica sets define a configurable number of pods that match a specification.","title":"ReplicaSet"},{"location":"k8s/EX180_notes/#project","text":"A project is a Kubernetes namespace with additional annotations, and is the primary method for managing access to resources in a cluster.","title":"Project"},{"location":"k8s/EX180_notes/#service","text":"an object for internal pod-to-pod communication. Application send requests to a service and and port.","title":"Service"},{"location":"k8s/EX180_notes/#persistentvolume-claimpvc","text":"","title":"PersistentVolume Claim(PVC)"},{"location":"k8s/EX180_notes/#secrets","text":"","title":"Secrets"},{"location":"k8s/EX180_notes/#resource-management","text":"Imperative - instructs what the system does Declarative - defines the state that the cluster attempts to match.","title":"Resource Management"},{"location":"k8s/OpenShift/","text":"Distributions Red Hat OpenShift Kubernetes Engine (RHOKE) - includes the latest version of the Kubernetes platform with RH security hardening. It runs on RHEL Linux CoreOS. Red Hat OpenShift Container Platform (RHOCP) - include developer console, as well as log management, cost management, and metering information. Additionally includes the following features: Red Hat OpenShift Severless (Knative), Red Hat OpenShift Mesh (istio), Red Hat OpenShift Pipelines (Tekton), and Red Hat OpenShift GitOps (ArgoCD). Red Hat OpenShift Platform Plus () - Red Hat Advanced Cluster Management for Kubernetes, Red Hat Advanced Cluster Security for Kubernetes, and the Red Hat Quay private registry platform. Link to installers Documentation OpenShift origin (OKD) - https://www.okd.io/ User types Regular users Most interactive RHOCP users are represented by this user type. An RHOCP User object represents a regular user. System users Infrastructure uses system users to interact with the API securely. Some system users are automatically created, including the cluster administrator, with access to everything. By default, unauthenticated requests use an anonymous system user. Service accounts ServiceAccount objects represent service accounts. OCP creates service accounts automatically when a project is created. Project administrators can create additional service accounts to define access to the contents of each project. OpenShift key concepts Pods : The smallest unit of a Kubernetes-managed containerized application. A pod consists of one or more containers. Deployments : The operational unit that provides granular management of a running application. Projects : A Kubernetes namespace with additional annotations that provide multitenancy scoping for applications. Routes : Networking configuration to expose your applications and services to resources outside the cluster. Operators - a method of packaging, deploying, and managing a Kubernetes application. OpenShift uses this method to add capabilities to a Kubernetes cluster. CVO (Cluster Version Operator) OLM (Operator Lifecycle Manager) RBAC (Role Based Access Control) CSV (Cluster Service Version) CRD (Custom Resource Definition)","title":"Distributions"},{"location":"k8s/OpenShift/#distributions","text":"Red Hat OpenShift Kubernetes Engine (RHOKE) - includes the latest version of the Kubernetes platform with RH security hardening. It runs on RHEL Linux CoreOS. Red Hat OpenShift Container Platform (RHOCP) - include developer console, as well as log management, cost management, and metering information. Additionally includes the following features: Red Hat OpenShift Severless (Knative), Red Hat OpenShift Mesh (istio), Red Hat OpenShift Pipelines (Tekton), and Red Hat OpenShift GitOps (ArgoCD). Red Hat OpenShift Platform Plus () - Red Hat Advanced Cluster Management for Kubernetes, Red Hat Advanced Cluster Security for Kubernetes, and the Red Hat Quay private registry platform. Link to installers Documentation OpenShift origin (OKD) - https://www.okd.io/","title":"Distributions"},{"location":"k8s/OpenShift/#user-types","text":"Regular users Most interactive RHOCP users are represented by this user type. An RHOCP User object represents a regular user. System users Infrastructure uses system users to interact with the API securely. Some system users are automatically created, including the cluster administrator, with access to everything. By default, unauthenticated requests use an anonymous system user. Service accounts ServiceAccount objects represent service accounts. OCP creates service accounts automatically when a project is created. Project administrators can create additional service accounts to define access to the contents of each project.","title":"User types"},{"location":"k8s/OpenShift/#openshift-key-concepts","text":"Pods : The smallest unit of a Kubernetes-managed containerized application. A pod consists of one or more containers. Deployments : The operational unit that provides granular management of a running application. Projects : A Kubernetes namespace with additional annotations that provide multitenancy scoping for applications. Routes : Networking configuration to expose your applications and services to resources outside the cluster. Operators - a method of packaging, deploying, and managing a Kubernetes application. OpenShift uses this method to add capabilities to a Kubernetes cluster. CVO (Cluster Version Operator) OLM (Operator Lifecycle Manager) RBAC (Role Based Access Control) CSV (Cluster Service Version) CRD (Custom Resource Definition)","title":"OpenShift key concepts"},{"location":"k8s/Utilities/","text":"kubctl Install kubectl on linux curl -LO \"https://dl.k8s.io/release/$(curl -L \\ -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L \\ -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" sudo install -o root -g root -m 0755 kubectl \\ /usr/local/bin/kubectl kubectl version --client yq oc get pods -o yaml | yq r - 'items[0].status.podIP' r - read specified path \"-\" - stdin lsns list namespaces lsns -p pid ldd print shared object dependencies ldd /bin/kubectl","title":"kubctl"},{"location":"k8s/Utilities/#kubctl","text":"","title":"kubctl"},{"location":"k8s/Utilities/#install-kubectl-on-linux","text":"curl -LO \"https://dl.k8s.io/release/$(curl -L \\ -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L \\ -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" sudo install -o root -g root -m 0755 kubectl \\ /usr/local/bin/kubectl kubectl version --client","title":"Install kubectl on linux"},{"location":"k8s/Utilities/#yq","text":"oc get pods -o yaml | yq r - 'items[0].status.podIP' r - read specified path \"-\" - stdin","title":"yq"},{"location":"k8s/Utilities/#lsns","text":"list namespaces lsns -p pid","title":"lsns"},{"location":"k8s/Utilities/#ldd","text":"print shared object dependencies ldd /bin/kubectl","title":"ldd"},{"location":"k8s/_overview/api-groups/","text":"API Groups Kubernetes API is divided into multiple API groups. Each group has its own set of resources and endpoints. Official docs /version kubectl proxy curl -k http://127.0.0.1:8001 curl -k http://127.0.0.1:8001/version /metrics /healthz /logs /api (core group) Old api group /v1 namespaces pods services replicationcontrollers nodes .... /apis (named groups) New, more organized /apis /apps (a group) /v1 /deployments (a resource) list (verb) get (verb) create (verb) update (verb) delete (verb) /daemonsets /statefulsets /replicasets /networking.k8s.io /v1 /networkpolicies /ingresses /rbac.authorization.k8s.io /v1 /roles /rolebindings /clusterroles /clusterrolebindings /apiextensions.k8s.io /v1 /customresourcedefinitions (CRD) ...","title":"API Groups"},{"location":"k8s/_overview/api-groups/#api-groups","text":"Kubernetes API is divided into multiple API groups. Each group has its own set of resources and endpoints. Official docs","title":"API Groups"},{"location":"k8s/_overview/api-groups/#version","text":"kubectl proxy curl -k http://127.0.0.1:8001 curl -k http://127.0.0.1:8001/version","title":"/version"},{"location":"k8s/_overview/api-groups/#metrics","text":"","title":"/metrics"},{"location":"k8s/_overview/api-groups/#healthz","text":"","title":"/healthz"},{"location":"k8s/_overview/api-groups/#logs","text":"","title":"/logs"},{"location":"k8s/_overview/api-groups/#api-core-group","text":"Old api group /v1 namespaces pods services replicationcontrollers nodes ....","title":"/api (core group)"},{"location":"k8s/_overview/api-groups/#apis-named-groups","text":"New, more organized /apis /apps (a group) /v1 /deployments (a resource) list (verb) get (verb) create (verb) update (verb) delete (verb) /daemonsets /statefulsets /replicasets /networking.k8s.io /v1 /networkpolicies /ingresses /rbac.authorization.k8s.io /v1 /roles /rolebindings /clusterroles /clusterrolebindings /apiextensions.k8s.io /v1 /customresourcedefinitions (CRD) ...","title":"/apis (named groups)"},{"location":"k8s/_overview/notes/","text":"SKIPPED - 0.6 cluster maintenance - REVIEW SKIPPED - 0.7 security - REVIEW Key Concepts Master Node : Controls the Kubernetes cluster, managing the API server , scheduler , and controller manager . Worker Node : Runs the applications and workloads, containing the kubelet , kube-proxy and container runtime . Etcd : Key-value store for all cluster data, used by the API server . Kube-scheduler : Assigns pods to nodes based on resource availability and constraints. Controllers : Node Controller : Monitors the status of nodes. Replication Controller : Ensures the desired number of pod replicas are running. Endpoints Controller : Manages the endpoints for services. Service Account & Token Controllers : Create default service accounts and tokens for new namespaces. Api-server : The front-end for the Kubernetes control plane, handling REST requests and updates to etcd. Container Runtime : Software responsible for running containers, such as Docker or containerd. Kubelet : An agent that runs on each node, ensuring containers are running in pods. Kube-proxy : Manages network rules for pod communication, enabling service discovery and load balancing. Pods Pod is the smallest deployable unit in Kubernetes. kubectl run nginx --image=nginx --restart=Never # pod-definition.yaml apiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx-container image: nginx ports: - containerPort: 80 kubectl apply -f pod-definition.yaml kubectl get pods Deployments kubectl create deployment --image=nginx nginx --dry-run=client -o yaml Services Service is an abstraction that defines a logical set of pods and a policy by which to access them. Service types: - ClusterIP : Exposes the service on a cluster-internal IP. - NodePort : Exposes the service on each node's IP at a static port. - LoadBalancer : Exposes the service externally using a cloud provider's load balancer # node-port-definition.yaml apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: NodePort ports: - port: 80 # Service port (within the cluster) targetPort: 80 # Port on the pod nodePort: 30080 # Node port (external access) selector: app: nginx # Select pods with this label type: frontend # Additional label for selection # cluster-ip-definition.yaml apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: ClusterIP # if not specified, defaults to ClusterIP ports: - port: 80 # Service port (within the cluster) targetPort: 80 # Port on the pod selector: app: nginx # Select pods with this label type: frontend # Additional label for selection # load-balancer-definition.yaml apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: LoadBalancer # Exposes the service externally using a cloud provider's load balancer ports: - port: 80 # Service port (within the cluster) targetPort: 80 # Port on the pod nodePort: 30080 # Node port (external access) Labels and Selectors and Annotations Labels - used to organize and select resources in Kubernetes. Annotations - used to store additional metadata about resources. kubectl get pods --selector app=nginx kubectl get all --selector app=nginx,type=frontend Taints and Tolerations Taint is a node property that prevents pods from being scheduled. Toleration is a pod property that allows it to be scheduled on nodes with specific taints. kubectl taint node node1 key=value:NoSchedule{PreferNoSchedule,NoExecute} #untaint kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule- # pod-with-toleration.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx-container image: nginx tolerations: - key: \"key\" operator: \"Equal\" value: \"value\" effect: \"NoSchedule\" Node Selectors and Affinity # pod-with-node-selector.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx-container image: nginx nodeSelector: disktype: ssd # Node label to select # pod-with-affinity.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx-container image: nginx affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: disktype operator: In values: - ssd # Node label to select Resource Requests and Limits 0.1 CPU = 100m CPU # means 100 milli CPU 1 CPU = 1000m CPU # pod-with-resources.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx-container image: nginx resources: requests: memory: \"64Mi\" # Minimum memory required cpu: \"250m\" # Minimum CPU required limits: memory: \"128Mi\" # Maximum memory allowed cpu: \"500m\" # Maximum CPU allowed Limit Ranges sets on a namespace level # limit-range.yaml apiVersion: v1 kind: LimitRange metadata: name: my-limit-range spec: limits: - default: memory: \"256Mi\" cpu: \"500m\" defaultRequest: memory: \"128Mi\" cpu: \"250m\" type: Container Resource Quotas Resource quotas limit the total resources that can be consumed in a namespace. # resource-quota.yaml apiVersion: v1 kind: ResourceQuota metadata: name: my-resource-quota spec: hard: requests.cpu: \"10\" # Total CPU requests allowed requests.memory: \"20Gi\" # Total memory requests allowed limits.cpu: \"20\" # Total CPU limits allowed limits.memory: \"40Gi\" # Total memory limits allowed pods: \"10\" # Total number of pods allowed services: \"5\" # Total number of services allowed Static Pods Static pods are managed directly by the kubelet on a node, not by the Kubernetes API server. They can be used for deploying kubernetes system components or other critical applications that need to run on specific nodes. kubelet watches this directory /etc/kubernetes/manifests for pod definitions. (directory can be configured) --pod-manifest-path option can be used to specify a different directory. staticPodPath option in kubelet configuration file can also be used to specify the directory. Cli Commands ```sh Create an NGINX Pod kubectl run nginx --image=nginx Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run) kubectl run nginx --image=nginx --dry-run=client -o yaml Create a deployment kubectl create deployment --image=nginx nginx Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) kubectl create deployment --image=nginx nginx --dry-run=client -o yaml Generate Deployment YAML file (-o yaml). Don\u2019t create it(\u2013dry-run) and save it to a file. kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml Make necessary changes to the file (for example, adding more replicas) and then create the deployment. kubectl create -f nginx-deployment.yaml specify the --replicas option to create a deployment with 4 replicas. kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml Set a label on a node kubectl label node node01 color=blue Create DaemonSet kubectl create create daemonset my-daemonset --image=nginx Check the differences between live objects and manifests kubectl diff -f my-app/example-deployment.yaml","title":"Notes"},{"location":"k8s/_overview/notes/#key-concepts","text":"Master Node : Controls the Kubernetes cluster, managing the API server , scheduler , and controller manager . Worker Node : Runs the applications and workloads, containing the kubelet , kube-proxy and container runtime . Etcd : Key-value store for all cluster data, used by the API server . Kube-scheduler : Assigns pods to nodes based on resource availability and constraints. Controllers : Node Controller : Monitors the status of nodes. Replication Controller : Ensures the desired number of pod replicas are running. Endpoints Controller : Manages the endpoints for services. Service Account & Token Controllers : Create default service accounts and tokens for new namespaces. Api-server : The front-end for the Kubernetes control plane, handling REST requests and updates to etcd. Container Runtime : Software responsible for running containers, such as Docker or containerd. Kubelet : An agent that runs on each node, ensuring containers are running in pods. Kube-proxy : Manages network rules for pod communication, enabling service discovery and load balancing.","title":"Key Concepts"},{"location":"k8s/_overview/notes/#pods","text":"Pod is the smallest deployable unit in Kubernetes. kubectl run nginx --image=nginx --restart=Never # pod-definition.yaml apiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx-container image: nginx ports: - containerPort: 80 kubectl apply -f pod-definition.yaml kubectl get pods","title":"Pods"},{"location":"k8s/_overview/notes/#deployments","text":"kubectl create deployment --image=nginx nginx --dry-run=client -o yaml","title":"Deployments"},{"location":"k8s/_overview/notes/#services","text":"Service is an abstraction that defines a logical set of pods and a policy by which to access them. Service types: - ClusterIP : Exposes the service on a cluster-internal IP. - NodePort : Exposes the service on each node's IP at a static port. - LoadBalancer : Exposes the service externally using a cloud provider's load balancer # node-port-definition.yaml apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: NodePort ports: - port: 80 # Service port (within the cluster) targetPort: 80 # Port on the pod nodePort: 30080 # Node port (external access) selector: app: nginx # Select pods with this label type: frontend # Additional label for selection # cluster-ip-definition.yaml apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: ClusterIP # if not specified, defaults to ClusterIP ports: - port: 80 # Service port (within the cluster) targetPort: 80 # Port on the pod selector: app: nginx # Select pods with this label type: frontend # Additional label for selection # load-balancer-definition.yaml apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: LoadBalancer # Exposes the service externally using a cloud provider's load balancer ports: - port: 80 # Service port (within the cluster) targetPort: 80 # Port on the pod nodePort: 30080 # Node port (external access)","title":"Services"},{"location":"k8s/_overview/notes/#labels-and-selectors-and-annotations","text":"Labels - used to organize and select resources in Kubernetes. Annotations - used to store additional metadata about resources. kubectl get pods --selector app=nginx kubectl get all --selector app=nginx,type=frontend","title":"Labels and Selectors and Annotations"},{"location":"k8s/_overview/notes/#taints-and-tolerations","text":"Taint is a node property that prevents pods from being scheduled. Toleration is a pod property that allows it to be scheduled on nodes with specific taints. kubectl taint node node1 key=value:NoSchedule{PreferNoSchedule,NoExecute} #untaint kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule- # pod-with-toleration.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx-container image: nginx tolerations: - key: \"key\" operator: \"Equal\" value: \"value\" effect: \"NoSchedule\"","title":"Taints and Tolerations"},{"location":"k8s/_overview/notes/#node-selectors-and-affinity","text":"# pod-with-node-selector.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx-container image: nginx nodeSelector: disktype: ssd # Node label to select # pod-with-affinity.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx-container image: nginx affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: disktype operator: In values: - ssd # Node label to select","title":"Node Selectors and Affinity"},{"location":"k8s/_overview/notes/#resource-requests-and-limits","text":"0.1 CPU = 100m CPU # means 100 milli CPU 1 CPU = 1000m CPU # pod-with-resources.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx-container image: nginx resources: requests: memory: \"64Mi\" # Minimum memory required cpu: \"250m\" # Minimum CPU required limits: memory: \"128Mi\" # Maximum memory allowed cpu: \"500m\" # Maximum CPU allowed","title":"Resource Requests and Limits"},{"location":"k8s/_overview/notes/#limit-ranges","text":"sets on a namespace level # limit-range.yaml apiVersion: v1 kind: LimitRange metadata: name: my-limit-range spec: limits: - default: memory: \"256Mi\" cpu: \"500m\" defaultRequest: memory: \"128Mi\" cpu: \"250m\" type: Container","title":"Limit Ranges"},{"location":"k8s/_overview/notes/#resource-quotas","text":"Resource quotas limit the total resources that can be consumed in a namespace. # resource-quota.yaml apiVersion: v1 kind: ResourceQuota metadata: name: my-resource-quota spec: hard: requests.cpu: \"10\" # Total CPU requests allowed requests.memory: \"20Gi\" # Total memory requests allowed limits.cpu: \"20\" # Total CPU limits allowed limits.memory: \"40Gi\" # Total memory limits allowed pods: \"10\" # Total number of pods allowed services: \"5\" # Total number of services allowed","title":"Resource Quotas"},{"location":"k8s/_overview/notes/#static-pods","text":"Static pods are managed directly by the kubelet on a node, not by the Kubernetes API server. They can be used for deploying kubernetes system components or other critical applications that need to run on specific nodes. kubelet watches this directory /etc/kubernetes/manifests for pod definitions. (directory can be configured) --pod-manifest-path option can be used to specify a different directory. staticPodPath option in kubelet configuration file can also be used to specify the directory.","title":"Static Pods"},{"location":"k8s/_overview/notes/#cli-commands","text":"```sh","title":"Cli Commands"},{"location":"k8s/_overview/notes/#create-an-nginx-pod","text":"kubectl run nginx --image=nginx","title":"Create an NGINX Pod"},{"location":"k8s/_overview/notes/#generate-pod-manifest-yaml-file-o-yaml-dont-create-it-dry-run","text":"kubectl run nginx --image=nginx --dry-run=client -o yaml","title":"Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)"},{"location":"k8s/_overview/notes/#create-a-deployment","text":"kubectl create deployment --image=nginx nginx","title":"Create a deployment"},{"location":"k8s/_overview/notes/#generate-deployment-yaml-file-o-yaml-dont-create-it-dry-run","text":"kubectl create deployment --image=nginx nginx --dry-run=client -o yaml","title":"Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)"},{"location":"k8s/_overview/notes/#generate-deployment-yaml-file-o-yaml-dont-create-itdry-run-and-save-it-to-a-file","text":"kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml","title":"Generate Deployment YAML file (-o yaml). Don\u2019t create it(\u2013dry-run) and save it to a file."},{"location":"k8s/_overview/notes/#make-necessary-changes-to-the-file-for-example-adding-more-replicas-and-then-create-the-deployment","text":"kubectl create -f nginx-deployment.yaml specify the --replicas option to create a deployment with 4 replicas. kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml","title":"Make necessary changes to the file (for example, adding more replicas) and then create the deployment."},{"location":"k8s/_overview/notes/#set-a-label-on-a-node","text":"kubectl label node node01 color=blue","title":"Set a label on a node"},{"location":"k8s/_overview/notes/#create-daemonset","text":"kubectl create create daemonset my-daemonset --image=nginx","title":"Create DaemonSet"},{"location":"k8s/_overview/notes/#check-the-differences-between-live-objects-and-manifests","text":"kubectl diff -f my-app/example-deployment.yaml","title":"Check the differences between live objects and manifests"},{"location":"k8s/api-objects/configmaps/","text":"# ConfigMaps ConfigMaps are Kubernetes objects that allow you to store configuration data in key-value pairs. They can be used to decouple configuration artifacts from image content to keep containerized applications portable. ```sh kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2 # Create a ConfigMap from a file kubectl create configmap my-config --from-file=config.txt # config.txt content: ## key1=value1 ## key2=value2 ``` # Example ConfigMap YAML ```yaml # configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: my-config data: key1: value1 key2: value2 ``` # Use ConfigMap in a Pod ```yaml # (envFrom)pod-with-configmap.yaml apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: - name: my-container image: nginx envFrom: - configMapRef: name: my-config # Reference to the ConfigMap # (valueFrom)pod-with-configmap.yaml apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: - name: my-container image: nginx env: - name: KEY1 valueFrom: configMapKeyRef: name: my-config # Reference to the ConfigMap key: key1 # Key in the ConfigMap ```","title":"Configmaps"},{"location":"k8s/api-objects/cronJob/","text":"# Job ```sh o=create job --dry-run=client -o yaml test \\ --image=registry.access.redhat.com/ubi8/ubi:8.6 \\ -- curl https://example.com ``` # CronJob ```sh o=create cronjob --dry-run=client -o yaml test \\ --image=registry.access.redhat.com/ubi8/ubi:8.6 \\ --schedule='0 0 * * *' \\ -- curl https://example.com ``` config map for CronJob ```yaml apiVersion: v1 kind: ConfigMap metadata: name: maintenance labels: ge: appsec-prune app: crictl data: maintenance.sh: | #!/bin/bash -eu NODES=$(oc get nodes -o=name) for NODE in ${NODES} do echo ${NODE} oc debug ${NODE} -- \\ chroot /host \\ /bin/bash -euxc 'crictl images ; crictl rmi --prune' done ```","title":"cronJob"},{"location":"k8s/api-objects/deployemnts/","text":"# Deployments ```sh # Create a deployment from a YAML file kubectl create -f deployment-definition.yml # Get deployments kubectl get deployments # Update a deployment (or create if it doesn't exist) kubectl apply -f deployment-definition.yml # Update a deployment with a new image kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1 # Check rollout status kubectl rollout status deployment/myapp-deployment # Check rollout history and undo if necessary kubectl rollout history deployment/myapp-deployment kubectl rollout undo deployment/myapp-deployment ```","title":"Deployemnts"},{"location":"k8s/api-objects/scaling/","text":"# HPA (Horizontal Pod Autoscaler) ## HPA manual ```sh kubectl scale deployment nginx --replicas=5 ``` ## HPA automatic * Imperative way ```sh kubectl autoscale deployment nginx-deployment --max=3 --cpu-percent=80 ``` * Declarative way ```yaml # hpa-definition.yaml apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: nginx-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nginx-deployment minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 80 ``` # VPA (Vertical Pod Autoscaler) ## installation","title":"Scaling"},{"location":"k8s/api-objects/secrets/","text":"# Secrets Kubernetes Secrets are used to store sensitive information, such as passwords, OAuth tokens, SSH keys, etc. They are designed to hold small amounts of sensitive data that you do not want to expose in your application code or configuration files. Secrets can be created from literal values, files, or directories. ```sh # Create a secret from literal values kubectl create secret generic my-secret --from-literal=username=myuser --from-literal=password=mypassword # Create a secret from a file kubectl create secret generic my-secret --from-file=ssh-privatekey=/path/to/private/key ``` # Example Secret YAML ```yaml # secret.yaml apiVersion: v1 kind: Secret metadata: name: my-secret type: Opaque # Default type for generic secrets data: username: bXl1c2Vy # Base64 encoded value of 'myuser' password: bXlwYXNzd29yZA== # Base64 encoded value of 'mypassword' ``` Base64 encoding ```sh # Encode a string to Base64 echo -n 'myuser' | base64 # Output: bXl1cmVy echo -n 'mypassword' | base64 # Output: bXlwcGFzc3dvcmQ= ``` # Use Secret in a Pod ```yaml # pod-with-secret.yaml apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: - name: my-container image: nginx env: - name: USERNAME valueFrom: secretKeyRef: name: my-secret # Reference to the Secret key: username # Key in the Secret - name: PASSWORD valueFrom: secretKeyRef: name: my-secret # Reference to the Secret key: password # Key in the Secret volumes: - name: secret-volume secret: secretName: my-secret # Reference to the Secret volumeMounts: - name: secret-volume mountPath: /etc/secret-volume # Path where the secret will be mounted readOnly: true # Mount as read-only","title":"Secrets"},{"location":"k8s/cluster-admin/api-certificates/","text":"API Certificates Kubernetes (in a typical kubeadm cluster) includes a cluster CA, and the kube-controller-manager runs the CSR approving/signing controllers that (upon approval) use that CA to sign certificates for components and users. Signed by the main cluster CA: - kube-apiserver serving certificate - kube-controller-manager and kube-scheduler client certificates - Kubelet client certificates (bootstrap and rotated) - Kubelet serving certificates (when using signer kubernetes.io/kubelet-serving) - User / custom client certificates (via CSR, signer kubernetes.io/kube-apiserver-client) Usually signed by separate authorities or keys: - etcd: Separate etcd CA (kubeadm creates etcd/ca.crt, ca.key) - Front-proxy (aggregation layer): front-proxy-ca (front-proxy-ca.crt/key) - Aggregated / extension API servers: may rely on front-proxy CA for client auth and their own serving certs - Service account tokens: Signed by service-account key pair (not an x509 CA); projected as JWTs Notes: - signerName must match the subject pattern (e.g. kubelet client: CN=system:node: , O=system:nodes). - A failed CSR with reason SignerValidationFailure often indicates mismatch between signerName, usages, or subject (CN/O). User Certificates User certificates are used to authenticate users to the Kubernetes API server. Generate a private key and CSR openssl req -new -newkey rsa:2048 -nodes -keyout user.key -out user.csr -subj \"/CN=akshay/O=devops\" Create a CSR object in Kubernetes kubectl explain certificatesigningrequest.spec kubectl get csr (if any exist) -o yaml # csr.yaml apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: csr_user spec: request: <base64-encoded-csr> signerName: kubernetes.io/kube-apiserver-client usages: - client authentication groups: - system:authenticated Create the CSR object kubectl apply -f csr.yaml kubectl get csr Approve the CSR kubectl certificate approve csr_user kubectl certificate approve akshay","title":"API Certificates"},{"location":"k8s/cluster-admin/api-certificates/#api-certificates","text":"Kubernetes (in a typical kubeadm cluster) includes a cluster CA, and the kube-controller-manager runs the CSR approving/signing controllers that (upon approval) use that CA to sign certificates for components and users. Signed by the main cluster CA: - kube-apiserver serving certificate - kube-controller-manager and kube-scheduler client certificates - Kubelet client certificates (bootstrap and rotated) - Kubelet serving certificates (when using signer kubernetes.io/kubelet-serving) - User / custom client certificates (via CSR, signer kubernetes.io/kube-apiserver-client) Usually signed by separate authorities or keys: - etcd: Separate etcd CA (kubeadm creates etcd/ca.crt, ca.key) - Front-proxy (aggregation layer): front-proxy-ca (front-proxy-ca.crt/key) - Aggregated / extension API servers: may rely on front-proxy CA for client auth and their own serving certs - Service account tokens: Signed by service-account key pair (not an x509 CA); projected as JWTs Notes: - signerName must match the subject pattern (e.g. kubelet client: CN=system:node: , O=system:nodes). - A failed CSR with reason SignerValidationFailure often indicates mismatch between signerName, usages, or subject (CN/O).","title":"API Certificates"},{"location":"k8s/cluster-admin/api-certificates/#user-certificates","text":"User certificates are used to authenticate users to the Kubernetes API server. Generate a private key and CSR openssl req -new -newkey rsa:2048 -nodes -keyout user.key -out user.csr -subj \"/CN=akshay/O=devops\" Create a CSR object in Kubernetes kubectl explain certificatesigningrequest.spec kubectl get csr (if any exist) -o yaml # csr.yaml apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: csr_user spec: request: <base64-encoded-csr> signerName: kubernetes.io/kube-apiserver-client usages: - client authentication groups: - system:authenticated Create the CSR object kubectl apply -f csr.yaml kubectl get csr Approve the CSR kubectl certificate approve csr_user kubectl certificate approve akshay","title":"User Certificates"},{"location":"k8s/cluster-admin/upgrade/","text":"Upgrade Kubernetes Cluster Kubernetes cluster upgrades involve updating the control plane components and worker nodes to a newer version. Only a one-minor-version gap is generally considered safe and officially supported limit for version skew. # check current cluster version kubectl version # check node kubelet version kubectl get nodes -o wide kubelet --version Upgrade cluster components # update kubeadm apt-get install kubeadm=1.33.0-1.1 kubeadm upgrade plan Update nodes ```sh take node out of service kubectl drain node01 --ignore-daemonsets kubeadm upgrade plan v1.33.0 kubeadm upgrade apply v1.33.0 apt-get install kubelet=1.33.0-1.1 systemctl daemon-reload systemctl restart kubelet mark node as schedulable kubectl uncordon node01 kubeadm upgrade node apt-get install kubeadm=1.33.0-1.1 kubeadm upgrade node","title":"Upgrade Kubernetes Cluster"},{"location":"k8s/cluster-admin/upgrade/#upgrade-kubernetes-cluster","text":"Kubernetes cluster upgrades involve updating the control plane components and worker nodes to a newer version. Only a one-minor-version gap is generally considered safe and officially supported limit for version skew. # check current cluster version kubectl version # check node kubelet version kubectl get nodes -o wide kubelet --version","title":"Upgrade Kubernetes Cluster"},{"location":"k8s/cluster-admin/upgrade/#upgrade-cluster-components","text":"# update kubeadm apt-get install kubeadm=1.33.0-1.1 kubeadm upgrade plan","title":"Upgrade cluster components"},{"location":"k8s/cluster-admin/upgrade/#update-nodes","text":"```sh","title":"Update nodes"},{"location":"k8s/cluster-admin/upgrade/#take-node-out-of-service","text":"kubectl drain node01 --ignore-daemonsets kubeadm upgrade plan v1.33.0 kubeadm upgrade apply v1.33.0 apt-get install kubelet=1.33.0-1.1 systemctl daemon-reload systemctl restart kubelet","title":"take node out of service"},{"location":"k8s/cluster-admin/upgrade/#mark-node-as-schedulable","text":"kubectl uncordon node01","title":"mark node as schedulable"},{"location":"k8s/cluster-admin/upgrade/#kubeadm-upgrade-node","text":"apt-get install kubeadm=1.33.0-1.1 kubeadm upgrade node","title":"kubeadm upgrade node"},{"location":"k8s/labs/EX328/installing/","text":"Installing Service Mesh on OpenShift Custom Resource Definitions (CRDs) A resource - is a Kubernetes API endpoint that stores objects of the same kind. A Custom Resource Definition (CRD) describes a custom resource used to extend the Kubernetes API. This feature supports custom object definitions, using them as native Kubernetes objects. CRD definition example: apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com # Name of the CRD, must be in the format `<plural>.<group>` spec: group: stable.example.com # Name to use in the REST API version: v1 scope: Namespaced # Defines the scope of the CRD names: plural: crontabs # Plural name used in the REST API singular: crontab # Singular name of the CRD. Used in the CLI and REST API. kind: CronTab # Type of objects managed by the CRD shortNames: # Short names for the CRD, used in the CLI - ct After a CRD is created, Kubernetes enables a new RESTful API endpoint to managed it. The endpoint created for the preceded example is: /apis/stable.example.com/v1/namespaces/{namespace}/crontabs Defining Kubernetes Operators A Kubernetes operator packages a Kubernetes application to automate installation, updates, and management. Operators rely on Custom Resource Definitions to extend the Kubernetes API. Operators run on a pod, and monitor the application to ensure it performs as expected. Installing Service Mesh on OpenShift OpenShift Service Mesh is installed using the Web Console, or CLI, and a Kubernetes operator. The installation process requires first installing the required operators , then deploying the Control Plane , and finally creating a Service Mesh Member Roll . Installing Required Operators OpenShift Service Mesh relies on the following operators: Jaeger - provides tracing features to monitor and troubleshoot applications . Kiali - provides observability to the service mesh through a web user interface . Elasticsearch - Stores traces and logs generated by Jaeger. Deploying the Control Plane (also operator) The control plane manages the configuration and policies for the service mesh. (The OpenShift Service Mesh Operator installation makes the operators available in all namespaces, so you can install the control plane in any project.)) To deploy a control plane in a project with the web UI, first navigate to the Installed Operators page, then to the Istio Service Mesh Control Plane page, and finally review and configure deployment parameters. Creating a Service Mesh Member Roll The ServiceMeshMemberRoll custom resource defines the projects belonging to a control plane . Any number of projects can be added to a ServiceMeshMemberRoll, however a project can be added only to one control plane. To create or edit a Service Mesh Member Roll, first navigate to the project where Red Hat OpenShift Service Mesh is installed, then navigate to the Istio Service Mesh Member Roll page, and finally review and configure installation parameters. Installing Service Mesh with the CLI subscription - The subscription is a custom resource that defines the operator to install, and the channel to use for updates. The channel defines the version of the operator to install. oc login -u developer -p redhat https://api.ocp4.example.com:6443 # clean up previous installations oc delete smcp --all oc -n openshift-operators get subs oc -n openshift-operators-redhat get subs oc -n openshift-operators delete sub jaeger-product kiali-ossm servicemeshoperator oc -n openshift-operators-redhat delete sub elasticsearch-operator oc delete project istio-system # verify the operators are deleted oc get csv oc get -n openshift-operators-redhat pods Deploy the Service Mesh Operator apiVersion: v1 kind: List metadata: {} items: - apiVersion: v1 kind: Namespace metadata: name: servicemesh-operators - apiVersion: operators.coreos.com/v1alpha1 kind: OperatorGroup metadata: name: servicemesh-operators namespace: servicemesh-operators spec: {} - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: elasticsearch-operator namespace: servicemesh-operators spec: channel: \"4.6\" installPlanApproval: Automatic name: elasticsearch-operator source: ossm-catalog sourceNamespace: openshift-marketplace # incomplete...","title":"Installing Service Mesh on OpenShift"},{"location":"k8s/labs/EX328/installing/#installing-service-mesh-on-openshift","text":"","title":"Installing Service Mesh on OpenShift"},{"location":"k8s/labs/EX328/installing/#custom-resource-definitions-crds","text":"A resource - is a Kubernetes API endpoint that stores objects of the same kind. A Custom Resource Definition (CRD) describes a custom resource used to extend the Kubernetes API. This feature supports custom object definitions, using them as native Kubernetes objects. CRD definition example: apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com # Name of the CRD, must be in the format `<plural>.<group>` spec: group: stable.example.com # Name to use in the REST API version: v1 scope: Namespaced # Defines the scope of the CRD names: plural: crontabs # Plural name used in the REST API singular: crontab # Singular name of the CRD. Used in the CLI and REST API. kind: CronTab # Type of objects managed by the CRD shortNames: # Short names for the CRD, used in the CLI - ct After a CRD is created, Kubernetes enables a new RESTful API endpoint to managed it. The endpoint created for the preceded example is: /apis/stable.example.com/v1/namespaces/{namespace}/crontabs","title":"Custom Resource Definitions (CRDs)"},{"location":"k8s/labs/EX328/installing/#defining-kubernetes-operators","text":"A Kubernetes operator packages a Kubernetes application to automate installation, updates, and management. Operators rely on Custom Resource Definitions to extend the Kubernetes API. Operators run on a pod, and monitor the application to ensure it performs as expected.","title":"Defining Kubernetes Operators"},{"location":"k8s/labs/EX328/installing/#installing-service-mesh-on-openshift_1","text":"OpenShift Service Mesh is installed using the Web Console, or CLI, and a Kubernetes operator. The installation process requires first installing the required operators , then deploying the Control Plane , and finally creating a Service Mesh Member Roll .","title":"Installing Service Mesh on OpenShift"},{"location":"k8s/labs/EX328/installing/#installing-required-operators","text":"OpenShift Service Mesh relies on the following operators: Jaeger - provides tracing features to monitor and troubleshoot applications . Kiali - provides observability to the service mesh through a web user interface . Elasticsearch - Stores traces and logs generated by Jaeger.","title":"Installing Required Operators"},{"location":"k8s/labs/EX328/installing/#deploying-the-control-plane-also-operator","text":"The control plane manages the configuration and policies for the service mesh. (The OpenShift Service Mesh Operator installation makes the operators available in all namespaces, so you can install the control plane in any project.)) To deploy a control plane in a project with the web UI, first navigate to the Installed Operators page, then to the Istio Service Mesh Control Plane page, and finally review and configure deployment parameters.","title":"Deploying the Control Plane (also operator)"},{"location":"k8s/labs/EX328/installing/#creating-a-service-mesh-member-roll","text":"The ServiceMeshMemberRoll custom resource defines the projects belonging to a control plane . Any number of projects can be added to a ServiceMeshMemberRoll, however a project can be added only to one control plane. To create or edit a Service Mesh Member Roll, first navigate to the project where Red Hat OpenShift Service Mesh is installed, then navigate to the Istio Service Mesh Member Roll page, and finally review and configure installation parameters.","title":"Creating a Service Mesh Member Roll"},{"location":"k8s/labs/EX328/installing/#installing-service-mesh-with-the-cli","text":"subscription - The subscription is a custom resource that defines the operator to install, and the channel to use for updates. The channel defines the version of the operator to install. oc login -u developer -p redhat https://api.ocp4.example.com:6443 # clean up previous installations oc delete smcp --all oc -n openshift-operators get subs oc -n openshift-operators-redhat get subs oc -n openshift-operators delete sub jaeger-product kiali-ossm servicemeshoperator oc -n openshift-operators-redhat delete sub elasticsearch-operator oc delete project istio-system # verify the operators are deleted oc get csv oc get -n openshift-operators-redhat pods","title":"Installing Service Mesh with the CLI"},{"location":"k8s/labs/EX328/installing/#deploy-the-service-mesh-operator","text":"apiVersion: v1 kind: List metadata: {} items: - apiVersion: v1 kind: Namespace metadata: name: servicemesh-operators - apiVersion: operators.coreos.com/v1alpha1 kind: OperatorGroup metadata: name: servicemesh-operators namespace: servicemesh-operators spec: {} - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: elasticsearch-operator namespace: servicemesh-operators spec: channel: \"4.6\" installPlanApproval: Automatic name: elasticsearch-operator source: ossm-catalog sourceNamespace: openshift-marketplace # incomplete...","title":"Deploy the Service Mesh Operator"},{"location":"k8s/labs/EX328/notes/","text":"OpenShift Service Mash Concepts Challenges Development challenges : - Discovery - services are often changing their IPs, each service needs to be referred to by a static name. - Elasticity - a system is scalable and have a orchestration solution. Security challenges - Security is a critical aspect of application development and deployment. In microservice architectures, services authenticate requests to validate identities. Microservices must authorize these validated requests and reject any that are unauthorized. Operational challenges : - Monitoring : measuring microservices performance and usage. Centralized logging : capturing and relating logs from all microservices. Tracing : correlating requests to multiple microservices belonging to the same user transaction. ## Components istio - istio is the core implementation of the service mesh architecture for the Kubernetes platform. Istio creates a control plane that centralizes service mesh capabilities and a data plane that creates the structure of the mesh. The data plane controls communication between services by injecting sidecar containers that capture traffic between services. Maistra - Maistra adds extended features to Istio , such as simplified multitenancy, explicit sidecar injection, and the use of OpenShift routes instead of Kubernetes ingress. Jaeger - an open-source server that centralizes and displays request traces. Each trace details all services a request interacts with. Maistra sends these traces to Jaeger for display, while microservices generate the necessary request headers for trace creation and aggregation. ElasticSearch Kiali - an open-source observability console for Istio that helps visualize the service mesh topology, monitor the health of services, and troubleshoot issues. Prometheus - an open-source server that collects and stores metrics from services in the mesh. Prometheus scrapes metrics from services and stores them in a time-series database. It also provides a query language to retrieve and display metrics. Grafana 3scale Red Hat OpenShift Service Mesh Architecture Data Plane A set of Envoy proxies istio-agent (aka istio Pilot) - running in each Envoy proxy The data plane performs the following tasks: Service discovery : Tracks the services deployed in a mesh. Health checks : Track the state (healthy or unhealthy) of the services deployed in a mesh. Traffic shaping and routing : Control the flow of network data between services. This includes tasks such as: Throttling the amount of traffic. Routing based on content. Circuit breaking. Controlling the amount of traffic routed among multiple versions of a service. Load balancing. Security : Perform authentication and authorization, and secure communication using mutual transport layer security (mTLS) between services in a mesh. Metrics and Telemetry : Gather metrics, logs, and distributed tracing information from services in the mesh. Control Plane The control plane manages the configuration and policies for the service mesh. The control plane consists of the istiod deployment. The istiod deployment consists of a single binary that contains a number of APIs used by the OpenShift Service Mesh. Istiod contains the APIs and functionality of the following components: Pilot - Provides service discovery, traffic management, and routing capabilities. Citadel - Provides security capabilities, such as certificate management and identity. Galley - Provides configuration validation, ingestion, and distribution.","title":"OpenShift Service Mash Concepts"},{"location":"k8s/labs/EX328/notes/#openshift-service-mash-concepts","text":"","title":"OpenShift Service Mash Concepts"},{"location":"k8s/labs/EX328/notes/#challenges","text":"Development challenges : - Discovery - services are often changing their IPs, each service needs to be referred to by a static name. - Elasticity - a system is scalable and have a orchestration solution. Security challenges - Security is a critical aspect of application development and deployment. In microservice architectures, services authenticate requests to validate identities. Microservices must authorize these validated requests and reject any that are unauthorized. Operational challenges : - Monitoring : measuring microservices performance and usage. Centralized logging : capturing and relating logs from all microservices. Tracing : correlating requests to multiple microservices belonging to the same user transaction. ## Components istio - istio is the core implementation of the service mesh architecture for the Kubernetes platform. Istio creates a control plane that centralizes service mesh capabilities and a data plane that creates the structure of the mesh. The data plane controls communication between services by injecting sidecar containers that capture traffic between services. Maistra - Maistra adds extended features to Istio , such as simplified multitenancy, explicit sidecar injection, and the use of OpenShift routes instead of Kubernetes ingress. Jaeger - an open-source server that centralizes and displays request traces. Each trace details all services a request interacts with. Maistra sends these traces to Jaeger for display, while microservices generate the necessary request headers for trace creation and aggregation. ElasticSearch Kiali - an open-source observability console for Istio that helps visualize the service mesh topology, monitor the health of services, and troubleshoot issues. Prometheus - an open-source server that collects and stores metrics from services in the mesh. Prometheus scrapes metrics from services and stores them in a time-series database. It also provides a query language to retrieve and display metrics. Grafana 3scale","title":"Challenges"},{"location":"k8s/labs/EX328/notes/#red-hat-openshift-service-mesh-architecture","text":"","title":"Red Hat OpenShift Service Mesh Architecture"},{"location":"k8s/labs/EX328/notes/#data-plane","text":"A set of Envoy proxies istio-agent (aka istio Pilot) - running in each Envoy proxy","title":"Data Plane"},{"location":"k8s/labs/EX328/notes/#the-data-plane-performs-the-following-tasks","text":"Service discovery : Tracks the services deployed in a mesh. Health checks : Track the state (healthy or unhealthy) of the services deployed in a mesh. Traffic shaping and routing : Control the flow of network data between services. This includes tasks such as: Throttling the amount of traffic. Routing based on content. Circuit breaking. Controlling the amount of traffic routed among multiple versions of a service. Load balancing. Security : Perform authentication and authorization, and secure communication using mutual transport layer security (mTLS) between services in a mesh. Metrics and Telemetry : Gather metrics, logs, and distributed tracing information from services in the mesh.","title":"The data plane performs the following tasks:"},{"location":"k8s/labs/EX328/notes/#control-plane","text":"The control plane manages the configuration and policies for the service mesh. The control plane consists of the istiod deployment. The istiod deployment consists of a single binary that contains a number of APIs used by the OpenShift Service Mesh. Istiod contains the APIs and functionality of the following components: Pilot - Provides service discovery, traffic management, and routing capabilities. Citadel - Provides security capabilities, such as certificate management and identity. Galley - Provides configuration validation, ingestion, and distribution.","title":"Control Plane"},{"location":"k8s/networking/cni/","text":"# CNI ``` /etc/cni/net.d/ ```","title":"Cni"},{"location":"k8s/networking/namespace/","text":"","title":"Namespace"},{"location":"k8s/networking/notes/","text":"# network namespaces ```sh # create network namespace ip netns add mynamespace # list network namespaces ip netns list # list network interfaces in a namespace ip netns exec mynamespace ip link ip -n mynamespace link # simpler way ```","title":"Notes"},{"location":"k8s/runtime-internals/containerd/","text":"ContainerD - Container Runtime ctr - Containerd's CLI tool for managing containers. (from containerd community) nerdctl - A Docker-compatible CLI for containerd, providing a familiar interface for users. (from containerd community) crictl - A CLI tool for CRI-compatible container runtimes, allowing you to manage pods and containers. (used for debugging, from k8s community)","title":"Containerd"},{"location":"k8s/runtime-internals/controllers/","text":"# Node controller Node monitoring period - 5 seconds Node monitoring Grace Period - 40 seconds (before marking a node as NotReady) POD Eviction Timeout - 5 minutes # Replication Controller and ReplicaSet **Replication Controller** is an older resource that ensures a specified number of pod replicas are running. **ReplicaSet** is a newer version that provides the same functionality but with additional features like support for set-based label selectors. ## yaml example ```yaml # replication-controller.yaml apiVersion: v1 kind: ReplicationController metadata: name: nginx-rc labels: app: myapp type: frontend spec: template: metadata: name: myapp-pod labels: app: myapp type: frontend spec: containers: - name: nginx-container image: nginx replicas: 3 ``` ```yaml # replicaset.yaml apiVersion: apps/v1 kind: ReplicaSet metadata: name: myapp-replicaset labels: app: myapp type: front-end spec: replicas: 3 template: metadata: name: myapp-pod labels: app: myapp type: front-end spec: containers: - name: nginx-container image: nginx selector: matchLabels: type: front-end ``` ## Commands ```sh kubectl get replicationcontroller kubectl get replicaset kubectl delete replicaset myapp-replicaset kubectl replace -f replicaset.yaml kubectl scale --replicas=5 replicaset myapp-replicaset ``` # Kube-ControllerManager Kube-ControllerManager is a control plane component that runs controller processes. Each controller is a separate process that watches the state of the cluster and makes or requests changes where needed. ```sh # settings cat /etc/kubernetes/manifests/kube-controller-manager.yaml ps -aux | grep kube-controller-manager ``` # Admission controller Admission controller is a piece of code that intercepts requests to the Kubernetes API server before they are persisted in etcd. It can modify or reject requests based on certain criteria. check admission controllers: ```sh kube-apiserver -h | grep enable-admission-plugins or kubectl exec kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep ```","title":"Controllers"},{"location":"k8s/runtime-internals/etcd/","text":"ETCD is a distributed reliable key-value store that is Simple, Secure and Fast.","title":"Etcd"},{"location":"k8s/runtime-internals/kube-proxy/","text":"**Kube-proxy** is a network proxy that __runs on each node__ in a Kubernetes cluster. It is responsible for managing network rules and facilitating communication between pods and services. Kube-proxy can operate in different modes, such as `iptables`, `ipvs`, or `userspace`, to handle traffic routing. ```sh # Installation curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kube-proxy # Run as a service cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-proxy Documentation=https://kubernetes.io/docs/concepts/services-networking/service/ After=network.target ... EOF # Troubleshooting kubectl get pods -n kube-system # kubeadm kubectl get daemonset -n kube-system","title":"Kube proxy"},{"location":"k8s/runtime-internals/kubelet/","text":"# Kubelet Kubelet - An agent that runs on each node in a Kubernetes cluster, responsible for managing the lifecycle of containers in pods. It ensures that the containers are running as expected, reports their status to the API server, and can also handle tasks like logging and monitoring. ```sh # Installation curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubelet # Run as a service cat <<EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://kubernetes.io/docs/concepts/architecture/kubelet/ After=network.target [Service] ExecStart=/usr/local/bin/kubelet \\\\ --config=/etc/kubernetes/kubelet.yaml \\\\ --kubeconfig=/etc/kubernetes/kubelet.conf \\\\ --container-runtime-endpoint=unix:///run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --v=2 Restart=always RestartSec=10s EOF # Troubleshooting ps -aux | grep kubelet","title":"Kubelet"},{"location":"k8s/runtime-internals/scheduler/","text":"# Scheduler Scheduler is a control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on. ```sh # Installation curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kube-scheduler ``` ```sh # run as a service cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/ After=network.target [Service] ExecStart=/usr/local/bin/kube-scheduler \\\\ --config=/etc/kubernetes/kube-scheduler.yaml \\\\ --v=2 ``` ```sh # kube admin config cat /etc/kubernetes/manifests/kube-scheduler.yaml ``` ## Multiple Schedulers You can run multiple schedulers in a cluster. Each scheduler can have its own configuration and scheduling policies. ```yaml # my-scheduler2.yaml apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: - schedulerName: my-scheduler2 leaderElection: leaderElect: true resourceNamespace: kube-system resourceName: my-scheduler2 ``` ### Deploying as a Service ```sh #my-scheduler2.service [Unit] Description=Kubernetes Scheduler 2 Documentation=https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/ After=network.target [Service] ExecStart=/usr/local/bin/kube-scheduler \\\\ --config=/etc/kubernetes/my-scheduler2.yaml ``` ### Deploying a Pod with Custom Scheduler ```yaml # pod-with-custom-scheduler.yaml apiVersion: v1 kind: Pod metadata: name: my-custom-scheduler-pod namespace: kube-system spec: containers: - command: - kube-scheduler - --address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --config=/etc/kubernetes/my-scheduler2.yaml image: k8s.gcr.io/kube-scheduler:v1.27.0 name: my-custom-scheduler ``` ## Troubleshooting Scheduler ```sh # Check scheduler logs kubectl logs my-custom-scheduler --namespace=kube-system","title":"Scheduler"},{"location":"k8s/security/authorization/","text":"Authorization Modes Authorization mode is set via --authorization-mode flag on the kube-apiserver. Multiple modes can be specified, and they are evaluated in order . ExecStart=/usr/local/bin/kube-apiserver \\ --authorization-mode=Node,RBAC,Webhook,AlwaysAllow \\ --advertise-address=<master-ip> \\ --allow-privileged=true \\ ... AlwaysAllow AlwaysDeny Node Authorization kubelet ==> kube-apiserver authorized via Node Authorizer . The node's certificate CN must be in the format system:node:<nodeName> and belong to the system:nodes group. This identity (client cert) lets the Node Authorizer and the NodeRestriction admission plugin tightly scope the kubelet to only list/watch or mutate objects tied to pods scheduled on that node (e.g. its own Pods, referenced Secrets/ConfigMaps, node/volume status). Any broader access still requires explicit RBAC bindings; the node cert alone does not grant cluster\u2011wide privileges. Attribute-Based Access Control (ABAC) One policy per user or group. Inconvenient to manage at scale, so mostly deprecated in favor of RBAC. Role-Based Access Control (RBAC) Create a role with specific permissions and bind it to a user or group. # role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] # rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: default subjects: - kind: User name: jane apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io Webhook External service called to authorize requests. More complex but very flexible. for example, you can use Open Policy Agent (OPA) as an external authorization service.","title":"Authorization Modes"},{"location":"k8s/security/authorization/#authorization-modes","text":"Authorization mode is set via --authorization-mode flag on the kube-apiserver. Multiple modes can be specified, and they are evaluated in order . ExecStart=/usr/local/bin/kube-apiserver \\ --authorization-mode=Node,RBAC,Webhook,AlwaysAllow \\ --advertise-address=<master-ip> \\ --allow-privileged=true \\ ...","title":"Authorization Modes"},{"location":"k8s/security/authorization/#alwaysallow","text":"","title":"AlwaysAllow"},{"location":"k8s/security/authorization/#alwaysdeny","text":"","title":"AlwaysDeny"},{"location":"k8s/security/authorization/#node-authorization","text":"kubelet ==> kube-apiserver authorized via Node Authorizer . The node's certificate CN must be in the format system:node:<nodeName> and belong to the system:nodes group. This identity (client cert) lets the Node Authorizer and the NodeRestriction admission plugin tightly scope the kubelet to only list/watch or mutate objects tied to pods scheduled on that node (e.g. its own Pods, referenced Secrets/ConfigMaps, node/volume status). Any broader access still requires explicit RBAC bindings; the node cert alone does not grant cluster\u2011wide privileges.","title":"Node Authorization"},{"location":"k8s/security/authorization/#attribute-based-access-control-abac","text":"One policy per user or group. Inconvenient to manage at scale, so mostly deprecated in favor of RBAC.","title":"Attribute-Based Access Control (ABAC)"},{"location":"k8s/security/authorization/#role-based-access-control-rbac","text":"Create a role with specific permissions and bind it to a user or group. # role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] # rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: default subjects: - kind: User name: jane apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io","title":"Role-Based Access Control (RBAC)"},{"location":"k8s/security/authorization/#webhook","text":"External service called to authorize requests. More complex but very flexible. for example, you can use Open Policy Agent (OPA) as an external authorization service.","title":"Webhook"},{"location":"k8s/security/rbac/","text":"Role-Based Access Control (RBAC) kubectl get roles kubectl describe role pod-reader kubectl get rolebindings kubectl auth can-i create deployments --namespace=default --as=jane Role and RoleBinding Namespace-scoped ClusterRole and ClusterRoleBinding Cluster-scoped, also can grant namespace-scoped permissions. # imperative way kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods kubectl create clusterrolebinding read-pods-global --clusterrole=pod-reader --user=j","title":"Role-Based Access Control (RBAC)"},{"location":"k8s/security/rbac/#role-based-access-control-rbac","text":"kubectl get roles kubectl describe role pod-reader kubectl get rolebindings kubectl auth can-i create deployments --namespace=default --as=jane","title":"Role-Based Access Control (RBAC)"},{"location":"k8s/security/rbac/#role-and-rolebinding","text":"Namespace-scoped","title":"Role and RoleBinding"},{"location":"k8s/security/rbac/#clusterrole-and-clusterrolebinding","text":"Cluster-scoped, also can grant namespace-scoped permissions. # imperative way kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods kubectl create clusterrolebinding read-pods-global --clusterrole=pod-reader --user=j","title":"ClusterRole and ClusterRoleBinding"},{"location":"k8s/security/secrets/","text":"","title":"Secrets"},{"location":"k8s/security/serviceaccounts/","text":"Service Accounts A service account used by application to interact with the Kubernetes API. # imperative way kubectl create serviceaccount dashboard-sa kubectl create rolebinding dashboard-sa-view --clusterrole=view --serviceaccount=default:dashboard-sa # set service account for deployment kubectl set serviceaccount deploy/web-dashboard dashboard-sa # generate token for service account kubectl create token dashboard-sa decode token https://jwt.io jq -R 'split(\".\") | select(length > 0) | .[0],.[1] | @base64d | fromjson' <<< \"<token>\" Under the hood Service account object created ==> 2. Token generated ==> 3. Token stored in a secret ==> 4. Secret mounted into pods into /var/run/secrets/kubernetes.io/serviceaccount changes in 1.24+ From v1.24, service account tokens are no longer auto-generated and mounted into pods by default. This is to enhance security by reducing the attack surface.","title":"Service Accounts"},{"location":"k8s/security/serviceaccounts/#service-accounts","text":"A service account used by application to interact with the Kubernetes API. # imperative way kubectl create serviceaccount dashboard-sa kubectl create rolebinding dashboard-sa-view --clusterrole=view --serviceaccount=default:dashboard-sa # set service account for deployment kubectl set serviceaccount deploy/web-dashboard dashboard-sa # generate token for service account kubectl create token dashboard-sa decode token https://jwt.io jq -R 'split(\".\") | select(length > 0) | .[0],.[1] | @base64d | fromjson' <<< \"<token>\"","title":"Service Accounts"},{"location":"k8s/security/serviceaccounts/#under-the-hood","text":"Service account object created ==> 2. Token generated ==> 3. Token stored in a secret ==> 4. Secret mounted into pods into /var/run/secrets/kubernetes.io/serviceaccount","title":"Under the hood"},{"location":"k8s/security/serviceaccounts/#changes-in-124","text":"From v1.24, service account tokens are no longer auto-generated and mounted into pods by default. This is to enhance security by reducing the attack surface.","title":"changes in 1.24+"},{"location":"k8s/security/users/","text":"Users","title":"Users"},{"location":"k8s/security/users/#users","text":"","title":"Users"},{"location":"k8s/storage/storage/","text":"* CRI (Container Runtime Interface) cri - Interface between Kubernetes and container runtimes, allowing Kubernetes to use different container runtimes like Docker, containerd, or CRI-O. * CSI (Container Storage Interface) csi - Standard for exposing block and file storage systems to containerized workloads on Kubernetes, enabling https://github.com/container-storage-interface/spec # HostPath Volumes HostPath volumes allow you to mount a file or directory from the host node's filesystem into a pod. ```yaml # hostpath-volume.yaml apiVersion: v1 kind: Pod metadata: name: hostpath-pod spec: containers: - name: hostpath-container image: nginx volumeMounts: - mountPath: /opt name: hostpath-volume volumes: - name: hostpath-volume hostPath: path: /data ``` # Persistent Volumes (PV) ```yaml # pv-definition.yaml apiVersion: v1 kind: PersistentVolume metadata: name: my-pv spec: accessModes: - ReadWriteOnce # other options: ReadOnlyMany, ReadWriteMany capacity: storage: 1Gi hostPath: path: /data ```","title":"Storage"},{"location":"k8s/tools/Kustomize/","text":"Kustomize apiVersion: kustomize.config.k8s.io/v1beta1 Kustomize - is a command-line configuration manager for Kubernetes objects. Kustomize works on directories that contain a kustomization.yaml file at the root. Kustomize has a concept of base and overlays Base A base directory contains a kustomization.yml file. # a structure of base directory base \u251c\u2500\u2500 configmap.yaml \u251c\u2500\u2500 deployment.yaml \u251c\u2500\u2500 secret.yaml \u251c\u2500\u2500 service.yaml \u251c\u2500\u2500 route.yaml \u2514\u2500\u2500 kustomization.yaml overlay \u2514\u2500\u2500 development \u2514\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 testing \u2514\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 production \u251c\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 patch.yaml # kustomization.yml (base direcotry) apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomizatio resources: - configmap.yaml - deployment.yaml - secret.yaml - service.yaml - route.yaml Overlays Kustomize overlays declarative YAML artifacts, or patches, that override the general settings without modifying the original files. The overlay directory contains a kustomization.yaml file. The kustomization.yaml file can refer to one or more directories as bases. Multiple overlays can use a common base kustomization directory. # kustomization.yaml (overlays/development directory) apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: dev-env bases: - ../../base # uses the base kustomization file at ../../base to create all the application resources in the dev-env namespace Kustomize provides fields to set values for all resources in the kustomization file: Field Description namespace Set a specific namespace for all resources. namePrefix Add a prefix to the name of all resources. nameSuffix Add a suffix to the name of all resources. commonLabels Add labels to all resources and selectors. commonAnnotations Add annotations to all resources and selectors. The patches mechanism has two elements: patch and target . # kustomization.yaml (overlays/testing) apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: test-env patches: # The patches field contains a list of patches. - patch: |- - op: replace ## The patch field defines operation, path, and value keys. In this example, the name changes to frontend-test path: /metadata/name value: frontend-test target: # The target field specifies the kind and name of the resource to apply the patch. In this example, you are changing the frontend deployment name to frontend-test. kind: Deployment name: frontend - patch: |- # This patch updates the number of replicas of the frontend deployment. - op: replace path: /spec/replicas value: 15 target: kind: Deployment name: frontend bases: # The frontend-app/overlay/testing/kustomization.yaml file uses the base kustomization file at ../../base to create an application. - ../../base commonLabels: # The commonLabels field adds the env: test label to all resources. env: test The patches mechanism also provides an option to include patches from a separate YAML file by using the path key. # kustomization.yaml uses a patch.yaml file apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: prod-env patches: - path: patch.yaml # The path field specifies the name of the patching YAML file. target: kind: Deployment name: frontend options: allowNameChange: true # The allowNameChange field enables kustomization to update the name by using a patch YAML file. bases: - ../../base commonLabels: env: prod # patch.yaml apiVersion: apps/v1 kind: Deployment metadata: name: frontend-prod # The metadata.name field in the patch file updates the frontend deployment name to frontend-prod if the allowNameChange field is set to true in the kustomization YAML file. spec: replicas: 5 View and Deploy Resources by Using Kustomize kubectl kustomize <kustomization-directory> - command to render the manifests without applying them to the cluster kubectl apply -k overlay/production - comman applies a kustomization with the -k flag oc delete kustomize <kustomization-directory> - command to delete the resources that were deployed by using Kustomize Kustomize Generators Kustomize has configMapGenerator and secretGenerator fields that generate configuration map and secret resources. Generators help to manage the content of configuration maps and secrets, by taking care of encoding and including content from other sources. Configuration Map Generator","title":"Kustomize"},{"location":"k8s/tools/Kustomize/#kustomize","text":"apiVersion: kustomize.config.k8s.io/v1beta1 Kustomize - is a command-line configuration manager for Kubernetes objects. Kustomize works on directories that contain a kustomization.yaml file at the root. Kustomize has a concept of base and overlays","title":"Kustomize"},{"location":"k8s/tools/Kustomize/#base","text":"A base directory contains a kustomization.yml file. # a structure of base directory base \u251c\u2500\u2500 configmap.yaml \u251c\u2500\u2500 deployment.yaml \u251c\u2500\u2500 secret.yaml \u251c\u2500\u2500 service.yaml \u251c\u2500\u2500 route.yaml \u2514\u2500\u2500 kustomization.yaml overlay \u2514\u2500\u2500 development \u2514\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 testing \u2514\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 production \u251c\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 patch.yaml # kustomization.yml (base direcotry) apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomizatio resources: - configmap.yaml - deployment.yaml - secret.yaml - service.yaml - route.yaml","title":"Base"},{"location":"k8s/tools/Kustomize/#overlays","text":"Kustomize overlays declarative YAML artifacts, or patches, that override the general settings without modifying the original files. The overlay directory contains a kustomization.yaml file. The kustomization.yaml file can refer to one or more directories as bases. Multiple overlays can use a common base kustomization directory. # kustomization.yaml (overlays/development directory) apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: dev-env bases: - ../../base # uses the base kustomization file at ../../base to create all the application resources in the dev-env namespace Kustomize provides fields to set values for all resources in the kustomization file: Field Description namespace Set a specific namespace for all resources. namePrefix Add a prefix to the name of all resources. nameSuffix Add a suffix to the name of all resources. commonLabels Add labels to all resources and selectors. commonAnnotations Add annotations to all resources and selectors. The patches mechanism has two elements: patch and target . # kustomization.yaml (overlays/testing) apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: test-env patches: # The patches field contains a list of patches. - patch: |- - op: replace ## The patch field defines operation, path, and value keys. In this example, the name changes to frontend-test path: /metadata/name value: frontend-test target: # The target field specifies the kind and name of the resource to apply the patch. In this example, you are changing the frontend deployment name to frontend-test. kind: Deployment name: frontend - patch: |- # This patch updates the number of replicas of the frontend deployment. - op: replace path: /spec/replicas value: 15 target: kind: Deployment name: frontend bases: # The frontend-app/overlay/testing/kustomization.yaml file uses the base kustomization file at ../../base to create an application. - ../../base commonLabels: # The commonLabels field adds the env: test label to all resources. env: test The patches mechanism also provides an option to include patches from a separate YAML file by using the path key. # kustomization.yaml uses a patch.yaml file apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: prod-env patches: - path: patch.yaml # The path field specifies the name of the patching YAML file. target: kind: Deployment name: frontend options: allowNameChange: true # The allowNameChange field enables kustomization to update the name by using a patch YAML file. bases: - ../../base commonLabels: env: prod # patch.yaml apiVersion: apps/v1 kind: Deployment metadata: name: frontend-prod # The metadata.name field in the patch file updates the frontend deployment name to frontend-prod if the allowNameChange field is set to true in the kustomization YAML file. spec: replicas: 5","title":"Overlays"},{"location":"k8s/tools/Kustomize/#view-and-deploy-resources-by-using-kustomize","text":"kubectl kustomize <kustomization-directory> - command to render the manifests without applying them to the cluster kubectl apply -k overlay/production - comman applies a kustomization with the -k flag oc delete kustomize <kustomization-directory> - command to delete the resources that were deployed by using Kustomize","title":"View and Deploy Resources by Using Kustomize"},{"location":"k8s/tools/Kustomize/#kustomize-generators","text":"Kustomize has configMapGenerator and secretGenerator fields that generate configuration map and secret resources. Generators help to manage the content of configuration maps and secrets, by taking care of encoding and including content from other sources.","title":"Kustomize Generators"},{"location":"k8s/tools/Kustomize/#configuration-map-generator","text":"","title":"Configuration Map Generator"},{"location":"k8s/troubleshooting/debug/","text":"```sh oc debug --to-namespace=\"default\" \\ -- curl -sS --connect-timeout 5 http://10.8.0.138:8080 ``` ```sh oc exec \\ deploy/long-load -- curl -s localhost:3000/hiccup?time=5 ```","title":"Debug"},{"location":"k8s/troubleshooting/help/","text":"kubectl explain deployment.spec.template.spec command provides the details for any field in the manifest. --dry-run=client -o yaml option to generate manifests.","title":"Help"},{"location":"k8s/workloads/scaling/","text":"","title":"Scaling"},{"location":"k8s/workloads/scalling/","text":"","title":"Scalling"},{"location":"k8s/workloads/scheduler/","text":"","title":"Scheduler"},{"location":"k8s/workloads-scheduling/scalling/","text":"","title":"Scalling"},{"location":"k8s/workloads-scheduling/scheduler/","text":"","title":"Scheduler"},{"location":"linux/","text":"Network Manager NFS SMB SystemD Teaming","title":"Linux"},{"location":"linux/EX342/ch4_fs_corruption/","text":"Recovering from File System Corruption Checking ext4 File System # -n (dry-run) - places the file system in read-only mode and answers \"no\" umount /dev/sdb1 e2fsck -n /dev/sdb1 Checking a file system requires a usable superblock. If the superblock location is corrupted, then locate a backup superblock. # Determine the location of backup superblocks dumpe2fs /dev/sdb1 | grep -i superblock >Backup superblock at 32768, Group descriptors at 32769-32769 >Backup superblock at 98304, Group descriptors at 98305-98305 >Backup superblock at 163840, Group descriptors at 163841-163841 >Backup superblock at 229376, Group descriptors at 229377-229377 # After locating a backup superblock, run e2fsck with the -b option e2fsck -b 32768 /dev/sdb1 echo $? # 0 - No errors; 1 - Errors corrected; 2 - File system errors not corrected ... Checking XFS File System # -n (dry-run) - places the file system in read-only mode and answers \"no\" umount /dev/sdb1 xfs_repair -n /dev/sdb1 xfs_repair /dev/sdb1 The xfs_repair command does not execute on an XFS file system that does not have a clear journal log. Mount and unmount the file system to clear the journal log. Restoring XFS File System ```sh umount /mnt/etc_restore xfs_repair -n /dev/vdb1 Metadata corruption detected at 0x564c8914a8e8, xfs_dir3_block block 0xa7e0/0x1000 entry \"subscription-manager\" at block 0 offset 456 in directory inode 144973 references invalid inode 1234567890 xfs_repair /dev/vdb1 mount /dev/vdb1 /mnt/etc_restore ls -la /mnt/etc_restore/lost+found find /mnt/etc_restore -inum 144973 tar -C /tmp -xzf /root/etc.tgz cd /tmp/etc/security/console.apps diff -s subscription-manager /mnt/etc_restore/lost+found/145282 cd /mnt/etc_restore/etc/security/console.apps mv /mnt/etc_restore/lost+found/145282 subscription-manager","title":"Recovering from File System Corruption"},{"location":"linux/EX342/ch4_fs_corruption/#recovering-from-file-system-corruption","text":"","title":"Recovering from File System Corruption"},{"location":"linux/EX342/ch4_fs_corruption/#checking-ext4-file-system","text":"# -n (dry-run) - places the file system in read-only mode and answers \"no\" umount /dev/sdb1 e2fsck -n /dev/sdb1 Checking a file system requires a usable superblock. If the superblock location is corrupted, then locate a backup superblock. # Determine the location of backup superblocks dumpe2fs /dev/sdb1 | grep -i superblock >Backup superblock at 32768, Group descriptors at 32769-32769 >Backup superblock at 98304, Group descriptors at 98305-98305 >Backup superblock at 163840, Group descriptors at 163841-163841 >Backup superblock at 229376, Group descriptors at 229377-229377 # After locating a backup superblock, run e2fsck with the -b option e2fsck -b 32768 /dev/sdb1 echo $? # 0 - No errors; 1 - Errors corrected; 2 - File system errors not corrected ...","title":"Checking ext4 File System"},{"location":"linux/EX342/ch4_fs_corruption/#checking-xfs-file-system","text":"# -n (dry-run) - places the file system in read-only mode and answers \"no\" umount /dev/sdb1 xfs_repair -n /dev/sdb1 xfs_repair /dev/sdb1 The xfs_repair command does not execute on an XFS file system that does not have a clear journal log. Mount and unmount the file system to clear the journal log.","title":"Checking XFS File System"},{"location":"linux/EX342/ch4_fs_corruption/#restoring-xfs-file-system","text":"```sh umount /mnt/etc_restore xfs_repair -n /dev/vdb1 Metadata corruption detected at 0x564c8914a8e8, xfs_dir3_block block 0xa7e0/0x1000 entry \"subscription-manager\" at block 0 offset 456 in directory inode 144973 references invalid inode 1234567890 xfs_repair /dev/vdb1 mount /dev/vdb1 /mnt/etc_restore ls -la /mnt/etc_restore/lost+found find /mnt/etc_restore -inum 144973 tar -C /tmp -xzf /root/etc.tgz cd /tmp/etc/security/console.apps diff -s subscription-manager /mnt/etc_restore/lost+found/145282 cd /mnt/etc_restore/etc/security/console.apps mv /mnt/etc_restore/lost+found/145282 subscription-manager","title":"Restoring XFS File System"},{"location":"linux/EX342/ch4_hardware/","text":"Kernel messages dmesg - preallocated a ring buffer dmesg or journalctl -k dmesg -f kern -l warn # dmesg -h # -l level (emerg, alert, crit, err, warning, notice, info, debug) # -f facility (kern, user, mail, daemon ... etc) # -T human readable time dmesg -T -l emerg,alert,crit,err CPU lscpu - display information about the CPU architecture Memory dmidecode - display information about the system's hardware components dmidecode -t memory Storage lsscsi - list SCSI devices lsscsi -v hdparm - get/set hard disk parameters hdparm -I /dev/sda USB lsusb - list USB devices lspci - list PCI devices Hardware errors rasdaemon - R A S (Reliability, Availability, Serviceability) daemon; logs hardware errors that are generated by kernel tracing. These trace events are logged in /sys/kernel/debug/tracing and are reported by rsyslog and journald . dnf install rasdaemon systemctl enable --now rasdaemon ras-mc-ctl --help ras-mc-ctl --summary ras-mc-ctl --errors Memory testing memtest86 - memory testing tool dnf install memtest86+ memtest-setup grub2-mkconfig -o /boot/grub2/grub.cfg","title":"Kernel messages"},{"location":"linux/EX342/ch4_hardware/#kernel-messages","text":"dmesg - preallocated a ring buffer dmesg or journalctl -k dmesg -f kern -l warn # dmesg -h # -l level (emerg, alert, crit, err, warning, notice, info, debug) # -f facility (kern, user, mail, daemon ... etc) # -T human readable time dmesg -T -l emerg,alert,crit,err","title":"Kernel messages"},{"location":"linux/EX342/ch4_hardware/#cpu","text":"lscpu - display information about the CPU architecture","title":"CPU"},{"location":"linux/EX342/ch4_hardware/#memory","text":"dmidecode - display information about the system's hardware components dmidecode -t memory","title":"Memory"},{"location":"linux/EX342/ch4_hardware/#storage","text":"lsscsi - list SCSI devices lsscsi -v hdparm - get/set hard disk parameters hdparm -I /dev/sda","title":"Storage"},{"location":"linux/EX342/ch4_hardware/#usb","text":"lsusb - list USB devices lspci - list PCI devices","title":"USB"},{"location":"linux/EX342/ch4_hardware/#hardware-errors","text":"rasdaemon - R A S (Reliability, Availability, Serviceability) daemon; logs hardware errors that are generated by kernel tracing. These trace events are logged in /sys/kernel/debug/tracing and are reported by rsyslog and journald . dnf install rasdaemon systemctl enable --now rasdaemon ras-mc-ctl --help ras-mc-ctl --summary ras-mc-ctl --errors","title":"Hardware errors"},{"location":"linux/EX342/ch4_hardware/#memory-testing","text":"memtest86 - memory testing tool dnf install memtest86+ memtest-setup grub2-mkconfig -o /boot/grub2/grub.cfg","title":"Memory testing"},{"location":"linux/EX342/ch4_kernel_modules/","text":"Managing Kernel Modules Most device drivers are built as kernel modules, as are file system drivers, encryption algorithms, and many other kernel features. Modules are found in the /lib/modules/\\<KERNEL_VERSION>/kernel/\\<SUBSYSTEM>/\\<KERNEL_MOdULE_NAME>.ko.xz . Viewing Kernel Modules lsmod - list loaded modules lsmod - nicely formats the contents of /proc/modules . Loading and Unloading Kernel Modules insmod - upload a module into the kernel rmmod - unload a module from the kernel modprobe - load and unload modules, as well as resolve dependencies and the checks kernel version(recommended) modprobe mce-inject -v > insmod /lib/modules/4.18.0-305.el8.x86_64/kernel/arch/x86/kernel/cpu/mce/mce-inject.ko.xz modprobe -r mce-inject -v > rmmod mce-inject Viewing Module Parameters Most modules set their parameters in /sys/module/\\<MODULE_NAME>/parameters/ directory, with each parameter being a file. If a parameter is modifiable at runtime, the corresponding file is root writable. modinfo -p kvm Managing Module Parameters modprobe - can set module parameters modprobe -v st buffer_kbs=64 > insmod /lib/modules/4.18.0-305.el8.x86_64/kernel/drivers/scsi/st.ko buffer_kbs=64 modprobe -c to set a parameter permanently, add a line to /etc/modprobe.d/\\<MEANINGFUL>.conf . options st buffer_kbs=64 max_sg_segs=512 options usb-storage quirks=0513:0132:w Kernel Module Documentation dnf install kernel-doc /usr/share/doc/kernel-doc-VER/Documentation/admin-guide/kernel-parameters.txt","title":"Managing Kernel Modules"},{"location":"linux/EX342/ch4_kernel_modules/#managing-kernel-modules","text":"Most device drivers are built as kernel modules, as are file system drivers, encryption algorithms, and many other kernel features. Modules are found in the /lib/modules/\\<KERNEL_VERSION>/kernel/\\<SUBSYSTEM>/\\<KERNEL_MOdULE_NAME>.ko.xz .","title":"Managing Kernel Modules"},{"location":"linux/EX342/ch4_kernel_modules/#viewing-kernel-modules","text":"lsmod - list loaded modules lsmod - nicely formats the contents of /proc/modules .","title":"Viewing Kernel Modules"},{"location":"linux/EX342/ch4_kernel_modules/#loading-and-unloading-kernel-modules","text":"insmod - upload a module into the kernel rmmod - unload a module from the kernel modprobe - load and unload modules, as well as resolve dependencies and the checks kernel version(recommended) modprobe mce-inject -v > insmod /lib/modules/4.18.0-305.el8.x86_64/kernel/arch/x86/kernel/cpu/mce/mce-inject.ko.xz modprobe -r mce-inject -v > rmmod mce-inject","title":"Loading and Unloading Kernel Modules"},{"location":"linux/EX342/ch4_kernel_modules/#viewing-module-parameters","text":"Most modules set their parameters in /sys/module/\\<MODULE_NAME>/parameters/ directory, with each parameter being a file. If a parameter is modifiable at runtime, the corresponding file is root writable. modinfo -p kvm","title":"Viewing Module Parameters"},{"location":"linux/EX342/ch4_kernel_modules/#managing-module-parameters","text":"modprobe - can set module parameters modprobe -v st buffer_kbs=64 > insmod /lib/modules/4.18.0-305.el8.x86_64/kernel/drivers/scsi/st.ko buffer_kbs=64 modprobe -c to set a parameter permanently, add a line to /etc/modprobe.d/\\<MEANINGFUL>.conf . options st buffer_kbs=64 max_sg_segs=512 options usb-storage quirks=0513:0132:w","title":"Managing Module Parameters"},{"location":"linux/EX342/ch4_kernel_modules/#kernel-module-documentation","text":"dnf install kernel-doc /usr/share/doc/kernel-doc-VER/Documentation/admin-guide/kernel-parameters.txt","title":"Kernel Module Documentation"},{"location":"linux/EX342/ch4_virtualization/","text":"Virtualization Support KVM - Kernel-based Virtual Machine. Hardware Virtualization Support egrep 'processor|vmx|svm' /proc/cpuinfo # svm - AMD \"Secure Virtual Machine\" # vmx - Intel \"Virtual Machine Extension\" Load modules modprobe -v kvm-intel Use the virsh capabilities command to check for hardware virtualization support. virsh capabilities Viewing Resource Use KVM virtual machines are implemented as regular processes on the host. To view, resource usage, user normal performance metrics tools such as top . More specialized tools: virsh nodecpustats virsh nodememstats virsh dommemstats ?? Libvirt XML Configuration libvirt service stores virtual machine definitions and related configuration as XML files. Path: /etc/libvirt Schema path: /usr/share/libvirt/schemas To ensure that the file is valid XML: xmllint --noout FILENAME After the file validates as well-structured XML, use: virt-xml-validate FILENAME Libvirt Networking Libvirt uses software bridges to provide virtual networks to virtual machines. Networks are defined in files in the /etc/libvirt/qemu/networks/ , and can be configured to autostart by adding a symbolic link in the /etc/libvirt/qemu/networks/autostart/ Virtual Networking issues: * A virtual machine is unreachable from the outside * nat * firewall on the hypervisor, or on teh VM * routing * The virtual network is operating in isolated mode. * A firewall rule on the hypervisor might be blocking outgoing connections.","title":"Virtualization Support"},{"location":"linux/EX342/ch4_virtualization/#virtualization-support","text":"KVM - Kernel-based Virtual Machine.","title":"Virtualization Support"},{"location":"linux/EX342/ch4_virtualization/#hardware-virtualization-support","text":"egrep 'processor|vmx|svm' /proc/cpuinfo # svm - AMD \"Secure Virtual Machine\" # vmx - Intel \"Virtual Machine Extension\" Load modules modprobe -v kvm-intel Use the virsh capabilities command to check for hardware virtualization support. virsh capabilities","title":"Hardware Virtualization Support"},{"location":"linux/EX342/ch4_virtualization/#viewing-resource-use","text":"KVM virtual machines are implemented as regular processes on the host. To view, resource usage, user normal performance metrics tools such as top . More specialized tools: virsh nodecpustats virsh nodememstats virsh dommemstats ??","title":"Viewing Resource Use"},{"location":"linux/EX342/ch4_virtualization/#libvirt-xml-configuration","text":"libvirt service stores virtual machine definitions and related configuration as XML files. Path: /etc/libvirt Schema path: /usr/share/libvirt/schemas To ensure that the file is valid XML: xmllint --noout FILENAME After the file validates as well-structured XML, use: virt-xml-validate FILENAME","title":"Libvirt XML Configuration"},{"location":"linux/EX342/ch4_virtualization/#libvirt-networking","text":"Libvirt uses software bridges to provide virtual networks to virtual machines. Networks are defined in files in the /etc/libvirt/qemu/networks/ , and can be configured to autostart by adding a symbolic link in the /etc/libvirt/qemu/networks/autostart/ Virtual Networking issues: * A virtual machine is unreachable from the outside * nat * firewall on the hypervisor, or on teh VM * routing * The virtual network is operating in isolated mode. * A firewall rule on the hypervisor might be blocking outgoing connections.","title":"Libvirt Networking"},{"location":"linux/EX342/ch5_lab/","text":"Review lab iSCSI Storage iscsiadm -m session iscsiadm -m node iscsiadm -m discovery -t st -p serverb.lab.example.com dig +short serverb.lab.example.com vi /etc/hosts iscsiadm -m discovery -t st -p serverb.lab.example.com iscsiadm -m node -T iqn.2016-01.com.example.lab:iscsistorage --login iscsiadm -m node -T iqn.2016-01.com.example.lab:iscsistorage | grep authmethod vi etc/iscsi/initiatorname.iscsi systemctl restart iscsid iscsiadm -m node -T iqn.2016-01.com.example.lab:iscsistorage --login grep \"Attached SCSI\" /var/log/messages LUKS # check if password is working lsblk cryptsetup luksOpen /dev/sda1 finance LVM mkdir /mnt/save mount /dev/save/old /mnt/save ls /mnt/save umount /mnt/save lvs vgcfgrestore -l save vgcfgrestore -f /etc/lvm/archive/save_00003-184394976.vg save lvchange -an /dev/save/old lvchange -ay /dev/save/old mount /dev/save/old /mnt/save XFS repair ls -la /mnt/save/luks blkid /dev/save/old umount /dev/save/old xfs_repair -n /dev/save/old xfs_repair -n -o force_geometry /dev/save/old xfs_repair -o force_geometry /dev/save/old mount /dev/save/old /mnt/save ls -la /mnt/save/lost+found/ file /mnt/save/lost+found/13801 mv /mnt/save/lost+found/13801 /mnt/save/luks/iscsistorage_luks_header LUKS restore header cryptsetup luksHeaderRestore /dev/sda1 --header-backup-file /mnt/save/luks/iscsistorage_luks_header cryptsetup luksOpen /dev/sda1 finance ### mkdir /mnt/finance mount /dev/mapper/finance /mnt/finance/ ls -la /mnt/finance Make persistent vi /etc/crypttab finance /dev/sda1 /path/to/keyfile vi /etc/fstab /dev/mapper/finance /mnt/finance xfs defaults 0 0","title":"Review lab"},{"location":"linux/EX342/ch5_lab/#review-lab","text":"","title":"Review lab"},{"location":"linux/EX342/ch5_lab/#iscsi-storage","text":"iscsiadm -m session iscsiadm -m node iscsiadm -m discovery -t st -p serverb.lab.example.com dig +short serverb.lab.example.com vi /etc/hosts iscsiadm -m discovery -t st -p serverb.lab.example.com iscsiadm -m node -T iqn.2016-01.com.example.lab:iscsistorage --login iscsiadm -m node -T iqn.2016-01.com.example.lab:iscsistorage | grep authmethod vi etc/iscsi/initiatorname.iscsi systemctl restart iscsid iscsiadm -m node -T iqn.2016-01.com.example.lab:iscsistorage --login grep \"Attached SCSI\" /var/log/messages","title":"iSCSI Storage"},{"location":"linux/EX342/ch5_lab/#luks","text":"# check if password is working lsblk cryptsetup luksOpen /dev/sda1 finance","title":"LUKS"},{"location":"linux/EX342/ch5_lab/#lvm","text":"mkdir /mnt/save mount /dev/save/old /mnt/save ls /mnt/save umount /mnt/save lvs vgcfgrestore -l save vgcfgrestore -f /etc/lvm/archive/save_00003-184394976.vg save lvchange -an /dev/save/old lvchange -ay /dev/save/old mount /dev/save/old /mnt/save","title":"LVM"},{"location":"linux/EX342/ch5_lab/#xfs-repair","text":"ls -la /mnt/save/luks blkid /dev/save/old umount /dev/save/old xfs_repair -n /dev/save/old xfs_repair -n -o force_geometry /dev/save/old xfs_repair -o force_geometry /dev/save/old mount /dev/save/old /mnt/save ls -la /mnt/save/lost+found/ file /mnt/save/lost+found/13801 mv /mnt/save/lost+found/13801 /mnt/save/luks/iscsistorage_luks_header","title":"XFS repair"},{"location":"linux/EX342/ch5_lab/#luks-restore-header","text":"cryptsetup luksHeaderRestore /dev/sda1 --header-backup-file /mnt/save/luks/iscsistorage_luks_header cryptsetup luksOpen /dev/sda1 finance ### mkdir /mnt/finance mount /dev/mapper/finance /mnt/finance/ ls -la /mnt/finance","title":"LUKS restore header"},{"location":"linux/EX342/ch5_lab/#make-persistent","text":"vi /etc/crypttab finance /dev/sda1 /path/to/keyfile vi /etc/fstab /dev/mapper/finance /mnt/finance xfs defaults 0 0","title":"Make persistent"},{"location":"linux/EX342/ch5_lvm_issues/","text":"Repairing LVM Issues Logical Volume Management (LVM) Similar to dm-multipath , logical volume management (LVM) uses the kernel Device Mapper subsystem to create the LVM devices. Logical volumes are created from in the /dev/ directory as dm- device nodes, but with symlinks in both the /dev/mapper/ and /dev/vg_name/ directories with persistent names. Configuration files LVM behavior is configured in the /etc/lvm/lvm.conf file. scan - witch directories to scan for physical volume device nodes obtain_device_list_from_udev - if udev should be used to obtain the list of devices for scanning preferred_names - A list of regular expressions that are used to match device names to the preferred name filter - A list of regular expressions that are used to filter devices that are scanned for physical volumes backup - determines whether a to store a text-based backup of volume metadata after every change archive - determines whether to archive old volume group configurations or layouts to use later to revert changes Reverting Changes If you need to revert changes to the LVM configuration, you can use the vgcfgrestore command to restore the previous configuration. vgcfgrestore -l vg_example vgcfgrestore -f /etc/lvm/archive/vg_example_00002-1484695080.vg vg_example In some scenarios, it might be necessary to deactivate the volume group and reactivate it to apply the changes. lvchange -an /dev/vg_example/lv_example lvchange -ay /dev/vg_example/lv_example","title":"Repairing LVM Issues"},{"location":"linux/EX342/ch5_lvm_issues/#repairing-lvm-issues","text":"","title":"Repairing LVM Issues"},{"location":"linux/EX342/ch5_lvm_issues/#logical-volume-management-lvm","text":"Similar to dm-multipath , logical volume management (LVM) uses the kernel Device Mapper subsystem to create the LVM devices. Logical volumes are created from in the /dev/ directory as dm- device nodes, but with symlinks in both the /dev/mapper/ and /dev/vg_name/ directories with persistent names.","title":"Logical Volume Management (LVM)"},{"location":"linux/EX342/ch5_lvm_issues/#configuration-files","text":"LVM behavior is configured in the /etc/lvm/lvm.conf file. scan - witch directories to scan for physical volume device nodes obtain_device_list_from_udev - if udev should be used to obtain the list of devices for scanning preferred_names - A list of regular expressions that are used to match device names to the preferred name filter - A list of regular expressions that are used to filter devices that are scanned for physical volumes backup - determines whether a to store a text-based backup of volume metadata after every change archive - determines whether to archive old volume group configurations or layouts to use later to revert changes","title":"Configuration files"},{"location":"linux/EX342/ch5_lvm_issues/#reverting-changes","text":"If you need to revert changes to the LVM configuration, you can use the vgcfgrestore command to restore the previous configuration. vgcfgrestore -l vg_example vgcfgrestore -f /etc/lvm/archive/vg_example_00002-1484695080.vg vg_example In some scenarios, it might be necessary to deactivate the volume group and reactivate it to apply the changes. lvchange -an /dev/vg_example/lv_example lvchange -ay /dev/vg_example/lv_example","title":"Reverting Changes"},{"location":"linux/EX342/ch5_storage/","text":"Linux Storage Virtual File System (VFS) The Virtual File System (VFS) provides supprt for standard POSIX system calls to read and write files. It implement system calls such as open() , read() , write() , close() , stat() , and lseek() . File system implemenentations such as XFS, ext4, FAT32, and others plugin to VFS as modules and so VFS provides a common abstraction layer for the data on those file systems. VFS maintains caches to improve performance, including an inode cache , dentry cache , buffer cache , and page cache . /proc/slabinfo file records the memory usage of inode and directory caches. # View VFS Memory Usage cat /proc/meminfo # Clear VFS caches echo 3 > /proc/sys/vm/drop_caches File Systems File systems provide the logical structures for organizing and naming metada and data in storage. Device Mapper The device mapper creates mapping tables of blocks from one device layer to blocks in another logical device. Device mapper use is optional; you can directly format physical block devices with a file system without using it. Example: an LVM logical volume named /dev/mapper/myvg-mylv1 is built from two physical volumes, /dev/vdb1 and /dev/vdb2 . When initially created with LVM utilities, the device mapper mapped the /dev/vdb1 and /dev/vdb2 physical block device partitions to the /dev/dm-0 higher logical device. # Use the dmsetup command to view the device mapper table dmsetup ls ls -l /dev/mapper/myvg1-mylv1 dmsetup table /dev/mapper/myvg1-mylv1 > 0 1015808 linear 253:17 2048 > 1015808 1015808 linear 253:18 2048 # The /dev/mapper/myvg1-mylv1 volume has two mappings. The first is a 1:1 linear mapping of the block device with major:minor number 253:17 to the first 1015808 blocks of /dev/mapper/myvg1-mylv1. The second is a 1:1 linear mapping of the block device with major:minor number 253:18 to the next 1015808 blocks of /dev/mapper/myvg1-mylv1, starting at block 1015808. The major:minor numbers 253:17 and 253:18 correspond to /dev/vdb1 and /dev/vdb2: ls -l /dev/vdb* > brw-rw----. 1 root disk 253, 16 Sep 30 18:28 /dev/vdb > brw-rw----. 1 root disk 253, 17 Sep 30 18:30 /dev/vdb1 > brw-rw----. 1 root disk 253, 18 Sep 30 18:30 /dev/vdb2 Disk Schedulers Disk schedulers are responsible for ordering the I/O requests that are submitted to a storage device. In RHEL8, block devices support only multi-queue scheduling. This enables block layer performance to scale with fast solid-state drives (SSDs) and multi-core CPUs. dmesg | grep -i 'io scheduler' none - implements a first-in first-out (FIFO) scheduling algorithm. mq-deadline - Sorts I/O requests into a read or write batch and schedules them for executuion in increasing logical block address (LBA) order. This scheduler is especially suitable where read operations are more frequent than write operations. kyber - Tunes itself to achieve a latency goal by calculating the latencyes of every I/O request that is submitted to the block I/O layer. bfq - Ensures that a single application never uses all the bandwith. Focuses on providing low latency rather than achieving high throughput. # to determine the current scheduler cat /sys/block/sda/queue/scheduler > [mq-deadline] kyber bfq none Device Mapper Multipath Device mapper multipath (DM-Multipath) configures multiple I/O paths between servers and storage arrays so that they appear as a single device. The SCSI Mid-layer The SCSI mid-layer is a bridge between the SCSI targets that present storage devices and the host bus adapters or hardware interface card drivers that communicate with the storage devices. The block device drivers are the SCSI disk (sd) and the SCSI CDROM (sr) driver. The SCSI mid-layer also provides one SCSI driver for character-based tape devices (st) and one for generic SCSI devices such as scanners (sg) . All devices that can use or emulate the SCSI protocol can use the SCSI mid-layer. The mid-layer includes seemingly unrelated devices such as SATA devices, USB storage devices, and virtual machine disks. These devices are presented as SCSI devices and use appropriate device names, such as /dev/sda . Some storage devices bypass this layer. For example, the /dev/vd devices are paravirtualized devices that use the virtio_blk * driver, which does not emulate a SCSI protocol. Low-level Drivers Low-level drivers communicate with physical system hardware. Examples: SCSI drivers for Qlogic(qla2xxx), local SATA(libata) or USB(ahci). Paravirtualized drivers such as virtio_blk interact directly with the scheduler at the block layer.","title":"Linux Storage"},{"location":"linux/EX342/ch5_storage/#linux-storage","text":"","title":"Linux Storage"},{"location":"linux/EX342/ch5_storage/#virtual-file-system-vfs","text":"The Virtual File System (VFS) provides supprt for standard POSIX system calls to read and write files. It implement system calls such as open() , read() , write() , close() , stat() , and lseek() . File system implemenentations such as XFS, ext4, FAT32, and others plugin to VFS as modules and so VFS provides a common abstraction layer for the data on those file systems. VFS maintains caches to improve performance, including an inode cache , dentry cache , buffer cache , and page cache . /proc/slabinfo file records the memory usage of inode and directory caches. # View VFS Memory Usage cat /proc/meminfo # Clear VFS caches echo 3 > /proc/sys/vm/drop_caches","title":"Virtual File System (VFS)"},{"location":"linux/EX342/ch5_storage/#file-systems","text":"File systems provide the logical structures for organizing and naming metada and data in storage.","title":"File Systems"},{"location":"linux/EX342/ch5_storage/#device-mapper","text":"The device mapper creates mapping tables of blocks from one device layer to blocks in another logical device. Device mapper use is optional; you can directly format physical block devices with a file system without using it. Example: an LVM logical volume named /dev/mapper/myvg-mylv1 is built from two physical volumes, /dev/vdb1 and /dev/vdb2 . When initially created with LVM utilities, the device mapper mapped the /dev/vdb1 and /dev/vdb2 physical block device partitions to the /dev/dm-0 higher logical device. # Use the dmsetup command to view the device mapper table dmsetup ls ls -l /dev/mapper/myvg1-mylv1 dmsetup table /dev/mapper/myvg1-mylv1 > 0 1015808 linear 253:17 2048 > 1015808 1015808 linear 253:18 2048 # The /dev/mapper/myvg1-mylv1 volume has two mappings. The first is a 1:1 linear mapping of the block device with major:minor number 253:17 to the first 1015808 blocks of /dev/mapper/myvg1-mylv1. The second is a 1:1 linear mapping of the block device with major:minor number 253:18 to the next 1015808 blocks of /dev/mapper/myvg1-mylv1, starting at block 1015808. The major:minor numbers 253:17 and 253:18 correspond to /dev/vdb1 and /dev/vdb2: ls -l /dev/vdb* > brw-rw----. 1 root disk 253, 16 Sep 30 18:28 /dev/vdb > brw-rw----. 1 root disk 253, 17 Sep 30 18:30 /dev/vdb1 > brw-rw----. 1 root disk 253, 18 Sep 30 18:30 /dev/vdb2","title":"Device Mapper"},{"location":"linux/EX342/ch5_storage/#disk-schedulers","text":"Disk schedulers are responsible for ordering the I/O requests that are submitted to a storage device. In RHEL8, block devices support only multi-queue scheduling. This enables block layer performance to scale with fast solid-state drives (SSDs) and multi-core CPUs. dmesg | grep -i 'io scheduler' none - implements a first-in first-out (FIFO) scheduling algorithm. mq-deadline - Sorts I/O requests into a read or write batch and schedules them for executuion in increasing logical block address (LBA) order. This scheduler is especially suitable where read operations are more frequent than write operations. kyber - Tunes itself to achieve a latency goal by calculating the latencyes of every I/O request that is submitted to the block I/O layer. bfq - Ensures that a single application never uses all the bandwith. Focuses on providing low latency rather than achieving high throughput. # to determine the current scheduler cat /sys/block/sda/queue/scheduler > [mq-deadline] kyber bfq none","title":"Disk Schedulers"},{"location":"linux/EX342/ch5_storage/#device-mapper-multipath","text":"Device mapper multipath (DM-Multipath) configures multiple I/O paths between servers and storage arrays so that they appear as a single device.","title":"Device Mapper Multipath"},{"location":"linux/EX342/ch5_storage/#the-scsi-mid-layer","text":"The SCSI mid-layer is a bridge between the SCSI targets that present storage devices and the host bus adapters or hardware interface card drivers that communicate with the storage devices. The block device drivers are the SCSI disk (sd) and the SCSI CDROM (sr) driver. The SCSI mid-layer also provides one SCSI driver for character-based tape devices (st) and one for generic SCSI devices such as scanners (sg) . All devices that can use or emulate the SCSI protocol can use the SCSI mid-layer. The mid-layer includes seemingly unrelated devices such as SATA devices, USB storage devices, and virtual machine disks. These devices are presented as SCSI devices and use appropriate device names, such as /dev/sda . Some storage devices bypass this layer. For example, the /dev/vd devices are paravirtualized devices that use the virtio_blk * driver, which does not emulate a SCSI protocol.","title":"The SCSI Mid-layer"},{"location":"linux/EX342/ch5_storage/#low-level-drivers","text":"Low-level drivers communicate with physical system hardware. Examples: SCSI drivers for Qlogic(qla2xxx), local SATA(libata) or USB(ahci). Paravirtualized drivers such as virtio_blk interact directly with the scheduler at the block layer.","title":"Low-level Drivers"},{"location":"linux/EX342/ch5_stratis/","text":"Stratis Storage Management Stratis - a local storage management solution for Linux - provides a unified interface for managing storage volumes. Installation dnf install stratis-cli stratisd systemctl enable --now stratisd Prepare devices for use in pools by erasing any existing file system, partition tables. wipes --all /dev/sdb Creating a Pool Stratis file systems are thinly provisioned without a fixed total size. If the size of the data approaches the virtual size of the file system, then Stratis grows the thin volume and its XFS file system automatically. # Create a pool named pool1 with the devices /dev/sdb and /dev/sdc stratis pool create pool1 /dev/sdb /dev/sdc # List block devices stratis blockdev # List pools stratis pool list # Create a file system named fs1 in the pool1 pool stratis fs create pool1 fs1 # List file systems stratis fs list pool1 # List device UUID lsblk --output=UUID /dev/stratis/pool1/fs1 blkid /dev/stratis/pool1/fs1 # Mount the file system mkdir /mnt/test mount /dev/stratis/pool1/fs1 /mnt/test df -h /mnt/test # Create/Delete a snapshot stratis fs snapshot pool1 fs1 snapshot1 stratis fs stratis filesystem destroy pool1 snapshot1","title":"Stratis Storage Management"},{"location":"linux/EX342/ch5_stratis/#stratis-storage-management","text":"Stratis - a local storage management solution for Linux - provides a unified interface for managing storage volumes.","title":"Stratis Storage Management"},{"location":"linux/EX342/ch5_stratis/#installation","text":"dnf install stratis-cli stratisd systemctl enable --now stratisd Prepare devices for use in pools by erasing any existing file system, partition tables. wipes --all /dev/sdb","title":"Installation"},{"location":"linux/EX342/ch5_stratis/#creating-a-pool","text":"Stratis file systems are thinly provisioned without a fixed total size. If the size of the data approaches the virtual size of the file system, then Stratis grows the thin volume and its XFS file system automatically. # Create a pool named pool1 with the devices /dev/sdb and /dev/sdc stratis pool create pool1 /dev/sdb /dev/sdc # List block devices stratis blockdev # List pools stratis pool list # Create a file system named fs1 in the pool1 pool stratis fs create pool1 fs1 # List file systems stratis fs list pool1 # List device UUID lsblk --output=UUID /dev/stratis/pool1/fs1 blkid /dev/stratis/pool1/fs1 # Mount the file system mkdir /mnt/test mount /dev/stratis/pool1/fs1 /mnt/test df -h /mnt/test # Create/Delete a snapshot stratis fs snapshot pool1 fs1 snapshot1 stratis fs stratis filesystem destroy pool1 snapshot1","title":"Creating a Pool"},{"location":"linux/EX342/ch6_rpm/ch6_dependency/","text":"Resolving Package Dependencies Issues RPM - Red Hat Package Manager (doesn't resolve dependencies) YUM - Yellowdog Updater, Modified YUMv4 is based on DNF (Dandified YUM) - used in RHEL 8 BaseOS - content in the BaseOS repository provides the core set of the OS functionality. AppStream - content in the AppStream repository provides applications that run on top of the OS. EPEL - Extra Packages for Enterprise Linux Diagnosing Dependency Issues # contains the history of installed and removed packages cat /var/log/dnf.log # -v verbose mode to see the details of the transaction dnf install -v dnf Package Dependencies dnf deplist yum # alternatively use rpm rpm -q --required yum # additionally, to list capabilities that the package provides: rpm -q --provides yum # replace the specified package with an earlier version. Works for packages that allow only one version to be installed. yum downgrade yum-3.4.3-161.el7.centos Note: some packages, such as for the Linux kernel, C libraries, or the system dbus, are restricted to update-only or install-only and cannot be downgraded. # list all version of a package dnf list --showduplicates python3-bind.noarch # yum lock yum versionlock list yum versionlock add WILDCARD yum versionlock delete WILDCARD yum versionlock clear Transaction History dnf history # to view the details of a specific transaction dnf history info 7 # undo a transaction dnf history undo 7 # To repeat a specific transaction dnf history redo 7 Section lab yum -v install rht-main yum deplist rht-main yum list --showduplicates rht-prereq yum versionlock yum versionlock delete rht-prereq-0:0.2-1.el8 yum versionlock yum install rht-main yum update rht-prereq yum versionlock add rht-prereq yum versionlock yum update rht-prereq","title":"Resolving Package Dependencies Issues"},{"location":"linux/EX342/ch6_rpm/ch6_dependency/#resolving-package-dependencies-issues","text":"RPM - Red Hat Package Manager (doesn't resolve dependencies) YUM - Yellowdog Updater, Modified YUMv4 is based on DNF (Dandified YUM) - used in RHEL 8 BaseOS - content in the BaseOS repository provides the core set of the OS functionality. AppStream - content in the AppStream repository provides applications that run on top of the OS. EPEL - Extra Packages for Enterprise Linux","title":"Resolving Package Dependencies Issues"},{"location":"linux/EX342/ch6_rpm/ch6_dependency/#diagnosing-dependency-issues","text":"# contains the history of installed and removed packages cat /var/log/dnf.log # -v verbose mode to see the details of the transaction dnf install -v dnf","title":"Diagnosing Dependency Issues"},{"location":"linux/EX342/ch6_rpm/ch6_dependency/#package-dependencies","text":"dnf deplist yum # alternatively use rpm rpm -q --required yum # additionally, to list capabilities that the package provides: rpm -q --provides yum # replace the specified package with an earlier version. Works for packages that allow only one version to be installed. yum downgrade yum-3.4.3-161.el7.centos Note: some packages, such as for the Linux kernel, C libraries, or the system dbus, are restricted to update-only or install-only and cannot be downgraded. # list all version of a package dnf list --showduplicates python3-bind.noarch # yum lock yum versionlock list yum versionlock add WILDCARD yum versionlock delete WILDCARD yum versionlock clear","title":"Package Dependencies"},{"location":"linux/EX342/ch6_rpm/ch6_dependency/#transaction-history","text":"dnf history # to view the details of a specific transaction dnf history info 7 # undo a transaction dnf history undo 7 # To repeat a specific transaction dnf history redo 7","title":"Transaction History"},{"location":"linux/EX342/ch6_rpm/ch6_dependency/#section-lab","text":"yum -v install rht-main yum deplist rht-main yum list --showduplicates rht-prereq yum versionlock yum versionlock delete rht-prereq-0:0.2-1.el8 yum versionlock yum install rht-main yum update rht-prereq yum versionlock add rht-prereq yum versionlock yum update rht-prereq","title":"Section lab"},{"location":"linux/EX342/ch6_rpm/ch6_rpm_database/","text":"Recovering RPM Database Berkeley DB - embedded database library manages the RPM package and transaction data that are stored in the /var/lib/rpm directory. Cleaning Stale Lock Files Typically, stale locks are left behind by abnormal termination from a kernel crash, a kill command, or a loss of system power. Two methods are available to clean stale lock files: (Recommended) Reboot the system. Rebooting removes the locks during the sysinit portion of the boot. If rebooting does not resolve the issue, then manually remove the lock files and RPM database index files. # delete all files in the /var/spool/up2date directory cd /var/spool/up2date rm -f * rm .* # verify that no processes with open RPM database files are running ps -aux | grep -e rpm -e yum -e up2date # if no RPM database processes are running, delete the database index files rm /var/lib/rpm/__db* Recovering the RPM Database Recovering the RPM database might require rebuilding the package metadata files in the /var/lib/rpm directory. Rebuilding the metadata files also requires reconstructing the database indexes. # backup the existing RPM database tar cjvf rpmdb-$(date +%Y%m%d-%H%M).tar.bz2 /var/lib/rpm # verify RPM database integrity cd /var/lib/rpm /usr/lib/rpm/rpmdb_verify Packages # if verification fails, rebuild the RPM database mv Packages Packages.bad /usr/lib/rpm/rpmdb_dump Packages.bad | /usr/lib/rpm/rpmdb_load Packages /usr/lib/rpm/rpmdb_verify Packages # rebuild the RPM database indexes rpm -v --rebuilddb # verify the RPM database integrity /usr/lib/rpm/rpmdb_verify Packages","title":"Recovering RPM Database"},{"location":"linux/EX342/ch6_rpm/ch6_rpm_database/#recovering-rpm-database","text":"Berkeley DB - embedded database library manages the RPM package and transaction data that are stored in the /var/lib/rpm directory.","title":"Recovering RPM Database"},{"location":"linux/EX342/ch6_rpm/ch6_rpm_database/#cleaning-stale-lock-files","text":"Typically, stale locks are left behind by abnormal termination from a kernel crash, a kill command, or a loss of system power. Two methods are available to clean stale lock files: (Recommended) Reboot the system. Rebooting removes the locks during the sysinit portion of the boot. If rebooting does not resolve the issue, then manually remove the lock files and RPM database index files. # delete all files in the /var/spool/up2date directory cd /var/spool/up2date rm -f * rm .* # verify that no processes with open RPM database files are running ps -aux | grep -e rpm -e yum -e up2date # if no RPM database processes are running, delete the database index files rm /var/lib/rpm/__db*","title":"Cleaning Stale Lock Files"},{"location":"linux/EX342/ch6_rpm/ch6_rpm_database/#recovering-the-rpm-database","text":"Recovering the RPM database might require rebuilding the package metadata files in the /var/lib/rpm directory. Rebuilding the metadata files also requires reconstructing the database indexes. # backup the existing RPM database tar cjvf rpmdb-$(date +%Y%m%d-%H%M).tar.bz2 /var/lib/rpm # verify RPM database integrity cd /var/lib/rpm /usr/lib/rpm/rpmdb_verify Packages # if verification fails, rebuild the RPM database mv Packages Packages.bad /usr/lib/rpm/rpmdb_dump Packages.bad | /usr/lib/rpm/rpmdb_load Packages /usr/lib/rpm/rpmdb_verify Packages # rebuild the RPM database indexes rpm -v --rebuilddb # verify the RPM database integrity /usr/lib/rpm/rpmdb_verify Packages","title":"Recovering the RPM Database"},{"location":"linux/EX358/NFS/","text":"NFS Server Services dnf install nfs-utils systemctl enable --now nfs-server check the protocol versions cat /proc/fs/nfsd/versions Firewall NFSv3 server behind firewall NFS 4.1 default port: tcp/2049 firewall-cmd --add-service=nfs --permanent firewall-cmd --reload Exports cat /etc/exports ls /etc/exports.d/ vim /etc/export.d/nfs.exports example.exports /srv/myshare client1.example.com /srv/myshare *.example.com /srv/myshare 192.168.0.0/24 /srv/myshare 192.168.0.0/24 client1.example.com *.example.net /srv/myshare client1.example.com(rw,no_root_squash) # no_root_squash: root user on client1.example.com will have root access on NFS share Inspectiong NFS Exports # show current exports exportfs # show current exports with details exportfs -v # apply changes exportfs -r # show NFS shares on localhost showmount -e localhost Client mount serverd.lab.example.com:/nfsshare /mnt /etc/fstab serverd.lab.example.com:/nfsshare /share nfs defaults 0 0 mount /share Links NFS Performance tuning NFS and Smb exports of the same directory","title":"NFS Server"},{"location":"linux/EX358/NFS/#nfs-server","text":"","title":"NFS Server"},{"location":"linux/EX358/NFS/#services","text":"dnf install nfs-utils systemctl enable --now nfs-server check the protocol versions cat /proc/fs/nfsd/versions","title":"Services"},{"location":"linux/EX358/NFS/#firewall","text":"NFSv3 server behind firewall NFS 4.1 default port: tcp/2049 firewall-cmd --add-service=nfs --permanent firewall-cmd --reload","title":"Firewall"},{"location":"linux/EX358/NFS/#exports","text":"cat /etc/exports ls /etc/exports.d/ vim /etc/export.d/nfs.exports example.exports /srv/myshare client1.example.com /srv/myshare *.example.com /srv/myshare 192.168.0.0/24 /srv/myshare 192.168.0.0/24 client1.example.com *.example.net /srv/myshare client1.example.com(rw,no_root_squash) # no_root_squash: root user on client1.example.com will have root access on NFS share","title":"Exports"},{"location":"linux/EX358/NFS/#inspectiong-nfs-exports","text":"# show current exports exportfs # show current exports with details exportfs -v # apply changes exportfs -r # show NFS shares on localhost showmount -e localhost","title":"Inspectiong NFS Exports"},{"location":"linux/EX358/NFS/#client","text":"mount serverd.lab.example.com:/nfsshare /mnt /etc/fstab serverd.lab.example.com:/nfsshare /share nfs defaults 0 0 mount /share","title":"Client"},{"location":"linux/EX358/NFS/#links","text":"NFS Performance tuning NFS and Smb exports of the same directory","title":"Links"},{"location":"linux/EX358/NetworkManager/","text":"NetworkManager - system demon, which monitors and manages network settings and uses files in /etc/sysconfig/network-scripts/ to store them. In NetworkManager: * A device is a network interface. * A 'connection' is a collection of settings that can be configured for a device. * Only one connection is active for any one device at a time. Multiple connections may exist, for use by different devices or to allow a configuration to be altered for the same device. * Each connection has a name or ID that identifies it. * The /etc/sysconfig/network-scripts/ifcfg-name file stores the persistent configuration for a connection, where name is the name of the connection. When the connection name has spaces in its name, the spaces are replaced with underscores in the file name. This file can be edited by hand, if desired. * The nmcli utility creates and edits connection files from the shell prompt. nmcli dev status nmcli dev show nmcli con show nmcli con show --active ip addr show eno1 nmcli gen permissions # adds a new connection named eno1 for the interface eno1. This connection: # Gets IPv4 networking information dynamically from DHCP. # Gets IPv6 networking information from router advertisements on the local link. nmcli con add con-name eno1 type ethernet ifname eno1 # adds a new connection named static-eno1 for the interface eno1. This connection: # Sets a static IPv4 address of 192.0.2.7/24 and an IPv4 gateway router of 192.0.2.1. # Sets a static IPv6 address of 2001:db8:0:1::c000:207/64 and an IPv6 gateway router of 2001:db8:0:1::1. nmcli con add con-name static-eno1 type ethernet ifname eno1 \\ ipv6.address 2001:db8:0:1::c000:207/64 ipv6.gateway 2001:db8:0:1::1 \\ ipv4.address 192.0.2.7/24 ipv4.gateway 192.0.2.1 nmcli con up static-eno1 nmcli dev dis eno1","title":"NetworkManager"},{"location":"linux/EX358/NetworkManager_A/","text":"yum install rhel-system-roles - name: Find 2nd network interface hosts: servers become: true vars: target_mac: \"52:54:00:01:fa:0a\" tasks: - name: Find the_interface for target_mac set_fact: the_interface: \"{{ item }}\" when: - ansible_facts[item]['macaddress'] is defined - ansible_facts[item]['macaddress'] == target_mac loop: \"{{ ansible_facts['interfaces'] }}\" - name: Display the_interface debug: var: the_interface --- - name: Configure 2nd network interface hosts: servers become: true vars: target_mac: \"52:54:00:01:fa:0a\" network_connections: - name: static_net type: ethernet mac: \"{{ target_mac }}\" state: up ip: dhcp4: no address: - 192.168.0.1/24 roles: - rhel-system-roles.network # nmcli con show # nmcli con show static_net | grep ipv4","title":"NetworkManager A"},{"location":"linux/EX358/SMB/","text":"Install and configure samba yum install samba systemctl enable --now smb mkdir /srv/sambashare chgrp developers /srv/sambashare # The SGID bit ensures that new content automatically belongs to the developers group chmod 2775 /srv/sambashare Settings SELinux Context Types For Samba to work correctly with SELinux, set the directory context to samba_share_t . Samba can also serve files labeled with the SELinux public_content_t (read-only) and public_content_rw_t (read-write) types. # add a rule to the SELinux policy so that the /srv/samabashare directory and its content have a context type of samba_share_t semanage fcontext -a -t samba_share_t '/srv/sambashare(/.*)?' # apply the SELinux rule to the /srv/sambashare directory restorecon -Rv /srv/sambashare Configuring Samba /etc/samba/smb.conf - divided into sections. Each section starts with the section name in square brackets. [global] - provides general server configuration and default values. The next sections define file or printer shares. [global] workgroup = MYCOMPANY smb encrypt = required server min protocol = SMB3 [data] path = /smbshare write list = @marketing # verify the configuration file testparm Prepare Samba Users useradd -s /sbin/nologin operator1 # add the linux account to the Samba database with the `smbpasswd` command from the `samba-common-tools` package smbpasswd -a operator1 # remove user from Samba database smbpasswd -x operator1 # list all users in the Samba database pdbedit -L samba maintains its database in /var/lib/samba/private/ directory Firewall firewall-cmd --add-service=samba --permanent firewall-cmd --reload Client dnf install cifs-utils mount -o username=operator1 //host.example.com/devcode /mnt mount -t cifs //malinka3.lan/transmission /mnt/malinka3-smb/ /ext/fstab //host.example.com/devcode /data cifs credentials=/etc/samba/credentials 0 0 /etc/samba/credentials username=operator1 password=redhat Multiuser cifscreds add host.example.com","title":"Install and configure samba"},{"location":"linux/EX358/SMB/#install-and-configure-samba","text":"yum install samba systemctl enable --now smb mkdir /srv/sambashare chgrp developers /srv/sambashare # The SGID bit ensures that new content automatically belongs to the developers group chmod 2775 /srv/sambashare","title":"Install and configure samba"},{"location":"linux/EX358/SMB/#settings-selinux-context-types","text":"For Samba to work correctly with SELinux, set the directory context to samba_share_t . Samba can also serve files labeled with the SELinux public_content_t (read-only) and public_content_rw_t (read-write) types. # add a rule to the SELinux policy so that the /srv/samabashare directory and its content have a context type of samba_share_t semanage fcontext -a -t samba_share_t '/srv/sambashare(/.*)?' # apply the SELinux rule to the /srv/sambashare directory restorecon -Rv /srv/sambashare","title":"Settings SELinux Context Types"},{"location":"linux/EX358/SMB/#configuring-samba","text":"/etc/samba/smb.conf - divided into sections. Each section starts with the section name in square brackets. [global] - provides general server configuration and default values. The next sections define file or printer shares. [global] workgroup = MYCOMPANY smb encrypt = required server min protocol = SMB3 [data] path = /smbshare write list = @marketing # verify the configuration file testparm","title":"Configuring Samba"},{"location":"linux/EX358/SMB/#prepare-samba-users","text":"useradd -s /sbin/nologin operator1 # add the linux account to the Samba database with the `smbpasswd` command from the `samba-common-tools` package smbpasswd -a operator1 # remove user from Samba database smbpasswd -x operator1 # list all users in the Samba database pdbedit -L samba maintains its database in /var/lib/samba/private/ directory","title":"Prepare Samba Users"},{"location":"linux/EX358/SMB/#firewall","text":"firewall-cmd --add-service=samba --permanent firewall-cmd --reload","title":"Firewall"},{"location":"linux/EX358/SMB/#client","text":"dnf install cifs-utils mount -o username=operator1 //host.example.com/devcode /mnt mount -t cifs //malinka3.lan/transmission /mnt/malinka3-smb/ /ext/fstab //host.example.com/devcode /data cifs credentials=/etc/samba/credentials 0 0 /etc/samba/credentials username=operator1 password=redhat","title":"Client"},{"location":"linux/EX358/SMB/#multiuser","text":"cifscreds add host.example.com","title":"Multiuser"},{"location":"linux/EX358/SystemD/","text":"# list systemd unit types systemctl -t help # list all units systemctl ## list service units systemctl --type=service systemctl --failed --type=service systemctl is-active sshd systemctl is-enabled sshd # View the enabled and disabled settings for all units systemctl list-unit-files --type=service systemctl list-dependencies sshd systemctl list-dependencies sshd --reverse # To avoid accidentally starting a service, you can mask that service # A disabled service is not started automatically at boot or by other unit files, but can be started manually. A masked service cannot be started manually or automatically. systemctl mask iptables systemctl unmask iptables","title":"SystemD"},{"location":"linux/EX358/SystemD_A/","text":"Ansible provides three modules to work with systemd: service , systemd , and service_facts . --- - name: Configure 2nd network interface hosts: servers become: true tasks: - name: Confirm NetworkManager is running service: name: NetworkManager state: started enabled: true","title":"SystemD A"},{"location":"linux/EX358/Teaming/","text":"RHEL implements network teaming with a small kernel driver and a user-space daemon, teamd Software, called runners , implements load balancing and active-backup logic, such as roundrobin. When controlling a team interface using NetworkManager, or when troubleshooting it, you should keep the following facts in mind: Starting the team interface does not automatically start its port interfaces. Starting a port interface always starts the team interface. Stopping the team interface also stops the port interfaces. A team interface without ports can start static IP connections. A team interface without ports waits for ports when starting DHCP connections. If a team interface has a DHCP connection and is waiting for ports, it completes its activation when a port with a carrier signal is added. If a team interface has a DHCP connection and is waiting for ports, it continues to wait when a port without a carrier signal is added. Create a team interface nmcli con add type team con-name CONN_NAME ifname IFACE_NAME team.runner RUNNER nmcli con add type team con-name team0 ifname team0 team.runner loadbalance nmcli con mod team0 ipv4.addresses 192.0.2.4/24 nmcli con mod team0 ipv4.method manual Create the Port Interfaces nmcli con add type team-slave con-name team0-eth1 ifname eth1 master team0 nmcli con add type team-slave con-name team0-eth2 ifname eth2 master team0 Bring the Team and Port Interfaces Up or Down nmcli con up team0 nmcli con up team0-eth1 teamdctl team0 state Troubleshooting Network Teams","title":"Teaming"},{"location":"linux/EX358/Teaming/#create-a-team-interface","text":"nmcli con add type team con-name CONN_NAME ifname IFACE_NAME team.runner RUNNER nmcli con add type team con-name team0 ifname team0 team.runner loadbalance nmcli con mod team0 ipv4.addresses 192.0.2.4/24 nmcli con mod team0 ipv4.method manual","title":"Create a team interface"},{"location":"linux/EX358/Teaming/#create-the-port-interfaces","text":"nmcli con add type team-slave con-name team0-eth1 ifname eth1 master team0 nmcli con add type team-slave con-name team0-eth2 ifname eth2 master team0","title":"Create the Port Interfaces"},{"location":"linux/EX358/Teaming/#bring-the-team-and-port-interfaces-up-or-down","text":"nmcli con up team0 nmcli con up team0-eth1 teamdctl team0 state","title":"Bring the Team and Port Interfaces Up or Down"},{"location":"linux/EX358/Teaming/#troubleshooting-network-teams","text":"","title":"Troubleshooting Network Teams"}]}